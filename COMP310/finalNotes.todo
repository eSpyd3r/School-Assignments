OS HISTORY:
 ☐ ENIAC:
    ☐ First programmable general purpose computer
    ☐ UPENN
    ☐ Program was input by flipping switches and connecting cables. Usually took weeks.
    ☐ At this time, NO OS as none had yet existed
    ☐ Did not store programs in memory
    ☐ Instead, computer resources were manged by operators
    ☐ Can be considered "mainframe computer"
      ☐ mainframe computer -> used by big companies for big data processing
      ☐ all computers of this era were of this type
 ☐ IBM 701:
   ☐ IBM's first comercially-available, stored-program mainframe computer
   ☐ Used for scientific computation (aircraft, nuclear explosives)
   ☐ Ran study on blackjack: rules ofr hitting, standing, doubling (1954)
   ☐ First "AI" program: checkers
   ☐ First automated translation software (Russion to English) for US Air Force
   ☐ Unlike ENIAC, IBM 701 was a stored-program computer.
   ☐ Programs were fed in via punch cards
     ☐ Much easier than having to rewire/flip switches every time
   ☐ No OS
 ☐ 1956 GM-NAA I/O
   ☐ Technological Advancements
     ☐ Tape systems: could read up to 15,000 characters per second, 45 times faster than punch card readers
       ☐ Allowed for larger programs to be read into memory
       ☐ Provided access to a library of commonly-used routines
     ☐ First computer user groups founded
       ☐ Regular meetings organized for computer users to share programs and exchange info on their programming practices
     ☐ First software companies founded
   ☐ The Operator
     ☐ The Eniac, IBM 701 and other computers of the time necessitated an operator
     ☐ The operator was the person who would take the pumch cards anc place them in the card reader
     ☐ They would start the program and supervise the use of the computer's resources.
     ☐ They would deal with malfunctions or bad commands
   ☐ Batch processing
     ☐ The IBM 701 was a batch processing system. Multiple jobs were put on the same tape (a "batch" of jobs), and executed one after the other; thus reducing idle time
     ☐ But manual intervention by human operators was often required between jobs and the supervise the jobs
  ☐ The Mock 701 Monitor (1955)
    ☐ Made at North American Aviation for the IBM 701
    ☐ Could switch betwen jobs in a batch automatically, and handle basic I/O operations without the need for an operator
    ☐ Knows as a resident monitor ("resident" since it was always in memory)
  ☐ GM-NAA I/O (1956):
    ☐ Made for the IBM 704
    ☐ Based on the 701 monitor ans similar work by one if its creators (Robert Patrick)
    ☐ More sophisticated, with better I/O control/automation
    ☐ Considered to be the first Operating System!
    ☐ Used on ~40 Systems
  ☐ Later Developments:
    ☐ 1959: SHARE (IBM user group) takes over. Makes their own version called SHARE OS
    ☐ 1960: IBM makes their own OS called IBSYS
 ☐ 1961: CTSS
   ☐ Recap:
     ☐ Computer mainframes being installed in big companies & universities
     ☐ Goal is to maximize throughput
   ☐ Debugging:
     ☐ 1959: Christopher Strachey suggested a system in which a programmer could debug a job while another job was running on the same computer
     ☐ Would take place at an auxiliary terminal connected to teh computer, such as a teletype machine
     ☐ Also in 1959, John McCarthy at MIT suggested that multiple users work simultaneously on the same compute. In other words, that the computer's processing time could be shared amongst multiple users
     ☐ This was accomplished by implementing a special interupt into the CPU (of the IBM 7094), called a clock interrupt
   ☐ Clock interrupts
     ☐ A clock interrupt is a signal generated by a timer in the CPU at some time interval, e.g., 0.2 seconds.
     ☐ When the clock interrupt is triggered, the CPU regains control from the current job
     ☐ It then decides whether to resume that job, or to start or resume another jb
   ☐ CTSS
     ☐ The Compatibile Time-Sharing System (CTSS)
     ☐ The first general purpose time-sharing OS
     ☐ Supported four terminals connected to the computer's I/O channel
     ☐ A clock interrupt would be triggered every 0.2 seconds. At each clock interrupt the OS regained control and could assign the CPU to another user's job.
     ☐ The first file system + accounts
       ☐ Each user was given their own tape to score their files
       ☐ The tapes were stored in the computer room
         ☐ The first file system*
       ☐ To access the files on their tape when were given an accouin was a password to login with
         ☐ Liekly the first invention of a computer password
     ☐ CTSS was also the first OS to let users enter text commands. Each time a user pressed the return key, their command was sent to the CPU and an interrup was triggered.
  ☐ OS/360  
    ☐ In the 1950s, many different machines were made, each with different architecture, memory, processing power, etc.
    ☐ The OS's we have seen so far (GM NAA I/O, CTSS) were each designed for a specific machine (IBM 701, 7094)
      ☐ In this era, IBM dominated the computer market
    ☐ IBM System/360
      ☐ Starting in 1965, IBM began releasing a family of general-purpose mainframe computers called System/360
      ☐ The computers had a large range in performance.
      ☐ Some were smaller, (relatively) cheaper, low-end systems for business use, while others were very expensive, very fast and with special capabilities for scientific omputing, and o on.
      ☐ But all used the same instruction set
      ☐ The goal was that all System/360 computers could run the same programas without need for modification
        ☐ A major shift: software often had to be rewritten to work on different models of a computer
      ☐ Each computer had different hardware components (CPU, memory, etc.) So they designed an OS that could work on all the hardware configurations and abstract the details of the differences from programs.
      ☐ In fact, one of the most important purposes of today's OS's is that they can work on all kinds of different computer models.
      ☐ IBM's OS/360 was the first such OS to be written. It provided a standardized API across a wide range of machines for file hadnling, memory management and task scheduling
        ☐ REALLY IMPORTANT: OS/360 set a prescedent for how OS's could abstract hardware differences and provide an API for software to use
      ☐ One of the most ambitious software engineering projects at the time
      ☐ More than 1,000 developers hired
      ☐ Tight deadline, and infancy of software engineering practice, led to huge delays, and over 1,000 known bugs at launch
 ☐ 1965-1972: Multics
   ☐ The Computers of Tomorrow
     ☐ Idea suggestion: information utility
     ☐ An information utility would be one which provides information to the community, through phone lines. People would have a terminal in their home, and connect to a computer mainframe to receive information
   ☐ The successor of CTSS:
     ☐ CTSS allowed users t oconnect to a system using dial-up terminals
     ☐ But it was an experimental system, and could not support many users at a time
     ☐ MIT decided to work on a more robust OS that could serve as a true information utility
   ☐ Multics:
     ☐ Developed by MIT in conjuction with GE and Bell Labs, funded by a DARPA grant (US Defense Advanced Research Projects Agency).
     ☐ Work started in 1964 and finished in 1969
     ☐ Designed for GE 646 (which was specially designed for it)
     ☐ Designed to be a 24/7 computing utility
       ☐ Could Continue running while additional processors, memory or disks were attached or removed
       ☐ Designed with security in mind since multiple users would be connecting
       ☐ It also featured innovations in memory management and filesystem to better accommodate multiple users.
     ☐ Virtual Memory
       ☐ Earl days: memory overlaying
       ☐ 1959: U. of Manchester working on Atlas Computer (one of the first supercomputers), introduces something called a one-level storage system, which was a system using paging
         ☐ The first distinction between virtual and physical addresses. They build hardware to do the address translation
         ☐ Invented "demand paging": lets the hard disk be used as a temporary memory. And build a page replacement policy
       ☐ virtual memory, being an untested technology, was not implemented in all computers right away (it took about a decade for implementations to become fast and for translation hardware to be more easily build)
       ☐ 1961: segmentation (Burroughs B5000)
       ☐ 1964: segmentation + paging (Multics/GE 645)
     ☐ Multics filesystem:
       ☐ CTSS: one directory per user (no subdirectories), simplistic protetion scheme
       ☐ Multics: first OS with hierarchial filesystem (nested subdirectories permitted)
         ☐ Also introduced the idea of Access Control Lists for each file (read/write/execute)
     ☐ Multics security
       ☐ Multics introduced a ring structure of access. Ring 0 had the highest level of privilege, and higher rings had lesser and lesser privileges.
       ☐ Each process operated in a specific ring. Processes in a lower ring could not be affected by thiose in higher rings. Hardware enforced the ring scheme.
       ☐ Prior OS's did not have such a formal approach to security were they supported with hardware to this extent.
       ☐ Today's architectures still use protection rings
         ☐ x86 has four rings, but modern OS's use only two of them (ring 0 - kernel mode, ring 3 - user mode)
         ☐ ARM TrustZone: two rings("worlds")
         ☐ Multics had support for 8 rings

The Process:
  ☐ Direct Execution -> allow one process to run until it's done
    ☐ No overhead since process has full control of the CPU, can do anything it wants and will not be paused
    ☐ Since full control of CPU, can hog all resources
    ☐ Can corrupt files on hard drive, crash OS, etc
    ☐ No isolation between processes
  ☐ Kernel:
    ☐ Subset of the OS with special rights and responsibilities
    ☐ Trust with full hardware access
    ☐ Has its own stack and special subset of instructions that only it can perform
  ☐ Processor Modes:
    ☐ User mode:
      ☐ code has restrictions: it cannot directly issue I/O requests. If it tried to do so, the processor would raise an exception and the OS would likely then kill the process
      ☐ No Privileged instructions
      ☐ No direct access to all of memory
      ☐ No direct access to devices
    ☐ Kernel Mode:
      ☐ code can do anything it likes, what the OS runs in
      ☐ Priviliged Instrictions
        ☐ Set mode bit
      ☐ Direct access to all of memory
      ☐ Direct access to devices
 ☐ System Calls:
   ☐ To safely switch between kernel and user mode, the OS exposes certain sensitive operations through a set of system calls, or syscalls for short.
   ☐ A system call is a special function call that goes into OS code and runs on the CPU in kernel mode
     ☐ Only in kernel mode can sensitive operations (like access to hardware be performed)
   ☐ The API to manage processes has a standard called the POSIX API: a standard set of system calles an OS must implement
   ☐ Most modern OS's are POSIX compliant
   ☐ Programs written for the POSIZ API can run on any POSIX compliant OS, ensuring program protability
 ☐ Perfmorming a system Call:
   ☐ Each OS has a kernel API that wraps system calls in function calls, to make them easier to use and more protable
   ☐ Then, the kernel API is then wrapped by program language libraries (like libc)
     ☐ the C library function printf invokes the write system call
     ☐ although libc makes system calls look like regular function calls, they are not function calls. Instead, they are a transition between the user and kernel (and thus much more expensive)
 ☐ Process APIs
   ☐ Some system calls involve management of processes. This is knows as the Process API and includes:
     ☐ create (creates a process)
     ☐ destroy (destroys a process)
     ☐ wait
     ☐ misc
     ☐ get info
 ☐ fork
   ☐ fork is a system call that can create a new process
   ☐ calling fork creates an (almost) exact copy of the calling process, which starts running at the line directly after the fork call. It is called the child process, while the initial process (which is still running) is called the parent.)
   ☐ the child will have its own copy of the address space, registers, PC, etc. One difference from the parent: the value it recieves from the fork call -- it recieves 0, while the parent receives the PID of the newly-created child.
     ☐ output of fork program is not detministic
   ☐ In Unix-like OS's, every process is created by forking from another process
 ☐ wait
   ☐ wait is a system call that lets a parent wait until a child process has finished running before continuing to execute. By using wait, we make out fork program deterministic
   ☐ will not return until the child has run and exited
   ☐ waitpid also exists which waits for a child process with a specific PID to end
 ☐ exec
   ☐ exec is a system call that lets us run a program different from the current (calling) program.
   ☐ it transforms the current process into the new one: it overwrites the ucrrent instructions in memory with the new program's instructions (after loading them from disk), and re-initializes the heap and stack and other parts of the program's memory space.
   ☐ exec never returns
 ☐ fork, wait, and exec together:
   ☐ these 3 are essential in building a shell
   ☐ When typing a command into the shell, it calls fork to make new child process, exec to make the child run the commnad, and wait to wait until the child completes before printing out a new prompt.
 ☐ Traps
   ☐ A trap is generated by the CPU as a result of error
     ☐ Divide by zero
     ☐ Execute privileged instruction in user mode
     ☐ Illegal acces to memory
   ☐ Works like an "involuntary" system call
     ☐ Sets mode to kernel mode
     ☐ Transferl control to kernel
   ☐ return-from-trap
     ☐ the trap and return-from-trap instructions are special. They do the following all at once:
       ☐ jump into kernel code (or process code, for return-from-trap)
       ☐ change the processor mode (user to kernel, or kernel to user)
       ☐ load the kernel stack (or process stack, for return-from-trap)
 ☐ Interupt:
   ☐ an interupt is generated by a device needing attention
     ☐ packet arrived from the network
     ☐ Disk I/O completed
     ☐ etc.
   ☐ Also works like an involuntary system call
 ☐ OS control flow
   ☐ the OS is an event-driven program. It only runs when a trap, interrupt or system call is generated
   ☐ In each of these cases, the processor will effect a mode switch from user mode to kernel
   ☐ Once in kernel mode, the kernel can perform whatever operation, then return control back to the process, using a special return-from-trap instruction
 ☐ Limited Direct Execution
   ☐ Recall direct execution: one process runs on CPU until done, and has full control of hardware (CPU, memory, disk)
   ☐ Limited direct execution: when a proces wants to do something sensitive, it issues a system call, which enters into the OS and then comes back using a return-from-trap. Once done, the process exits, which also traps back into the OS
 ☐ OS structure:
   ☐ User/OS separation -> "monolihthic OS"
     ☐ Downside:
       ☐ The OS is a huge piece of software; Millions of lines of code and growing
       ☐ If something goes wrong in kernel mode, most likely, machine will halt or crash.
       ☐ Incentive to move stuff out of kernel mode
     ☐ No need for entire OS in kernel
       ☐ Some pieces can be in user mode
         ☐ no need for privileged access
         ☐ no need for speed
     ☐ Example: daemons
       ☐ system log
       ☐ printer daemon
       ☐ Etc.
   ☐ Microkernel
     ☐ Absolute minimum in kernel mode
       ☐ interprocess communication primitives
     ☐ All the rest in user mode
     ☐ In practice, have failed commercieally (except for niches)
     ☐ The "systems programs" model has won out
 ☐ Single vs multiple process system
   ☐ Process operation
   ☐ From the point of view of an OS, a process does two things:
     ☐ either it computes (using the CPU), or
     ☐ it does I/O (using a device)
   ☐ In single process systems, process must be completed before moving onto next
     ☐ causes long wait times for processes waiting for the previous to complete
     ☐ Inefficient due to long CPU idle times and bad interactivity (can't do anything while a process is running)
   ☐ The issues of a single process system can be addressed by using a multi-process system.
     ☐ In multi-process systems, while a process waits for an I/O request to complete, another process can use the CPU!
     ☐ During the idle times of a process, the CPU will move to the next process until the CPU can resume the original process again
       ☐ High utilization, short CPU idle times
 ☐ Process States
   ☐ Running: executing instructions
   ☐ Ready: ready to run, but not executing at the moment
   ☐ Blocked: waiting for some event to take place (e.g., for OS to perform I/O request) until it can become ready
   ☐ See Lec 2. slide 89 of the Process State Diagram for Multiprocessing System
 ☐ Process switching
   ☐ Multiprocessing
     ☐ In a system that supports multiple processes, there are two important considerations:
       ☐ how to switch between processes, and 
       ☐ how to determine which process to run (scheduling)
     ☐ If a process is running on the CPU, then the OS is not. But if it is waiting on I/O, then another process should run
       ☐ Also, a process should not be able to run forever
     ☐ 2 problems: regaining control and context switching
     ☐ Problem 1: regaining control:
       ☐ How can the OS regain control of the CPU so that it can switch to another process?
       ☐ Two approaches:
         ☐ non-preemptive scheduling ("coorperative multitasking")
         ☐ preemptive scheduling (noncooperative)
       ☐ Cooperative Approach:
         ☐ The OS trusts processes to behave responsibly
         ☐ Special syscall "yield" lets the process give back control to the CPU... when it wants
           ☐ Note: if the program performs an illegal operation (like a bad memory access), OS also regains control
         ☐ Known as non-preemptive scheduling
         ☐ Problem: A process could run forever, locking all other processes out
         ☐ Solution: Timer interupts, which is resolved by the alternative "Non-cooperative approach"
       ☐ Non-cooperative approach:
         ☐ Timer interrups
           ☐ A timer device can be programmed to raise an interrupt every so often, at which point the process is forcibly paused and the OS executes and interrupt handler, regaining control
         ☐ Known as preemptive scheduling
       ☐ Non preemptive vs. Preemptive
         ☐ Non-preemptive:
           ☐ Process can monopolize the CPU
           ☐ Only useful in special circumstances
         ☐ Preemptive
           ☐ Process can be thrown out at any time
           ☐ Usually not a problem, but sometimes it is
         ☐ Intermediate solutions are possible

     ☐ Problem 2: context switching:
       ☐ When the OS regains control, it can decide whether to continue running the current process or switch to another
         ☐ This decision is made by the OS scheduler
       ☐ If the OS decides to switch, it executes a context switch
   ☐ Process info:
     ☐ Consists of:
       ☐ Code
       ☐ Stack
       ☐ Heap
       ☐ Registers (including program counter)
       ☐ MMU info
     ☐ Code, stack and heap are already stored in memory private to a process
     ☐ But registers and MMU info are stored in a place for the current running process
   ☐ Process switch P1 -> P2:
     ☐ Save registers (P1) to somewhere
     ☐ Restore registers (P2) from somewhere
     ☐ Where to save and restore from? -> see PCB
   ☐ Process Control Block (PCB)
     ☐ The kernel maintains an array of process control blocks (PCB) which contains information about current processes, including:
       ☐ Process identifier (unique id)
       ☐ Process state
       ☐ Space to support process switch (save area)
     ☐ The PCB array is indexed by a hash of the PID
   ☐ Process switch P1 -> P2 (with PCB in mind):
     ☐ Save registers -> PCB[P1].SaveArea
     ☐ Restore PCB[P2].SaveArea -> registers
     ☐ Run return-from-trap instruction
   ☐ Process siwtching is slow:
     ☐ A process switch is an expensive operation!
     ☐ Requires saving and restoring lots of stuff (registers, PC, MMU, info, etc.).
     ☐ Has to be implemented very efficiently and used with care.
   ☐ Context switching overhead
     ☐ Cost of saving registers
     ☐ Cost of scheduler to determine which job to run next
     ☐ Cost of restoring registers
     ☐ Cost of flushing caches (L1, L2, L3, TLB)
 ☐ Process Scheduling
   ☐ Multiprocessing
     ☐ In a system that supports multiple process, there are two important considerations:
       ☐ how to switch between processes, and 
       ☐ how to determine which process to run (scheduling)
   ☐ Scheduling
     ☐ The OS scheduler decides when (and which) to run a ready process
       ☐ If a ready process is run, we say it has been scheduled
       ☐ Or if it running and moves to ready, we say it has been de-scheduled
     ☐ The role of the scheduler:
       ☐ think of scheduler as managing a queue
       ☐ when a process is ready, it is inserted into the queue, according to a scheduling policy
       ☐ scheduling decision: run head of queue
     ☐ Implementation:
       ☐ Remeber running process
       ☐ Maintain sets of queues
         ☐ (CPU) ready queue
         ☐ I/O device queue (one per device)
       ☐ PCBs sit in queues
     ☐ How does athe scheduler run?
       ☐ The scheduler is part of the kernel, so it can run when the kernel runs, i.e. when:
         ☐ process starts or terminates (system call)
         ☐ running process performs an I/O (system call)
         ☐ I/O compeltes (I/O interrupt)
         ☐ timer expires (timer interrupt)
         ☐ a trap occurs
     ☐ Metrics:
       ☐ a scheduling policy determines which ready process will be scheduled next
       ☐ To compare scheduling policies, we need scheduling metrics
 ☐ Scheduling Metrics and Policies
   ☐ Workload:
     ☐ The workload is the set of running processes, or jobs. We will make the following assumptions about jobs:
       ☐ Each job runs for the same amount of time
       ☐ All jobs arrive at the same time
       ☐ Once started, each job runs to completion
       ☐ All jobs only use the CPU
       ☐ The run-time of each job is known
   ☐ Turnaround time:
     ☐ The time it takes for a job to complete: a measure of performance
     ☐ Turaround = Time of completion - Time of arrival
       ☐ Since we assume all jobs arrive at the same time, Time of arrival will be 0 for now
   ☐ Response time:
     ☐ Response time: the difference between the time the job arrives and the time it is first scheduled
     ☐ Response = Time of first run - Time of arrival
       ☐ So that users can see some kind of progress on their job without waiting too long
   ☐ Types of jobs: Interactive vs. Batch
     ☐ What makes a good scheduler for interactive or batch?
     ☐ Interactive = you are waiting for the result
       ☐ E.g., browser, editior
       ☐ Tend to be short
     ☐ Batch = you will look at result later
       ☐ E.g., supercomputing center, offline analysis
       ☐ Tend to be long
     ☐ What makes a good scheduler for interactive?
       ☐ Short response time
       ☐ Response time = wait from ready -> running
         ☐ Initial_schedule_time = arrival_time
     ☐ What makes a good scheduelr for batch?
       ☐ High throughput
       ☐ Throughput = number of jobs completed
         ☐ minimize scheduling overhead
           ☐ Reduce number of ready -> running switches
     ☐ Other Concerns:
       ☐ Fiarness: who gets the resources?
         ☐ We want an equitable division of resources
       ☐ Starvation: how bad can it get?
         ☐ Lack of progress by some job
       ☐ Oberhead: how much useless work?
         ☐ Time wasted switching between jobs
       ☐ Predictability: how consistent?
         ☐ Low variance in respnse time for repeated requests
     ☐ Basic Scheduling Algorithms
       ☐ FIFO (first in, first out)
         ☐ the most basic we can implement. Jobs are executed by the time they arrived in the system. (They are inserted at tail of queue)
         ☐ By definition, non-preemptive
         ☐ Pros and Cons:
           ☐ Low overhead - few scheduling events
           ☐ Good throughput
           ☐ Uneven response time - stuck behind long job
           ☐ Extreme cases - process monopolizes CPU
       ☐ SJF (shortest job first)
         ☐ Improves average turnaround time
         ☐ Optimal scheduling algorithm if all jobs arrive at the same time
         ☐ Pros and Cons:
           ☐ good response time for short jobs, but only if all jobs arrive at same time
           ☐ Can lead to starvation of long jobs
       ☐ STCF (Shortest Time To Completion First)
         ☐ Under the circumstance where jobs DONT NEED to run until done and the scheduler can preempt a job (pause one, and start another), this is the optimal algorithm
       ☐ Round-Robin
         ☐ Instead of running jobs to completion, run a job for a time slice (aka scheduling quantum), then switch to the next job in the queue (the one that hasn't been run for the longest time)
         ☐ Continue until all are done
         ☐ Pros and Cons:
           ☐ Good compromise for long and short jobs
           ☐ Short jobs finish quickly
           ☐ Long jobs are not postponed forever
           ☐ No need to know job length
         ☐ Picking a scheduling quantum:
           ☐ Too small
             ☐ Many scheduling events
             ☐ Good response time
             ☐ Low throughput
           ☐ Too large
             ☐ few scheduling events
             ☐ Good throughput
             ☐ Poor response time
           ☐ Typical value 10 milliseconds

The Memeory:
  ☐ Memory: the dream
    ☐ What every programmer would like is a:
      ☐ private,
      ☐ infinitely large,
      ☐ infinitely fast,
      ☐ nonvolatile, and 
      ☐ cheap memory
  ☐ ASSUMPTION START FOR LECTURE: All of a program must be in main memory
  ☐ Memory allocation: roadmap
    ☐ Where to locate the kernel?
    ☐ Cocept of address space
    ☐ Virtual vs physical mempory
    ☐ Goals of memory management
  ☐ Allocating Main memory for kernel:
    ☐ Almost always low in memory (0 - 100) because Interrupt vectors are in low memory
  ☐ The address space:
    ☐ The OS provides an abstraction of physical memory to each process, called the address space
    ☐ The address space of a pcoress contains:
      ☐ the code (instructions)
      ☐ the stack (local vars, params, return vals), and
      ☐ the heap (dynamically allocated memory)
  ☐ The problem:
    ☐ The address space for each process starts at address 0. But the address in this space are virtual addresses, not real addresses into physical memory
    ☐ How can the OS virtualize the single, physical memory, so that each process has its own private address space, with virtual addresses translated into actual physical addresses
  ☐ Virtual vs. Physical Address space
    ☐ Virtual/logical address space = What the program thinks is its memory
      ☐ Address generated by program/CPU
    ☐ Physical address space = where the program actually is in physical memory
      ☐ Address that the memory sees
    ☐ Must map/translate every access from the CPU to memory
  ☐ Goals:
    ☐ Transparency: should be invisible
      ☐ Processes shouldnt need to know that memory is shared
      ☐ Should work regardless of number and/or location of proceses
      ☐ Programmer shouldn't have to worry about:
        ☐ Where their program is in memory, or
        ☐ where or what other programs are in memory
    ☐ Protection: a process should not be able to do bad things
      ☐ Privacy: Should not be able to read/write the memory of another process or the kernel
      ☐ Should not be able to corrupt OS or other porcesses
    ☐ Efficiency: in terms of time and memory space
  ☐ Memory Management Unit (MMU)
    ☐ Provides mapping virtual-to-physical
    ☐ Provides protection at the same time
    ☐ Hardware!
  ☐ Size of address sapces:
    ☐ Maximum virtual address space size
      ☐ Limited by address size of CPU
      ☐ Typically 32 or 64 bit addresses
      ☐ So, 2^32 = 4 GB, or 2^64 = 16 exabytes (BIG!)
    ☐ Physical address space size:
      ☐ Limited by size of memory
      ☐ Nowadays, order of tens/hundreds of GB
  ☐ Stack and heap:
     ☐ Motivation for dynamic memory allocation
       ☐ Why do processes need dynamic allocation of memory:
         ☐ Do not know amount of memory needed at compile time
         ☐ Must be pessimistic for static memory allocation.
         ☐ If they statically allocate enough for worst possible case, storage is used inefficiently
         ☐ Recursive procedures:
           ☐ Do not know how many times procedure will be nested
         ☐ Complex data structures: lists and trees
     ☐ Stack and heap are 2 types of dynamic allocation
     ☐ Stack:
       ☐ OS uses stack for procedure call frames (local variables and parameters)
       ☐ Memory is freed in opposite order from allocation
       ☐ Ex:
         ☐ alloc(A);
         ☐ alloc(B);
         ☐ alloc(C);
         ☐ ...puts on stack, reading from bottom up: A->B->C
           ☐ C on top, A on bottom
         ☐ So, to free memory, need to call free(C) first, freeing from the top of the stack
       ☐ Simple and efficient implementation:
         ☐ Pointer separates allocated freed space
         ☐ Allocate: Increment pointer
         ☐ Free: Decrement pointer
       ☐ No fragmentation
       ☐ Stack management done automatically
     ☐ Heap:
       ☐ Allocate from any random location
         ☐ Heap consists of allocated areas and free areas (holes)
         ☐ Order of allocation and free is unpredictable
       ☐ + Works for all Data Structres
       ☐ - Allocation can be slow
       ☐ - Fragmentation
       ☐ Programmers manage allocations/deallocations with library calls (malloc/free)
 ☐ The Memory API
   ☐ The stack
     ☐ Allocations and deallocations managed by the compiler
   ☐ The heap
     ☐ You handle the allocations and deallocations
     ☐ malloc() allocates memory on the heap. You pass it a size, and it gives you a pointer to the newly-allocated space (or 0 on failure).
       ☐ The size is usually given by sizeof()
       ☐ It returns a pointer of type void*, so we must cast it
       ☐ double *d = (double *) malloc(sizeof(double));
     ☐ free() deallocates memory on the heap. You pass it a pointer to the memory you want to deaalocate
       ☐ int *x = malloc(10 * sizeof(int));
       ☐ // ...
       ☐ free(x);
 ☐ Address Translation
   ☐ Virtual addresses are passed through the MMU and translated (mapped) to physical addresses
     ☐ This occurs during an instruction fetch, load or store
   ☐ We will look at a simple example of a translation, then see three difference translation (mapping) approaches
     ☐ This lecture assumes that the process address space is placed contiguously in physical memory, and that its size is smaller than the physical memory
 ☐ Mapping Schemes
   ☐ Base and bounds
     ☐ Main idea: use two hardware registers in the MMU: the base register and the bounds register
     ☐ It will let us place a process' address space anywhere in physical memory, while maintaining isolation of the address space from others'
     ☐ When a process starts, the OS will decide where in physical memory the address space of that process should start. It then sets the base register to that address.
     ☐ When the process references a virtual memoryy address, the MMU translates it to a physical address by incrementing it by the base address
     ☐ The bounds register stores the size of the process' address space
     ☐ The MMU will check that any memory reference used by the process is within bounds, i.e., that the virtual address is between 0 and the bounds register
       ☐ If not, the MMU generates a trap, so that the OS can kill the process for trying to access memory outside of its address space
     ☐ Virtual address space
       ☐ linear address space from 0 to MAX
     ☐ Physical address space:
       ☐ linear address space from BASE to BOUNDS = BASE + MAX
     ☐ The base and bounds registers are kept in the MMU
     ☐ Modifications to the base and bounds are privileged operations, i.e., they can only be changed in kernel mode
     ☐ Main memory:
       ☐ Regions in use
       ☐ "Holes" are regions not in use, so a new process must go in a hole
     ☐ Free List:
       ☐ A list of the range of the physical memory not in use
     ☐ Problem: External Fragmentation
       ☐ Small holes become unusable, part of memory can't be used
     ☐ Problem: Internal fragmentation
       ☐ Big chunk of "free" space in virtual address space
       ☐ "free" space takes up physical memory
       ☐ Inefficient
   ☐ Fragmentation:
     ☐ Internal fragmentation: Typical paper book is a collection of pages (text divided into pages). When a chapter's end isn't located at the end of page and new chapter starts from new page, there's a gap between those chapters and it's a waste of space — a chunk (page for a book) has unused space inside (internally) — "white space"
     ☐ External fragmentation: Say you have a paper diary and you didn't write your thoughts sequentially page after page, but, rather randomly. You might end up with a situation when you'd want to write 3 pages in row, but you can't since there're no 3 clean pages one-by-one, you might have 15 clean pages in the diary totally, but they're not contiguous
   ☐ Segmentation
     ☐ What is a segment:
       ☐ Anything we want it to be
       ☐ A segment is a contiguous portion of the address space of a particular
     ☐ Examples:
       ☐ Code
       ☐ Heap
       ☐ Stack
     ☐ Segmentation: instead of a single base/bounds pair per process, we will have a base/bounds pair per segment of the address space
     ☐ then, we can place each segment in different places of the physical memory
     ☐ Virtual Address Space
       ☐ Two-dimensional
       ☐ Set of segments 0 ... n
       ☐ Each segment i is linear from 0 to MAXi
     ☐ Physical Address Space
       ☐ Set of segments, each linear
     ☐ Attempting to refer to an address outside of a segment causes a segmentation fault
     ☐ The MMU must know which segment is associated with each address
     ☐ One way to do this is to set aside the first two btis of each address and use them to indicate the segment (00/01/10).
     ☐ Course-grained = only a few segments (code/stack/heap)
     ☐ Fine-grained = a large number of smaller segments
     ☐ Pros of segmentation:
       ☐ Unlike base and bounds, avodis assigning an entire address space of code, heap, stack and unused virtual memory to process
         ☐ Instead, heap and stack can grow into unused physical memory
       ☐ Minimal overhead
     ☐ Problem: External Fragmentation
       ☐ Physical memory will quickly become full of little holes of free space
       ☐ Makes it difficult to allocate new segments, or grow existing ones
     ☐ Possible Solution: compact physical memory by rearranging existing segments (copything them into a contiguous block).
       ☐ But it's expensive, and makes growing segments harder
     ☐ Segmentation ~= multiple base-and-bounds
       ☐ No internal fragmentation inside each segment
       ☐ External fragmentation problem is similar
         ☐ Pieces are typically smaller
     ☐ Sharing Memory between processes
       ☐ Why would we want to do that?
       ☐ For instance,
         ☐ Run the same program twice in different processes
         ☐ May want to share code
         ☐ Read the same file twice in different processes
         ☐ May want to share memory corresponding to file
       ☐ Sharing not possible with base and bounds, but is possible with segmentation
    ☐ SEGMENTATION PROVIDES EASY SUPPORT FOR SHARING
      ☐ Create segment for shared data
      ☐ Add segment entry in segment table of both processes
      ☐ Points to hsared segment in memory
      ☐ Extra hardware support is needed for form of Portection bits.
        ☐ A few more bits per segment to indicate permissions of read, write, and execute
   ☐ Paging (see next note section)

Paging:
  ☐ Free Space Management:
    ☐ The problem (continuing from Segmentation notes):
      ☐ How to deal with external fragmentation? Can it be minimized? What are the overheads involved?
    ☐ Free list:
      ☐ A free list contains a set of elements that describe the free space remaining in the heap
    ☐ Low-level mechanisms
      ☐ Splitting memory
      ☐ Coalescing memory
      ☐ Keeping track of allocated chunks
      ☐ Memory for the free list
    ☐ Splitting (Lecture 5 Slide 29 Example)
    ☐ Coalescing:
      ☐ when memory is freed (e.g., through free()), the allocator coalesces (merges) that memory with any adjacent free chunks on the free list
      ☐ coalescing free space helps to reduce external fragmentation a little bit
      ☐ can be done when memory is freed, or it can be deferred until a later point
    ☐ Tracking size of allocations
      ☐ when free() is called, we don't need to pass the size of memory to free. The allcator keeps track on its own
      ☐ to do so, each allocated piece of memory has a header block (So if the caller asked for N bytes, the allocator must find a chunk of size N + sizeof header)
    ☐ Free list in memory
      ☐ The free list itself must be placed in memory. It is represented as a linked list
      ☐ Like allocated memory, each free chunk in memory will begin with a header, in this case being a node of this type
    ☐ One free chunk
      ☐ Assume that we are managing an empty heap of 4KB, and that the header for the free chunk will be 8 bytes. the head pointer point to the start of the header
    ☐ One allocation, one free chunk
      ☐ Upon an allocation request, a free chunk of sufficient size will be split into two. the first chunk will fit the header and request, and the second will be a free chunk (also with a header)
    ☐ Allocation strategies:
      ☐ When a piece of memory is requested, the allocator must choose which chunk of free space to use for the new allocaiton. Some possible strategies: 
        ☐ Best fit
          ☐ Find free chunks that are as big or bigger than requested size
          ☐ Return the one that is the smallest (the "best-fit" chunk)
          ☐ Tries to reduce wasted space
          ☐ DrawbackL: needs to look through all free chunks
        ☐ Worst fit
          ☐ Opposite of best fit: find largest free chunk, split it and return the first part and keep the second (large) chunk on the free list
          ☐ Overall effect: after frees, lots of big chunks are left in memory instead lots of small chunks.
          ☐ Drawback: full seach of free space still
          ☐ Also leads to excess external fragmentation
        ☐ First fit
          ☐ Finds the first free chunk that is big enough
          ☐ Fast: does not need to search through all
          ☐ Drawback: Beginning of the free list gets polluited with small objects
        ☐ Next fit
          ☐ Like first fit, but maintains a pointer to the location in the list where the last search ended, then starts looking there for the next search.
          ☐ Thus allocations are made more uniformly through the list, so the beginning is not crowded with small objects
          ☐ Exhaustive search also again avoided
  ☐ Paging:
    ☐ Space management:
      ☐ Segmentation: chop space up into variable-sized pieces
        ☐ leads to fragmentation
      ☐ Paging: chop space into fixed-sized pieces
        ☐ A process' address space (code, heap stack) is divided into fixed-sized units called pages
        ☐ Physical memory: an array of fixed-sized slots called page frames
    ☐ Virtual Address Space
      ☐ Linear from 0 up to a multiple of page size
    ☐ Physical Address Space
      ☐ Noncontiguous set of frames, one per page
      ☐ Frame size is same as page size
    ☐ Paging advantages
      ☐ Flexibility: no assumptions needed about how heap and stack are used, or which direction they grow, etc.
      ☐ Simplicity: free list of free pages. Easy to slot in pages
    ☐ A page table is a data structure kept by the OS for each process. It stores the mapping between each virtual page of the address space and its place in physical memory
      ☐ It is used for address translations from virtual page to physcial page frame
    ☐ Paging: Virtual Address
      ☐ Two components in the virtual address
        ☐ VPN: virtual page number
        ☐ Offset: offset within the page; Page size = 2^(offset)
      ☐ See Lecture 5 Slide 104 for address translation examples
      ☐ MMU for paging:
        ☐ Page table:
          ☐ Data structure used to map the virtual address to physical
          ☐ Indexed by a page number
          ☐ Contains frame number of page in memory
          ☐ Each process has a page table
        ☐ Also need: Pointer to page table in memory, Length of page table
      ☐ Porblem?
        ☐ Address space sparsely used
        ☐ Access to unused portion will appear valid
        ☐ Would rather have an error
      ☐ Solution Valid/Invalid Bit
        ☐ Page table has length 2^p
          ☐ Page table does not cover the entire possible virtual address space, only the pages that the process has allocated
        ☐ Have valid bit in each page table entry
          ☐ Set to valid for used portions of address space
          ☐ Invalid of unused portions
          ☐ (this is the most common approach)
    ☐ Main Memory allocation with paging:
      ☐ Logical address space: fixed size pages
      ☐ Physical address space: fixed size frames
      ☐ New process:
        ☐ Find frames for all of process's pages
      ☐ Easier to find memory than with segmentation
        ☐ Fixed size
    ☐ Internal Fragmentation in Paging:
      ☐ With paging
        ☐ Address space = multiple of page size
      ☐ Part of last page may be unused
      ☐ Not a big problem with reasonable page size
    ☐ In Reality:
      ☐ Base-and-bounds only for niche
      ☐ Segmentation abandoned
        ☐ High complexity for little gain
        ☐ Effect approximated with paging + valid bits
      ☐ Paging is now universal
    ☐ Faster translations
      ☐ Speed problem (See lecture 6 slide 26): Paging address translation performance
      ☐ Solution: a new cache in hardware
        ☐ to speed up paging, we add a cache into the MMU known as the translation-lookaside-buffer (TLB)
        ☐ Upon each virtual memory reference, we check the TLB to see if the requested translation is there. If so, we can use it without consulting the page table
      ☐ TLB:
        ☐ If the TLB holds the translation for the VPN (TLB hit), we can use the TLB entry to get the page frame number
        ☐ If the TLB does not hold the translation (TLB miss), the hardware accesses the page table to get the translation.
          ☐ Misses are expensive, so we want to avoid them as much as possible


















