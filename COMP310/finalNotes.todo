OS HISTORY:
 ☐ ENIAC:
    ☐ First programmable general purpose computer
    ☐ UPENN
    ☐ Program was input by flipping switches and connecting cables. Usually took weeks.
    ☐ At this time, NO OS as none had yet existed
    ☐ Did not store programs in memory
    ☐ Instead, computer resources were manged by operators
    ☐ Can be considered "mainframe computer"
      ☐ mainframe computer -> used by big companies for big data processing
      ☐ all computers of this era were of this type
 ☐ IBM 701:
   ☐ IBM's first comercially-available, stored-program mainframe computer
   ☐ Used for scientific computation (aircraft, nuclear explosives)
   ☐ Ran study on blackjack: rules ofr hitting, standing, doubling (1954)
   ☐ First "AI" program: checkers
   ☐ First automated translation software (Russion to English) for US Air Force
   ☐ Unlike ENIAC, IBM 701 was a stored-program computer.
   ☐ Programs were fed in via punch cards
     ☐ Much easier than having to rewire/flip switches every time
   ☐ No OS
 ☐ 1956 GM-NAA I/O
   ☐ Technological Advancements
     ☐ Tape systems: could read up to 15,000 characters per second, 45 times faster than punch card readers
       ☐ Allowed for larger programs to be read into memory
       ☐ Provided access to a library of commonly-used routines
     ☐ First computer user groups founded
       ☐ Regular meetings organized for computer users to share programs and exchange info on their programming practices
     ☐ First software companies founded
   ☐ The Operator
     ☐ The Eniac, IBM 701 and other computers of the time necessitated an operator
     ☐ The operator was the person who would take the pumch cards anc place them in the card reader
     ☐ They would start the program and supervise the use of the computer's resources.
     ☐ They would deal with malfunctions or bad commands
   ☐ Batch processing
     ☐ The IBM 701 was a batch processing system. Multiple jobs were put on the same tape (a "batch" of jobs), and executed one after the other; thus reducing idle time
     ☐ But manual intervention by human operators was often required between jobs and the supervise the jobs
  ☐ The Mock 701 Monitor (1955)
    ☐ Made at North American Aviation for the IBM 701
    ☐ Could switch betwen jobs in a batch automatically, and handle basic I/O operations without the need for an operator
    ☐ Knows as a resident monitor ("resident" since it was always in memory)
  ☐ GM-NAA I/O (1956):
    ☐ Made for the IBM 704
    ☐ Based on the 701 monitor ans similar work by one if its creators (Robert Patrick)
    ☐ More sophisticated, with better I/O control/automation
    ☐ Considered to be the first Operating System!
    ☐ Used on ~40 Systems
  ☐ Later Developments:
    ☐ 1959: SHARE (IBM user group) takes over. Makes their own version called SHARE OS
    ☐ 1960: IBM makes their own OS called IBSYS
 ☐ 1961: CTSS
   ☐ Recap:
     ☐ Computer mainframes being installed in big companies & universities
     ☐ Goal is to maximize throughput
   ☐ Debugging:
     ☐ 1959: Christopher Strachey suggested a system in which a programmer could debug a job while another job was running on the same computer
     ☐ Would take place at an auxiliary terminal connected to teh computer, such as a teletype machine
     ☐ Also in 1959, John McCarthy at MIT suggested that multiple users work simultaneously on the same compute. In other words, that the computer's processing time could be shared amongst multiple users
     ☐ This was accomplished by implementing a special interupt into the CPU (of the IBM 7094), called a clock interrupt
   ☐ Clock interrupts
     ☐ A clock interrupt is a signal generated by a timer in the CPU at some time interval, e.g., 0.2 seconds.
     ☐ When the clock interrupt is triggered, the CPU regains control from the current job
     ☐ It then decides whether to resume that job, or to start or resume another jb
   ☐ CTSS
     ☐ The Compatibile Time-Sharing System (CTSS)
     ☐ The first general purpose time-sharing OS
     ☐ Supported four terminals connected to the computer's I/O channel
     ☐ A clock interrupt would be triggered every 0.2 seconds. At each clock interrupt the OS regained control and could assign the CPU to another user's job.
     ☐ The first file system + accounts
       ☐ Each user was given their own tape to score their files
       ☐ The tapes were stored in the computer room
         ☐ The first file system*
       ☐ To access the files on their tape when were given an accouin was a password to login with
         ☐ Liekly the first invention of a computer password
     ☐ CTSS was also the first OS to let users enter text commands. Each time a user pressed the return key, their command was sent to the CPU and an interrup was triggered.
  ☐ OS/360  
    ☐ In the 1950s, many different machines were made, each with different architecture, memory, processing power, etc.
    ☐ The OS's we have seen so far (GM NAA I/O, CTSS) were each designed for a specific machine (IBM 701, 7094)
      ☐ In this era, IBM dominated the computer market
    ☐ IBM System/360
      ☐ Starting in 1965, IBM began releasing a family of general-purpose mainframe computers called System/360
      ☐ The computers had a large range in performance.
      ☐ Some were smaller, (relatively) cheaper, low-end systems for business use, while others were very expensive, very fast and with special capabilities for scientific omputing, and o on.
      ☐ But all used the same instruction set
      ☐ The goal was that all System/360 computers could run the same programas without need for modification
        ☐ A major shift: software often had to be rewritten to work on different models of a computer
      ☐ Each computer had different hardware components (CPU, memory, etc.) So they designed an OS that could work on all the hardware configurations and abstract the details of the differences from programs.
      ☐ In fact, one of the most important purposes of today's OS's is that they can work on all kinds of different computer models.
      ☐ IBM's OS/360 was the first such OS to be written. It provided a standardized API across a wide range of machines for file hadnling, memory management and task scheduling
        ☐ REALLY IMPORTANT: OS/360 set a prescedent for how OS's could abstract hardware differences and provide an API for software to use
      ☐ One of the most ambitious software engineering projects at the time
      ☐ More than 1,000 developers hired
      ☐ Tight deadline, and infancy of software engineering practice, led to huge delays, and over 1,000 known bugs at launch
 ☐ 1965-1972: Multics
   ☐ The Computers of Tomorrow
     ☐ Idea suggestion: information utility
     ☐ An information utility would be one which provides information to the community, through phone lines. People would have a terminal in their home, and connect to a computer mainframe to receive information
   ☐ The successor of CTSS:
     ☐ CTSS allowed users t oconnect to a system using dial-up terminals
     ☐ But it was an experimental system, and could not support many users at a time
     ☐ MIT decided to work on a more robust OS that could serve as a true information utility
   ☐ Multics:
     ☐ Developed by MIT in conjuction with GE and Bell Labs, funded by a DARPA grant (US Defense Advanced Research Projects Agency).
     ☐ Work started in 1964 and finished in 1969
     ☐ Designed for GE 646 (which was specially designed for it)
     ☐ Designed to be a 24/7 computing utility
       ☐ Could Continue running while additional processors, memory or disks were attached or removed
       ☐ Designed with security in mind since multiple users would be connecting
       ☐ It also featured innovations in memory management and filesystem to better accommodate multiple users.
     ☐ Virtual Memory
       ☐ Earl days: memory overlaying
       ☐ 1959: U. of Manchester working on Atlas Computer (one of the first supercomputers), introduces something called a one-level storage system, which was a system using paging
         ☐ The first distinction between virtual and physical addresses. They build hardware to do the address translation
         ☐ Invented "demand paging": lets the hard disk be used as a temporary memory. And build a page replacement policy
       ☐ virtual memory, being an untested technology, was not implemented in all computers right away (it took about a decade for implementations to become fast and for translation hardware to be more easily build)
       ☐ 1961: segmentation (Burroughs B5000)
       ☐ 1964: segmentation + paging (Multics/GE 645)
     ☐ Multics filesystem:
       ☐ CTSS: one directory per user (no subdirectories), simplistic protetion scheme
       ☐ Multics: first OS with hierarchial filesystem (nested subdirectories permitted)
         ☐ Also introduced the idea of Access Control Lists for each file (read/write/execute)
     ☐ Multics security
       ☐ Multics introduced a ring structure of access. Ring 0 had the highest level of privilege, and higher rings had lesser and lesser privileges.
       ☐ Each process operated in a specific ring. Processes in a lower ring could not be affected by thiose in higher rings. Hardware enforced the ring scheme.
       ☐ Prior OS's did not have such a formal approach to security were they supported with hardware to this extent.
       ☐ Today's architectures still use protection rings
         ☐ x86 has four rings, but modern OS's use only two of them (ring 0 - kernel mode, ring 3 - user mode)
         ☐ ARM TrustZone: two rings("worlds")
         ☐ Multics had support for 8 rings
 ☐ 1969-75: Unix:
   ☐ PDP:
     ☐ Minicomputer created by DEC
     ☐ Minicomputer = smaller, much lower cost computer than a mainframe
     ☐ Was used to start the code for Unics by Thompson and Dennis Ritchie from Bell Labs, both of which were working on multics
     ☐ Unics, based on Multics was aimed to be more simple and minimalistic designed for lower-end machines.
     ☐ Later shortened to Unix
   ☐ Unix:
     ☐ Ritchie and Thompson began implementing a hierarchial file system (similat to Multics), along with a shell and assembler
       ☐ Devices treated as files
       ☐ Process control via form, et.c)
       ☐ Pipes for inter-process communication
     ☐ v1.0 released in 1971
     ☐ B and C:
       ☐ At the same time that they were working on Unix, Thompson and Ritchie developed the B programming language, a simplified version of something called BCPL
       ☐ ritchie continued to improve the new language, and by 1972, renamed it to C.
     ☐ Portability:
       ☐ In 1973, Unix v4 was released. Its kernel was rewritten in C, one of the first OS kernels to be written in a high-level lanfuage.
       ☐ the migration to C made Unix a portable OS, meaning that it could be easily ported to other machines, since only a small amount of machine-dependednt code had to be changed
     ☐ Popularity:
       ☐ Because of the continuing development, portability and low licensing cost, Unix became increasingly popular.
       ☐ New Version continued to be released in the 1980s, including versions for microcomputers
         ☐ A microcomputer was a small inexpensive computer for hobbyists, which first came out in the early 1970s
       ☐ By 1978, over 600 machines were running some form of Unix, and over 2,000 by 1981. By 1986, it had grown to 230,000 machines.
     ☐ The Unix Philosophy:  
       ☐ Make it easy to write, test, and run programs.
       ☐ Interactive use insteaad of batch processing.
       ☐ Economy and elegance of design due to size constraints
       ☐ Self-supporting System: all Unix software is maintained under Unix
 ☐ 1974: CP/M:
   ☐ CP/M = Control program for Microcomputers
   ☐ Like OS/360 (and Unix), allowed software developers to not have to worry about technical details about the computer hardware
   ☐ The most popular OS for microcomputers in the 70s
     ☐ Supported many different brands
   ☐ Sold by mail-order to hobbyists for $74
   ☐ 1982: Annual sales over $20 million
   ☐ Components:
     ☐ Basic Input/Output system (BIOS): Controlled hardware components (e.g., device input, output to display, reading/writing disk sectors). Differed based on the hardware. Small.
     ☐ Basic Disk Operating System (BDOS): Filesystem, process redirection etc. Well-defined set of system calls
     ☐ Console Command Processor (CCP): Shell (DIR, ERA, COM fiels for utility programs)
   ☐ Software Industry
     ☐ Software copies would sell in the hundreds/thousands for mainframs and microcomputers since they were limited to big corporations/institutions. But microcomputers were selling in the hundreds of thousands
     ☐ Since CP/M was dominant in this time period, software that targeted CP/M could run on a large array of differnt machines
     ☐ The market share for a software product thus expanded greatly. Companies started making software products for microcomputer users to buy directly.
 ☐ 1981: MS-DOS:
   ☐ IBM:
     ☐ Founded as a record-keeping/measuring system company in 1911
     ☐ Became leading manufacturer of punch-card tabulating systems
     ☐ 1940s: Initial forays into computers with MARK I (Harvard); first commercial hard drive
     ☐ 1964: System/360 and OS/360. Dominant in the computer mainframe industry
     ☐ IBM eventually decided to make minicomputers in 1976, 12 years after DEC's PDP-8. But they couldn't keep pace. By the start of the 1980s, their market share in minicomputers was falling
     ☐ Annual sales of microcomputers booming, so IBM also decided to make a microcomputer, but was also late to the game and couldn't keep pace; they knew they needed to enter the market with a strong product t oavoid another failure
     ☐ They started work on the IBM Personal Computer (IBM PC) in 1980 and released it the next year
       ☐ IBM PC used Intel's newest 8088 microprocessor, a varient of the 8088 (i.e. x86 instruction set)
       ☐ As the most popular OS, with the msot available software, CP/M (designed for the intel 8080 and Z80 chips) was expected to be updated to work on the 8087/88 as well
       ☐ IBM talked with Gary Kildall (?) about having an x86 version of CP/M come pre-installed on their PC. But they could not reach a deal, so IBM looked elsewhere
     ☐ Quick and Dirty Operating System (QDOS)
       ☐ Tim Paterson of Seattle Computer Products was one the people eagerly awaiting CP/M for the 8086/88 chip, since sales of their computer kits were falling. But it kept getting delayed by DRI
       ☐ So... he wrote his own version of CP/M for x86
       ☐ He took the CP/M reference manual and duplicated its API, making as exact of a copy as possible for the API, but with a different filesystem (FAT) and other changes.
         ☐ He called it the Quick and Dirty OS (QDOS)
     ☐ Microsoft
       ☐ 1975: Bill Gates and Paul Allen found Microsoft to distribute an implementation of BASIC for the Altair 8800 (the first microcomputer, released the year before)
     ☐ MS-DOS: 
       ☐ Seattle Computer Products was a business partner with Microsoft, as was IBM
       ☐ When Bill Gates heard that IBM was looking for an OS and Gary KIldall had turned them down, he deceaded to licese QDOS, work on it a bit and license it to IBM
       ☐ In 1981, Microsoft purchased QDOS outright for 75,000 and renamed it to Microsoft DISK Operating System, OR MS-DOS
       ☐ MS-DOS became the OS of choice for 8086-like chips
       ☐ Manufactureres would license the OS and add their own device driver, then pre-install it on their machines
     ☐ IBM PC
       ☐ The IBM PC became a runaway hit, highest sale in microcomputer market. 200,000 sold per month by second year. IBM becomes most valuable company in the world by 1984
 ☐ 1984: GUIs and the Mac
   ☐ December 9, 1968: "The mother of all Demos"
     ☐ the computer mouse
     ☐ window-based interface
     ☐ basic hypertext
     ☐ basic word processing
     ☐ video conferencing
     ☐ real-time collaborative document editing
     ☐ Today it is recognized as one of the most important events in the development of modern personal computing
   ☐ Xerox
     ☐ Founded 1906. Sold papers, photocopiers etc.
     ☐ In the late 1960s, with computers on the rise and the notion of the "paperless office" taking hold in some businesses, Xerox got nervous.
     ☐ They wanted to keep pace with the new technology. So, in 1970, they formed a research lab called PARC: Palo Alto Research Center
     ☐ Xerox PARC:
       ☐ PARC attracted the top CS researchers in the country, promising them that hey could work on whatever dream project they wanted.
       ☐ In particular, PARC was situated very close to ARC, at the Stanford Research Park. As ARC's funding began drying up, many ARC researchers moved over to PARC
       ☐ First invention was the laser printer, was THE printer of the time
       ☐ Next, researchers decided to build a computer that could prepare nice documents to be printed on the laser printer
       ☐ They called it the Alto. It was released in 1973
         ☐ A display with full raster-based, bitmapped graphics
           ☐ Each pixel could be turned on and off independently, unlike typical terminals of the time
         ☐ Had a version of Englebart's mouse. Mouse was depicted on screen as a diagonal pointing arrow
       ☐ 1974: Xerox makes Smalltalk for the Alto, a programming language and development environment
         ☐ The first object oriented language. Automatic memory management.
       ☐ Introduced many modern GUI concepts: movable windows with title bars, overlapping windows, icons, popup menus, scroll bars, radio buttons, dialog boxes
         ☐ Desktop metaphor/WIMP
   ☐ The desktop metaphor
     ☐ the idea that a computer monitor is like a "desk" with documents and file folders placed on top of it, which can be opened
     ☐ Small applications called "desk accessories" (alarm clock, note pad, etc.) can also be opened
     ☐ Trash can, etc.
   ☐ WIMP: Windows, icons, menus, pointer
     ☐ Model of human-computer interaction. Developed at PARC
     ☐ Most desktop GUIs use this model (But note that mobile OS's do not!)
   ☐ Failure of PARC
     ☐ The Alto was never commercial product. Only about 2,000 were made, mostly in various Xerox labs and some in universities. Cost was high (32,000)
     ☐ In 1981, Xerox commercialized the idea with an updated computer called the Xerox Star. But it also cost a lot ($17,000) and had very little market impact, even though it had a GUI. (First commercial computer with a GUI)
     ☐ Researchers started leaving
   ☐ Apple
     ☐ Apple founded on April 1 1976
     ☐ It was formed t osell the Apple I, a motherboard to be used as part of a microcomputer.
     ☐ In 1977, they released the Apple II microcomputer. It became the best-selling microcomputer, the first to be able to display color graphics (Could run DOS/CPM)
     ☐ But by 1983, it was surpassed in sales by the IBM PC
     ☐ Steve Jobs and Xerox:
       ☐ In 1978, Apple began working on a new computer called the Lisa, with a home-made oeprating system.
       ☐ Steve Jobs visited PARC in 1979, and was astounded by the Alto's GUI
       ☐ In exchange for some Apple shares, he was allowed to bring his Lisa team to get a tour of PARC and demo of the GUI
         ☐ Many PARC engineers also came over to Apple
     ☐ Lisa OS
       ☐ The Lisa OS expanded on the Alto's GUI
         ☐ First pull-down menu bar (menus at the top)
         ☐ Keyboard shortcuts (e.g., for copy and paste)
         ☐ Trash can
         ☐ Single-button mouse
         ☐ Icons for everything, windows for directories
         ☐ Drag and drop!!
       ☐ Supported pre-emptive shceduling so it could run many programs at once!
         ☐ Remember that CP/M, MS-DOS and others did not have this! Only Unix did, but it was only starting to be updated for microprocessors. (Unix also didn't have support for GUIs)
       ☐ Supported virutal memory with segmentation + swapping!
         ☐ 24-bit virtual addresses: 7 bits (segment number) + 17 (offset). (Total virtual address space 16 MB, while physical RAM was 2MB)
       ☐ Had its own filesystem
    ☐ Lisa Computer
      ☐ The Lisa was released in 1983
        ☐ 1MB of RAM
        ☐ Price: 10,000 (30,000 today)
        ☐ Commercial failure
    ☐ Macintosh
      ☐ Apple realized they had to lower the cost
      ☐ So they designed a new computer: the Macintosh, with lesser components (128KB of RAM vs Lisa's 1MB for example)
    ☐ Mac OS
      ☐ In part because of the smaller memory footprint, the Mac OS did not have the niceties of Lisa:
        ☐ No multitasking
        ☐ No MMU / no memory protection
        ☐ No user/kernel mode! Everythin ran in same mode!
      ☐ But, it still had the GUI (with improvements over Lisa's)
    ☐ Macintosh computer:
      ☐ Released in January 1984, announced by a Super Bowl commercial directed by Ridley Scott
      ☐ Cost: $2,495
      ☐ Like the Lisa, the Macintosh did not do well commercially. It was slow, had no hard drive, no fan, limited software.
        ☐ All software had been written for text-based interface, so it would be a huge challenge to rewrite them for a GUI
        ☐ But the GUI had a huge impact on future UI deisgn
      ☐ Jobs and Wozniak resigned from Apple
      ☐ 1986: The faster Mac Plus comes out. Becomes a hit with creative professionals
    ☐ Susan Kare:
      ☐ Employee #10 at Apple
      ☐ Designed the icons, symbols, typefaces for the Mac UI
        ☐ Trash can, dog-eared paper icon, cursor, Command key, etc.
      ☐ Later went to Microsoft and worked on Windows 3.0, then IBM, Facebook, Pinterest, others
      ☐ Pioneer of the graphical user interface
 ☐ 1985: Windows
   ☐ Designed to run on top of MS-DOS
   ☐ MS-DOS was still the OS, Windows was just an executable that provided a GUI
   ☐ Only needed an 8088 processor and 256k RAM
   ☐ Provided a GUI with (tiled) windows and mouse support
   ☐ Different business model thatn Mac: Windows only sold OS, not a whole machine. And the OS was compatible with many different computers/brands
   ☐ Problems:
     ☐ Slow
     ☐ Limited software compatibility
     ☐ did not come pre-installed like Mac did
   ☐ (Early) Mac vs. Windows
     ☐ Mac: OS build from scratch | Windows: built on top of MS-DOS. Not a standalone OS
     ☐ MAC: OS bundled with Mac computers only ($2,495) | Windows: could be bought for $99 for MS-DOS machines
     ☐ MAC: GUI (WIMP paradigm) | Windows: More primitive form of GUI. Tiled windows only
     ☐ MAC: set the standard for GUI | Windows: laid groundwork for 90s Windows, which would dominate the OS field for personal computers
   ☐ "Inspired" by the Mac
     ☐ Apple complains that Microsoft is coopying their idea. But Microsoft threatens to stop developing Word for Mac, a very popular product
     ☐ 1985: Sculley (Apple CEO) signes deal with Gates as follows
     ☐ In exchange for Microsoft continuing to work on Word for Mac, Microsoft revieces
       ☐ a non-exclusive, worldwide, royalty-free, perpetual, nontransferable license to use [parts ofthe Mac technology] in present and future software programs, and the license them to and through third parties for use in their software programs
     ☐ A huge coup for Microsoft
   ☐ Windows 2.0
     ☐ Overlapping windows. Icons. Keyboard shortcuts. Word and Excel
     ☐ Protected mode, Cooperative multitasking
       ☐ Mac got cooperative multitasking with System 5, released just a couple months before
   ☐ Look and Feel:
     ☐ 1988 Apple files lawsuit accusing of copyright violation. Known as the "look and feel" lawsuit
     ☐ Court ruled that apple could not patent the idea of a GUI, nor the desktop metaphor because they were either not invented by Apple, or they were the only way to express and idea
       ☐ And Apple had also licensed elements of the GUI to Microsoft
     ☐ Gui becomes open source
   ☐ Windows 3.0
     ☐ 1990, $3 million USD release party
     ☐ total overhaul of GUI. Emphasis on icons for file, instead of simple list. Improved multitasking, customizability of appearance (desktop picture, etc.). Better memory use.
     ☐ Huge success
       ☐ Cost was $149 compared to Mac OS which could only be used on an expensive Mac
       ☐ First big success of a GUI-based OS
       ☐ Software industry starts coalescing around Windows
     ☐ Microsoft becomes first software company to reach $1 billion revenue in a single year
 ☐ 1991: Linux
   ☐ Microcomputer OS's
     ☐ Primitive compared to Unix:
       ☐ Single-user, single-tasking
       ☐ Basic filesystems, sometime flat.
       ☐ Not many utilities
     ☐ But they worked on microcmputer's, unlike Unix
   ☐ Unix Advantages
     ☐ Portability: since it was written in C.
     ☐ Modifiability: since it was written in C.
     ☐ Open Source
     ☐ Rich set of development tools: easy to use for developers
     ☐ worked on minicomputers: still widely used in businesses and acadmeic through the 70s and 80s
   ☐ Unix in the 1980s
     ☐ We talked about Unix v6, realeased in 1976, which iwas licensed to commercial users.
     ☐ New versions continued to be realeased in the 70s/80s
     ☐ As part of the license, the licensee could add their own features to Unix and resell it to end-users
   ☐ Unix variants
     ☐ Some of the most popular Unix variants included:
       ☐ System V: A commercial form of Unix by AT&T
       ☐ BSD Unix (Berkeley): vi editor, TCP/IP networking
       ☐ Open Systems (not based on AT&T code)
       ☐ Xenix (Unix for microcomputers, by Microsoft)
     ☐ By 1991, there were over 30 commercial versions of Unix
   ☐ Unix wars
     ☐ Each vendor tried to lock in their customers with custom added features and APIs
       ☐ Software written for one varient would not work on another
       ☐ Customers started having problems with compatibility
     ☐ The struggle between these vendors was called the Unix wars
   ☐ POSIX
     ☐ 1988: IEEE introduces the Portable OS Interface (POSIX) specification, a "lowest common denominator" API that could be easily implemented on popular Unix variants.
     ☐ Many of today's OS's are POSIX-compliant:
       ☐ macOS (fully certified)
       ☐ Android, Linux (mostly)
       ☐ Windows (through the WSL)
   ☐ Factors in the devline of Unix:
     ☐ Compatibility problems between the different variants
     ☐ High license fee: $400+, depending on license type, vs $149 for Windows 3.0
     ☐ Rise of microcomputers (PCs): many of which ran Windows and had extensive software catalogs. Minicomputers became less prevalent
   ☐ MINIX
     ☐ MINIX (mini-Unix): a Unix-like OS created in 1987 for $69. Microkernel architecture
     ☐ Created to teach OS design
     ☐ Creator turned down requests to expand the OS so that its teaching value was not diminished
       ☐ Source code was available, but under a restrictive license (modification/redistribution)
   ☐ Linux
     ☐ Linus Torbalds, Finnish CS student makes a post on the comp.os.minix newsgroup
     ☐ Announces a "hobby" Unix-like kernel
       ☐ Note that "Linux" typically refers to the Linux kernel packaged with open-source system software and libraries (e.g., the bash shell, gcc compiler, etc.)
   ☐ Linux vs MINIX
     ☐ Linux was based on GNU tools/C compiler and MINIX kernel. But had a monolithic kernel as opposed to MINIX.
     ☐ Linux was distributed under GPL license: anyone can run, study, share, modify
       ☐ Torvalds was very open to feedback and collaboration with others
   ☐ X Window System
     ☐ A framework for a GUI environment for Unix-like OS's
     ☐ Open source license
     ☐ 1994: Ported to Linux as its first GUI. Increased popularity of Linux
   ☐ Linux vs Unix
     ☐ As the 1990s progressed, interest begins to coalesce around Linux
       ☐ Its free, open-source and GPL license allowed companies to modify it and re-sell it.
       ☐ Linux development took place through email and newsgroups, unlike prior OS's
       ☐ IBM, Intel and other big companies began to adopt it
     ☐ Today, Linux is the leading Unix-like OS
   ☐ Linux distros
     ☐ Independent, community-driven Linux distributions
       ☐ Debian (emphasis on free software).
         ☐ Ubuntu
       ☐ Arch Linux
       ☐ Red Hat (originally commercial)
       ☐ SUSE (originally Comercial)
       ☐ Slackware
   ☐ Today:
     ☐ Linux kernel has 13 million lines of code
       ☐ 5 patches contributed per hour on average
     ☐ Significant protion of code now contributed by:
       ☐ Commercial Linux distributors
       ☐ Consumer electronics manufacturers
       ☐ Chip vendors
     ☐ 15-20% from independednt developers (more than any company)
 ☐ 1989-1995: NeXTSTEP
   ☐ NeXT
     ☐ 1985: Steve Jobs resigns from Apple
     ☐ Immediately starts a new computer company, hires several important Apple employees
     ☐ Recruits Graphic designer Paul Rand to create a brand identitify for 100,000 dollars
     ☐ They device to make a computer workstation
       ☐ A workstation is a computer designed for scientific applications. it has more powerful (and expensice) hardware than a PC
     ☐ They release the NeXT computer in 1989 at a price of $6500
       ☐ Aimed at the higher education market
   ☐ NeXTSTEP
     ☐ The NeXT computer ran the NeXTSTEP operating system, a Unix-like OS.
     ☐ It was created by a team led by Avie Tevanian, one of the Mach kernel engineers.
     ☐ Based on the MACH kernel and BSD Unix
     ☐ Written in Objective-C
   ☐ Mach
     ☐ A kernel developed at Carnegie Mellon
       ☐ Based on BSD Unix
       ☐ Sponsored by DARPA
       ☐ Goal was to support OS research into distributed parallel computing
     ☐ Mach 3.0: microkernel
   ☐ Objective-C
     ☐ An OOP language that added Smalltalk-style messaging to C (Superset of C)
     ☐ Licences by NeXT and used for their OS and UI libraries
   ☐ NeXTSTEP UI
     ☐ Dock and shelf
     ☐ Better drag and drop
     ☐ Real-time scrolling, window-dragging
   ☐ NeXT users
     ☐ Sold to students, universities and researchers. Kept switching target audience
     ☐ Used by Sir Tim Berners-Lee at CERN to create first web server + web browser, and id software for Doom + Quake
   ☐ The end of NeXT
     ☐ 50,000 machines sold in total
       ☐ 1993: stopped making hardware. Focused on OpenStep (API)
       ☐ OS attracted interest from CS researchers.
       ☐ 1996: stopped updating OS except to port to new architectures 
   ☐ What's NeXT
     ☐ NeXT acquired by Apple in 1997. Steve Jobs becomes interim CEO
     ☐ NeXT vice-presidents replace Apple VPs on new board
 ☐ 1995-2000: Windows 9x
   ☐ Problems with Windows 3.0
     ☐ Based on MS-DOS
     ☐ 16-bit architecture
       ☐ 16 bit memory address, 2^16 addresses. 65,536 (64KB) in virtual address space
     ☐ Unix had multitasking, better networking, etc. Much superior to Windows. Microsoft was worried. they needed a "Unix killer."
   ☐ Windows NT
     ☐ Same UI as Windows 3.0, but new kernel written from scratch -- no longer reliant on MS-DOS.
       ☐ 32-bit architecture (4GB virtual address space)
       ☐ Multi-user, pre-emptive multi-tasking, networking
       ☐ Hybrid kernel (influenced by Mach)
       ☐ NTFS file system instead of FAT (journaling, ACLs, encryption, ...)
     ☐ Positioned as "higher-end" OS compared to Win 3.0
   ☐ Reception - Windows NT
     ☐ Much improved architechure. But:
       ☐ Much higher system requirements than Win 3.0
       ☐ 32-bit software apps very sparse
       ☐ OS very slow
     ☐ Sold about 300,000 copies in first year.
       ☐ Would be updated throughout 90s to keep track with regular Win: Windows NT 3.5, 4.0, 2000. Targeted to businesses
   ☐ Windows 95
     ☐ Consumer-oriented release following up from Win 3.
     ☐ Merged MS-DOS and Windows Products
     ☐ Lower system reqs than Win NT (but less features)
     ☐ 32-bit, pre-emptive multitasking
       ☐ Apps could address up to 2GB of RAM (OS was other 2GB)
       ☐ But only supported 512MB physical RAM
     ☐ Updated GUI
       ☐ Desktop metaphor
       ☐ Start button + taskbar
     ☐ "Plug-and-play"
   ☐ Reception - Windows 95
     ☐ Major commercial success. One million copies shipped in first four days. 65 million in first 16 months
     ☐ Stores held midnight release events
     ☐ Considered one of the most important products in the PC industry
     ☐ By end of 1998, Win 95 had 57.4% of desktop OS market. All Windows version: 82%
   ☐ Why was WIN 95 such a success?
     ☐ Much easier to use than Win 3:
       ☐ Boosted directly into GUi (not MS-DOS)
       ☐ Nicer and easier to use GUI
       ☐ Much more polished
       ☐ Better apps due to architectural improvements
     ☐ Compatibility with big pool of existing software
     ☐ Plug and play: auto configuration of devices
     ☐ Win 95 PCs required to ship with sound, speaker, CD-ROM drive for day one multimedia experience
   ☐ Further Updates:
     ☐ Windows 98: small update. Positive reception.
       ☐ Web integration
     ☐ Windows ME (2000): lots of stability issues
       ☐ Many users decided to stick with 98
   ☐ Halloween documents
     ☐ October 1998: confidential Microsoft memo leaks
     ☐ Reveals Microsoft is worried about Linux and open-source software
     ☐ Strategy was to spread FUD (fear, uncertainty and doubt) about open-source software/Linux
 ☐ 90s Mac OS
   ☐ Mac OS System 1 released in 1984
     ☐ Updates in the later 80's introduced a better filesystem, cooperative multitasking.
   ☐ 1991: System 8. Updated UI. Virtual memory support. 32-bit memory addressing
   ☐ 1997: Mac OS 8. Multi-threaded Finder. Updated UI again.
   ☐ IBM+Microsoft vs Apple:
     ☐ By the late 1980s the IBM PC and its clones were the most popular microcomputers (PC)
     ☐ Apple tried to launch all kinds of different Mac models at different price points and tech specs. But there were way too many models and it confused their customers
   ☐ The rise of the clones
     ☐ In 1995, Apple decided to license the Mac OS (System 7) to other companies, who would pay a license fee and royalty to put Mac OS on their own computers ("clones").
     ☐ But the clones started to eat into Apple's own profits
   ☐ Scrapping the Mac OS
     ☐ Mac OS was stagnating compared to Windows 95/98/NT
     ☐ Apple needed an OS that could compete, one with pre-emptive multitasking, networking, and other features found in Windows
     ☐ They decided to completely scrap the Mac OS and simply buy another company's OS
 ☐ 2001-present Mac OS X
   ☐ Recall: NeXT acquired by Apple, Steve Jobs becomes interim CEO, NeXT vice-presidents replace Apple VPs on new board
   ☐ The comeback
     ☐ Jobs killed the clones
     ☐ Restructures Apple product line. Simple distinction between consumer and pro line.
     ☐ Announces partnership with Microsoft, who commits to Office for Mac for five years. Settles dispute over GUI patent infringement. Internet Explorer ships as default browser
     ☐ Introduces iMac(1998), iPod(2001). Big success. Online store (1997) and retail stores (2001)
   ☐ MAC OS X
     ☐ Completely different code than original Mac OS
     ☐ Build on NeXTSTEP OS (Mac kernel/BSD Unix)
       ☐ Used Objective-C as main programming language
       ☐ Kernel repackaged as Darwin and realeased as open-source
     ☐ New UI with features of both Mac OS + NeXTSTEP
     ☐ Shipped in March 2001
     ☐ With the new kernel came important advances Windows NT had that Mac OS was lacking:
       ☐ pre-emptive multitasking
       ☐ memory protection
       ☐ command line interface (original Mac OS was one of few OS's without a shell)
   ☐ Aqua
     ☐ New UI: Aqua
       ☐ Transparency, shadows, animation, anti-alisasing
       ☐ Dock (from NeXTSTEP)
   ☐ APFS:
     ☐ Introduced in 2017. Suceeded HFS +
     ☐ Optimized for SSDs
       ☐ Copy-on-write (less wear on flash chips)
         ☐ Copies are much faster and don't take up space
       ☐ Uses knowledge of FTL
     ☐ Actually performs worse on mechanical HDs
     ☐ Prioritizes user I/O requests over background activity
     ☐ Snapshots
     ☐ Full disk encryption
     ☐ Does not use journaling! Uses redirect-on-write aproach (similar to shadow paging)
     ☐ User's filesystem automatically converted from HFS+ to APFS on upgrade to macOS 10.13
 ☐ 2001-present: Windows 5-11
   ☐ Windows NT
     ☐ Same UI as regular Windows, but new kernel written from scratch -- no longer relient on MS-DOS
       ☐ multi-user, pre-emptive multi-tasking, networking, NFTS filesystem
     ☐ Positioned as a "higher" OS compared to WIN 3.0
     ☐ Updated throughout 90s to keep track with regular Win: Windows NT 3.5, 4.0, 2000
     ☐ End goal was to unify Win NT and Win 9x
   ☐ Windows XP
     ☐ Their goal of unifying the two different kinds of Windows was accomplished with Windows XP
     ☐ The new OS was based on the WIN NT architecture, and thus no longer relied on MS-DOS
       ☐ It also included a refreshed UI and a huge number of other features
   ☐ Windows Subsystem for Linux
     ☐ Introduced with Windows 10
     ☐ Enables Linux binaries to be run by translating Linux system calls to Win NT kernel system calls
   ☐ Backwards Compatibility
     ☐ Windows is legendary for backwards ocmpatibility
       ☐ Win 95 development: software store bought out
     ☐ "Compabtibility mode"/troubleshooter
     ☐ Apps for Win NT can still run on Win 11
     ☐ Much, much, much worse on Mac
   ☐ Populatiry of Windows
     ☐ Existing market dominance since 1990s
     ☐ Enormous hardware compatibility
     ☐ Comes preloaded on hardware manufactures PCs
     ☐ Continued development
     ☐ Games (DirectX)
   ☐ U.S. v. Microsoft Corp. (2001)
     ☐ Landmark antitrust case. Part of the "browser wars"
     ☐ US accused Microsoft of illegal monopoly on web browser market by bundling Internet Explorer with Windows, prevented OEMs from removing it and made deals with ISPs to promote IE over other browsers
       ☐ This allegedly restricted the market for competing web browsers. Netscape had 80% of web browser marketshare, but was now falling
     ☐ Microsoft argued that a eb browser was an essential component of an OS
     ☐ Ruling: Microsoft's dominance of x86 PC market OS's was a monopoly, and Microsoft had used anticompetitive acts to maintain it, including bundling of IE
       ☐ Microsoft was to be broken up into two companies: one for OS and one for othersoftware
     ☐ Appeal and settlement: Microsoft breakup overturned, but must offer version of Windows without IE and let OEMs install their own browsers
       ☐ IE attains 96% of web browser usage share in 2001
     ☐ Coda: a decade later, IE would fall in rankings to Firefox and then Chrome
     ☐ Antitrust case brought by European Commission (EU)
     ☐ Alleged (in part) that Microsoft had abused its dominant position in the desktop PC OS market by integrating Windows Media Player into Windows
     ☐ Microsoft argued that it provided a benefit to consumers, that they did not need to use it and did not need to pay extra
     ☐ Ruling: Windows Media Player bundling gave Microsoft an unfair advantage over competing media players. Microsoft ordered to offer a version of Windows without Windows Media Player
       ☐ So they made Windows XP N (Windows XP Reduced Media Edition)
     ☐ In 2009: EC announced it would investigate IE bundling
       ☐ So Microsoft released "Windows 7 E" in Europe (no IE)
       ☐ And had to provide a selection screen for a default browser, in random order
     ☐ Goldsetin v. Microsoft Corp (2016)
       ☐ Terri Goldstein of California alleges an unauthorized Win 10 upgrade caused crashes and slowdowns on her PC, leading to big losses at her travel booking business
       ☐ Ruling found in favour of Goldstein and ordered Microsoft to pay $10,000 fine
 ☐ Mobile Operating Systems
   ☐ Mobile OS's in early 2000s
     ☐ PalmOS
     ☐ Windows Mobile
     ☐ BlackBerry OS
     ☐ Symbian
   ☐ iOS
     ☐ Released in 2007
     ☐ "iPhone runs OS X"
       ☐ Based on Darwin kernel and Mac OS
       ☐ Same (or similar) scheduler, virtual memory and FS subsystems
   ☐ New UI paradigm
     ☐ No more desktop
       ☐ "Post-WIMP"
       ☐ No more file system! (or really, hidden)
     ☐ Gesture-based interface called MultiTouch
       ☐ Home screen, with home button
   ☐ App store in 2008, the start of a new (heavily restricted) economy
     ☐ Guidelines:
       ☐ Safety
       ☐ Performance
       ☐ Business
       ☐ Design
   ☐ Thoughts on Flash
     ☐ Adobe Flash was very energy hungry, so in 2010 Steve Jobs announced it wouldn't support Flash
   ☐ Android
     ☐ Founded in 2003
     ☐ Goal was to develop an OS for digital cameras, but later switched to phones. Acquired by Google in 2005
     ☐ At Google, team used Linux kernel as basis. Designed for phones with Qwerty keyboards. But with arrival of iPhone, plans changed to use touch screen
     ☐ Basic Android OS is open source, but hardware manufacturers typically make custom modifications
       ☐ One UI for Samsung, Pixel UI for Google, ZenUI by Asus... etc
     ☐ Google develops a proprietary version with some closed-source software. 70% of Android phones use this version
     ☐ Today, the best-selling mobile OS worldwide
     ☐ Over three million aps on the Google Play Store
       ☐ Several third-party app stores also exist
   ☐ Advantages of Android over iOS:
     ☐ Open ecosystem: manufacturers can use it and modify it for their own devices
     ☐ Price point: because of the open market with many different manufacturers, there are many Android models, some being much cheaper than iPhones
     ☐ Customization: system is much more open to end users
   ☐ Disadvantage:
     ☐ Little "too much freedom":
       ☐ Cases of maleare infiltrating Google Play Apps
   ☐ Microsoft?
     ☐ Windows mobile: 2003-2013
     ☐ Windows Phone: 2010-2020 (Metro UI)
     ☐ Failed to gain any traction
     ☐ Tried to beat both iOS and Android: competing goals
     ☐ Big App Deficit: Not even YouTube
   ☐ DMA
     ☐ the Digital Markets Act is an EU regulation for 'gatekeepers' of digital platforms. Such gatekeepers include Apple, Facebook, Google, Microsoft etc.
     ☐ Objective is to guarantee a level playing field for competition and protect consumers from companies controlling different digital markets
     ☐ Ultimately a precautionary, yet extrememly significant regualtion.
     ☐ One of the regulated sectors is the OS platform
   ☐ iOS compliance with DMA
     ☐ Alternative app marketplaces
     ☐ Web distribution of apps
     ☐ Alternative browser engines
     ☐ Default web browser choice screen
     ☐ and more
     ☐ DMA requires Apple to open up iOS in certain ways which may decrease use safety and ease of use
     ☐ Problem: the iPhone brand is based on safety and ease of use
       ☐ What if a government demands citizens download government apps through a government app store?
   ☐ U.S. v. Apple Inc
     ☐ Landmark(?) antitrust lawsuit filed within a month ago
     ☐ Alleges that Apple has a monopoly on smartphone market and has engaged in anticompetitive acts, including
       ☐ Prohibiting "super apps"
       ☐ blocking cloud-streaming apps
       ☐ surpressing text message quality between iPhone/Android
       ☐ limiting third-parties from competing with Apple Watch
       ☐ blocking third-parties from competing with Apple Pay
     ☐ Proposed relief:
       ☐ Open up iOS to alternative app distribution methods
       ☐ Allow apps to use private APIs for messaging, smartwatches, digital wallets, etc
     ☐ What is part of the OS and what are apps allowed to do?
 ☐ Temple OS
   ☐ Made by terry davis -> diagnosed with schizo.
   ☐ Was apparently told by God to make an OS
   ☐ The OS would be a successor to the Second Temple (a historical place of worship destoryed in the year 70)
   ☐ he said that God instructed that the OS should have a 640x480 GUI resolution and a 16-bit color display
   ☐ Davis spent 10 years building the OS
   ☐ Architecture:
     ☐ Written using Holy C; a derivative of C
     ☐ 64-bit, non preemptive multi-tasking, multi-core, single address space, single ring
     ☐ GUI resembles early DOS-based OS's
     ☐ Over 100,000 lines of code
     ☐ Open source
   ☐ God's oracle
     ☐ One program built into the OS was designed to generate pseudo-random text and could be run by pressing F7
     ☐ Davis believed that these were codes messages from God
   ☐ Other features
     ☐ shell language is also Holy C; all shell commands are passes through the compiler
     ☐ System-wide autocomplete
     ☐ All text files (even source code) use a special format which allows for text, images and links in the same file
     ☐ Davis also wrote a large number of games and apps for his OS
   ☐ Reception
     ☐ Many online made fun of the OS
     ☐ But it was a very impressive feat for one person to do
 ☐ Red Star OS
   ☐ North Korea
     ☐ Untill 2002, North Korea used Windows
     ☐ Then, in accordance with a direction from Kim Jong Il, they decided to make their own OS
   ☐ A Linux distribution
     ☐ Specifically, SELinux (Security Enhanced Linux)
       ☐ Enforces mandatory access controls, which can prevent a user from making certain modificiations to the OS
   ☐ GUI made to resemble Windows XP, then 8, then OS X
   ☐ Has apps that are mostly just knockoffs of Microsoft/Apple's


The Process:
  ☐ Direct Execution -> allow one process to run until it's done
    ☐ No overhead since process has full control of the CPU, can do anything it wants and will not be paused
    ☐ Since full control of CPU, can hog all resources
    ☐ Can corrupt files on hard drive, crash OS, etc
    ☐ No isolation between processes
  ☐ Kernel:
    ☐ Subset of the OS with special rights and responsibilities
    ☐ Trust with full hardware access
    ☐ Has its own stack and special subset of instructions that only it can perform
  ☐ Processor Modes:
    ☐ User mode:
      ☐ code has restrictions: it cannot directly issue I/O requests. If it tried to do so, the processor would raise an exception and the OS would likely then kill the process
      ☐ No Privileged instructions
      ☐ No direct access to all of memory
      ☐ No direct access to devices
    ☐ Kernel Mode:
      ☐ code can do anything it likes, what the OS runs in
      ☐ Priviliged Instrictions
        ☐ Set mode bit
      ☐ Direct access to all of memory
      ☐ Direct access to devices
 ☐ System Calls:
   ☐ To safely switch between kernel and user mode, the OS exposes certain sensitive operations through a set of system calls, or syscalls for short.
   ☐ A system call is a special function call that goes into OS code and runs on the CPU in kernel mode
     ☐ Only in kernel mode can sensitive operations (like access to hardware be performed)
   ☐ The API to manage processes has a standard called the POSIX API: a standard set of system calles an OS must implement
   ☐ Most modern OS's are POSIX compliant
   ☐ Programs written for the POSIZ API can run on any POSIX compliant OS, ensuring program protability
 ☐ Perfmorming a system Call:
   ☐ Each OS has a kernel API that wraps system calls in function calls, to make them easier to use and more protable
   ☐ Then, the kernel API is then wrapped by program language libraries (like libc)
     ☐ the C library function printf invokes the write system call
     ☐ although libc makes system calls look like regular function calls, they are not function calls. Instead, they are a transition between the user and kernel (and thus much more expensive)
 ☐ Process APIs
   ☐ Some system calls involve management of processes. This is knows as the Process API and includes:
     ☐ create (creates a process)
     ☐ destroy (destroys a process)
     ☐ wait
     ☐ misc
     ☐ get info
 ☐ fork
   ☐ fork is a system call that can create a new process
   ☐ calling fork creates an (almost) exact copy of the calling process, which starts running at the line directly after the fork call. It is called the child process, while the initial process (which is still running) is called the parent.)
   ☐ the child will have its own copy of the address space, registers, PC, etc. One difference from the parent: the value it recieves from the fork call -- it recieves 0, while the parent receives the PID of the newly-created child.
     ☐ output of fork program is not detministic
   ☐ In Unix-like OS's, every process is created by forking from another process
 ☐ wait
   ☐ wait is a system call that lets a parent wait until a child process has finished running before continuing to execute. By using wait, we make out fork program deterministic
   ☐ will not return until the child has run and exited
   ☐ waitpid also exists which waits for a child process with a specific PID to end
 ☐ exec
   ☐ exec is a system call that lets us run a program different from the current (calling) program.
   ☐ it transforms the current process into the new one: it overwrites the ucrrent instructions in memory with the new program's instructions (after loading them from disk), and re-initializes the heap and stack and other parts of the program's memory space.
   ☐ exec never returns
 ☐ fork, wait, and exec together:
   ☐ these 3 are essential in building a shell
   ☐ When typing a command into the shell, it calls fork to make new child process, exec to make the child run the commnad, and wait to wait until the child completes before printing out a new prompt.
 ☐ Traps
   ☐ A trap is generated by the CPU as a result of error
     ☐ Divide by zero
     ☐ Execute privileged instruction in user mode
     ☐ Illegal acces to memory
   ☐ Works like an "involuntary" system call
     ☐ Sets mode to kernel mode
     ☐ Transferl control to kernel
   ☐ return-from-trap
     ☐ the trap and return-from-trap instructions are special. They do the following all at once:
       ☐ jump into kernel code (or process code, for return-from-trap)
       ☐ change the processor mode (user to kernel, or kernel to user)
       ☐ load the kernel stack (or process stack, for return-from-trap)
 ☐ Interupt:
   ☐ an interupt is generated by a device needing attention
     ☐ packet arrived from the network
     ☐ Disk I/O completed
     ☐ etc.
   ☐ Also works like an involuntary system call
 ☐ OS control flow
   ☐ the OS is an event-driven program. It only runs when a trap, interrupt or system call is generated
   ☐ In each of these cases, the processor will effect a mode switch from user mode to kernel
   ☐ Once in kernel mode, the kernel can perform whatever operation, then return control back to the process, using a special return-from-trap instruction
 ☐ Limited Direct Execution
   ☐ Recall direct execution: one process runs on CPU until done, and has full control of hardware (CPU, memory, disk)
   ☐ Limited direct execution: when a proces wants to do something sensitive, it issues a system call, which enters into the OS and then comes back using a return-from-trap. Once done, the process exits, which also traps back into the OS
 ☐ OS structure:
   ☐ User/OS separation -> "monolihthic OS"
     ☐ Downside:
       ☐ The OS is a huge piece of software; Millions of lines of code and growing
       ☐ If something goes wrong in kernel mode, most likely, machine will halt or crash.
       ☐ Incentive to move stuff out of kernel mode
     ☐ No need for entire OS in kernel
       ☐ Some pieces can be in user mode
         ☐ no need for privileged access
         ☐ no need for speed
     ☐ Example: daemons
       ☐ system log
       ☐ printer daemon
       ☐ Etc.
   ☐ Microkernel
     ☐ Absolute minimum in kernel mode
       ☐ interprocess communication primitives
     ☐ All the rest in user mode
     ☐ In practice, have failed commercieally (except for niches)
     ☐ The "systems programs" model has won out
 ☐ Single vs multiple process system
   ☐ Process operation
   ☐ From the point of view of an OS, a process does two things:
     ☐ either it computes (using the CPU), or
     ☐ it does I/O (using a device)
   ☐ In single process systems, process must be completed before moving onto next
     ☐ causes long wait times for processes waiting for the previous to complete
     ☐ Inefficient due to long CPU idle times and bad interactivity (can't do anything while a process is running)
   ☐ The issues of a single process system can be addressed by using a multi-process system.
     ☐ In multi-process systems, while a process waits for an I/O request to complete, another process can use the CPU!
     ☐ During the idle times of a process, the CPU will move to the next process until the CPU can resume the original process again
       ☐ High utilization, short CPU idle times
 ☐ Process States
   ☐ Running: executing instructions
   ☐ Ready: ready to run, but not executing at the moment
   ☐ Blocked: waiting for some event to take place (e.g., for OS to perform I/O request) until it can become ready
   ☐ See Lec 2. slide 89 of the Process State Diagram for Multiprocessing System
 ☐ Process switching
   ☐ Multiprocessing
     ☐ In a system that supports multiple processes, there are two important considerations:
       ☐ how to switch between processes, and 
       ☐ how to determine which process to run (scheduling)
     ☐ If a process is running on the CPU, then the OS is not. But if it is waiting on I/O, then another process should run
       ☐ Also, a process should not be able to run forever
     ☐ 2 problems: regaining control and context switching
     ☐ Problem 1: regaining control:
       ☐ How can the OS regain control of the CPU so that it can switch to another process?
       ☐ Two approaches:
         ☐ non-preemptive scheduling ("coorperative multitasking")
         ☐ preemptive scheduling (noncooperative)
       ☐ Cooperative Approach:
         ☐ The OS trusts processes to behave responsibly
         ☐ Special syscall "yield" lets the process give back control to the CPU... when it wants
           ☐ Note: if the program performs an illegal operation (like a bad memory access), OS also regains control
         ☐ Known as non-preemptive scheduling
         ☐ Problem: A process could run forever, locking all other processes out
         ☐ Solution: Timer interupts, which is resolved by the alternative "Non-cooperative approach"
       ☐ Non-cooperative approach:
         ☐ Timer interrups
           ☐ A timer device can be programmed to raise an interrupt every so often, at which point the process is forcibly paused and the OS executes and interrupt handler, regaining control
         ☐ Known as preemptive scheduling
       ☐ Non preemptive vs. Preemptive
         ☐ Non-preemptive:
           ☐ Process can monopolize the CPU
           ☐ Only useful in special circumstances
         ☐ Preemptive
           ☐ Process can be thrown out at any time
           ☐ Usually not a problem, but sometimes it is
         ☐ Intermediate solutions are possible

     ☐ Problem 2: context switching:
       ☐ When the OS regains control, it can decide whether to continue running the current process or switch to another
         ☐ This decision is made by the OS scheduler
       ☐ If the OS decides to switch, it executes a context switch
   ☐ Process info:
     ☐ Consists of:
       ☐ Code
       ☐ Stack
       ☐ Heap
       ☐ Registers (including program counter)
       ☐ MMU info
     ☐ Code, stack and heap are already stored in memory private to a process
     ☐ But registers and MMU info are stored in a place for the current running process
   ☐ Process switch P1 -> P2:
     ☐ Save registers (P1) to somewhere
     ☐ Restore registers (P2) from somewhere
     ☐ Where to save and restore from? -> see PCB
   ☐ Process Control Block (PCB)
     ☐ The kernel maintains an array of process control blocks (PCB) which contains information about current processes, including:
       ☐ Process identifier (unique id)
       ☐ Process state
       ☐ Space to support process switch (save area)
     ☐ The PCB array is indexed by a hash of the PID
   ☐ Process switch P1 -> P2 (with PCB in mind):
     ☐ Save registers -> PCB[P1].SaveArea
     ☐ Restore PCB[P2].SaveArea -> registers
     ☐ Run return-from-trap instruction
   ☐ Process siwtching is slow:
     ☐ A process switch is an expensive operation!
     ☐ Requires saving and restoring lots of stuff (registers, PC, MMU, info, etc.).
     ☐ Has to be implemented very efficiently and used with care.
   ☐ Context switching overhead
     ☐ Cost of saving registers
     ☐ Cost of scheduler to determine which job to run next
     ☐ Cost of restoring registers
     ☐ Cost of flushing caches (L1, L2, L3, TLB)
 ☐ Process Scheduling
   ☐ Multiprocessing
     ☐ In a system that supports multiple process, there are two important considerations:
       ☐ how to switch between processes, and 
       ☐ how to determine which process to run (scheduling)
   ☐ Scheduling
     ☐ The OS scheduler decides when (and which) to run a ready process
       ☐ If a ready process is run, we say it has been scheduled
       ☐ Or if it running and moves to ready, we say it has been de-scheduled
     ☐ The role of the scheduler:
       ☐ think of scheduler as managing a queue
       ☐ when a process is ready, it is inserted into the queue, according to a scheduling policy
       ☐ scheduling decision: run head of queue
     ☐ Implementation:
       ☐ Remeber running process
       ☐ Maintain sets of queues
         ☐ (CPU) ready queue
         ☐ I/O device queue (one per device)
       ☐ PCBs sit in queues
     ☐ How does athe scheduler run?
       ☐ The scheduler is part of the kernel, so it can run when the kernel runs, i.e. when:
         ☐ process starts or terminates (system call)
         ☐ running process performs an I/O (system call)
         ☐ I/O compeltes (I/O interrupt)
         ☐ timer expires (timer interrupt)
         ☐ a trap occurs
     ☐ Metrics:
       ☐ a scheduling policy determines which ready process will be scheduled next
       ☐ To compare scheduling policies, we need scheduling metrics
 ☐ Scheduling Metrics and Policies
   ☐ Workload:
     ☐ The workload is the set of running processes, or jobs. We will make the following assumptions about jobs:
       ☐ Each job runs for the same amount of time
       ☐ All jobs arrive at the same time
       ☐ Once started, each job runs to completion
       ☐ All jobs only use the CPU
       ☐ The run-time of each job is known
   ☐ Turnaround time:
     ☐ The time it takes for a job to complete: a measure of performance
     ☐ Turaround = Time of completion - Time of arrival
       ☐ Since we assume all jobs arrive at the same time, Time of arrival will be 0 for now
   ☐ Response time:
     ☐ Response time: the difference between the time the job arrives and the time it is first scheduled
     ☐ Response = Time of first run - Time of arrival
       ☐ So that users can see some kind of progress on their job without waiting too long
   ☐ Types of jobs: Interactive vs. Batch
     ☐ What makes a good scheduler for interactive or batch?
     ☐ Interactive = you are waiting for the result
       ☐ E.g., browser, editior
       ☐ Tend to be short
     ☐ Batch = you will look at result later
       ☐ E.g., supercomputing center, offline analysis
       ☐ Tend to be long
     ☐ What makes a good scheduler for interactive?
       ☐ Short response time
       ☐ Response time = wait from ready -> running
         ☐ Initial_schedule_time = arrival_time
     ☐ What makes a good scheduelr for batch?
       ☐ High throughput
       ☐ Throughput = number of jobs completed
         ☐ minimize scheduling overhead
           ☐ Reduce number of ready -> running switches
     ☐ Other Concerns:
       ☐ Fiarness: who gets the resources?
         ☐ We want an equitable division of resources
       ☐ Starvation: how bad can it get?
         ☐ Lack of progress by some job
       ☐ Oberhead: how much useless work?
         ☐ Time wasted switching between jobs
       ☐ Predictability: how consistent?
         ☐ Low variance in respnse time for repeated requests
     ☐ Basic Scheduling Algorithms
       ☐ FIFO (first in, first out)
         ☐ the most basic we can implement. Jobs are executed by the time they arrived in the system. (They are inserted at tail of queue)
         ☐ By definition, non-preemptive
         ☐ Pros and Cons:
           ☐ Low overhead - few scheduling events
           ☐ Good throughput
           ☐ Uneven response time - stuck behind long job
           ☐ Extreme cases - process monopolizes CPU
       ☐ SJF (shortest job first)
         ☐ Improves average turnaround time
         ☐ Optimal scheduling algorithm if all jobs arrive at the same time
         ☐ Pros and Cons:
           ☐ good response time for short jobs, but only if all jobs arrive at same time
           ☐ Can lead to starvation of long jobs
       ☐ STCF (Shortest Time To Completion First)
         ☐ Under the circumstance where jobs DONT NEED to run until done and the scheduler can preempt a job (pause one, and start another), this is the optimal algorithm
       ☐ Round-Robin
         ☐ Instead of running jobs to completion, run a job for a time slice (aka scheduling quantum), then switch to the next job in the queue (the one that hasn't been run for the longest time)
         ☐ Continue until all are done
         ☐ Pros and Cons:
           ☐ Good compromise for long and short jobs
           ☐ Short jobs finish quickly
           ☐ Long jobs are not postponed forever
           ☐ No need to know job length
         ☐ Picking a scheduling quantum:
           ☐ Too small
             ☐ Many scheduling events
             ☐ Good response time
             ☐ Low throughput
           ☐ Too large
             ☐ few scheduling events
             ☐ Good throughput
             ☐ Poor response time
           ☐ Typical value 10 milliseconds

The Memeory:
  ☐ Memory: the dream
    ☐ What every programmer would like is a:
      ☐ private,
      ☐ infinitely large,
      ☐ infinitely fast,
      ☐ nonvolatile, and 
      ☐ cheap memory
  ☐ ASSUMPTION START FOR LECTURE: All of a program must be in main memory
  ☐ Memory allocation: roadmap
    ☐ Where to locate the kernel?
    ☐ Cocept of address space
    ☐ Virtual vs physical mempory
    ☐ Goals of memory management
  ☐ Allocating Main memory for kernel:
    ☐ Almost always low in memory (0 - 100) because Interrupt vectors are in low memory
  ☐ The address space:
    ☐ The OS provides an abstraction of physical memory to each process, called the address space
    ☐ The address space of a pcoress contains:
      ☐ the code (instructions)
      ☐ the stack (local vars, params, return vals), and
      ☐ the heap (dynamically allocated memory)
  ☐ The problem:
    ☐ The address space for each process starts at address 0. But the address in this space are virtual addresses, not real addresses into physical memory
    ☐ How can the OS virtualize the single, physical memory, so that each process has its own private address space, with virtual addresses translated into actual physical addresses
  ☐ Virtual vs. Physical Address space
    ☐ Virtual/logical address space = What the program thinks is its memory
      ☐ Address generated by program/CPU
    ☐ Physical address space = where the program actually is in physical memory
      ☐ Address that the memory sees
    ☐ Must map/translate every access from the CPU to memory
  ☐ Goals:
    ☐ Transparency: should be invisible
      ☐ Processes shouldnt need to know that memory is shared
      ☐ Should work regardless of number and/or location of proceses
      ☐ Programmer shouldn't have to worry about:
        ☐ Where their program is in memory, or
        ☐ where or what other programs are in memory
    ☐ Protection: a process should not be able to do bad things
      ☐ Privacy: Should not be able to read/write the memory of another process or the kernel
      ☐ Should not be able to corrupt OS or other porcesses
    ☐ Efficiency: in terms of time and memory space
  ☐ Memory Management Unit (MMU)
    ☐ Provides mapping virtual-to-physical
    ☐ Provides protection at the same time
    ☐ Hardware!
  ☐ Size of address sapces:
    ☐ Maximum virtual address space size
      ☐ Limited by address size of CPU
      ☐ Typically 32 or 64 bit addresses
      ☐ So, 2^32 = 4 GB, or 2^64 = 16 exabytes (BIG!)
    ☐ Physical address space size:
      ☐ Limited by size of memory
      ☐ Nowadays, order of tens/hundreds of GB
  ☐ Stack and heap:
     ☐ Motivation for dynamic memory allocation
       ☐ Why do processes need dynamic allocation of memory:
         ☐ Do not know amount of memory needed at compile time
         ☐ Must be pessimistic for static memory allocation.
         ☐ If they statically allocate enough for worst possible case, storage is used inefficiently
         ☐ Recursive procedures:
           ☐ Do not know how many times procedure will be nested
         ☐ Complex data structures: lists and trees
     ☐ Stack and heap are 2 types of dynamic allocation
     ☐ Stack:
       ☐ OS uses stack for procedure call frames (local variables and parameters)
       ☐ Memory is freed in opposite order from allocation
       ☐ Ex:
         ☐ alloc(A);
         ☐ alloc(B);
         ☐ alloc(C);
         ☐ ...puts on stack, reading from bottom up: A->B->C
           ☐ C on top, A on bottom
         ☐ So, to free memory, need to call free(C) first, freeing from the top of the stack
       ☐ Simple and efficient implementation:
         ☐ Pointer separates allocated freed space
         ☐ Allocate: Increment pointer
         ☐ Free: Decrement pointer
       ☐ No fragmentation
       ☐ Stack management done automatically
     ☐ Heap:
       ☐ Allocate from any random location
         ☐ Heap consists of allocated areas and free areas (holes)
         ☐ Order of allocation and free is unpredictable
       ☐ + Works for all Data Structres
       ☐ - Allocation can be slow
       ☐ - Fragmentation
       ☐ Programmers manage allocations/deallocations with library calls (malloc/free)
 ☐ The Memory API
   ☐ The stack
     ☐ Allocations and deallocations managed by the compiler
   ☐ The heap
     ☐ You handle the allocations and deallocations
     ☐ malloc() allocates memory on the heap. You pass it a size, and it gives you a pointer to the newly-allocated space (or 0 on failure).
       ☐ The size is usually given by sizeof()
       ☐ It returns a pointer of type void*, so we must cast it
       ☐ double *d = (double *) malloc(sizeof(double));
     ☐ free() deallocates memory on the heap. You pass it a pointer to the memory you want to deaalocate
       ☐ int *x = malloc(10 * sizeof(int));
       ☐ // ...
       ☐ free(x);
 ☐ Address Translation
   ☐ Virtual addresses are passed through the MMU and translated (mapped) to physical addresses
     ☐ This occurs during an instruction fetch, load or store
   ☐ We will look at a simple example of a translation, then see three difference translation (mapping) approaches
     ☐ This lecture assumes that the process address space is placed contiguously in physical memory, and that its size is smaller than the physical memory
 ☐ Mapping Schemes
   ☐ Base and bounds
     ☐ Main idea: use two hardware registers in the MMU: the base register and the bounds register
     ☐ It will let us place a process' address space anywhere in physical memory, while maintaining isolation of the address space from others'
     ☐ When a process starts, the OS will decide where in physical memory the address space of that process should start. It then sets the base register to that address.
     ☐ When the process references a virtual memoryy address, the MMU translates it to a physical address by incrementing it by the base address
     ☐ The bounds register stores the size of the process' address space
     ☐ The MMU will check that any memory reference used by the process is within bounds, i.e., that the virtual address is between 0 and the bounds register
       ☐ If not, the MMU generates a trap, so that the OS can kill the process for trying to access memory outside of its address space
     ☐ Virtual address space
       ☐ linear address space from 0 to MAX
     ☐ Physical address space:
       ☐ linear address space from BASE to BOUNDS = BASE + MAX
     ☐ The base and bounds registers are kept in the MMU
     ☐ Modifications to the base and bounds are privileged operations, i.e., they can only be changed in kernel mode
     ☐ Main memory:
       ☐ Regions in use
       ☐ "Holes" are regions not in use, so a new process must go in a hole
     ☐ Free List:
       ☐ A list of the range of the physical memory not in use
     ☐ Problem: External Fragmentation
       ☐ Small holes become unusable, part of memory can't be used
     ☐ Problem: Internal fragmentation
       ☐ Big chunk of "free" space in virtual address space
       ☐ "free" space takes up physical memory
       ☐ Inefficient
   ☐ Fragmentation:
     ☐ Internal fragmentation: Typical paper book is a collection of pages (text divided into pages). When a chapter's end isn't located at the end of page and new chapter starts from new page, there's a gap between those chapters and it's a waste of space — a chunk (page for a book) has unused space inside (internally) — "white space"
     ☐ External fragmentation: Say you have a paper diary and you didn't write your thoughts sequentially page after page, but, rather randomly. You might end up with a situation when you'd want to write 3 pages in row, but you can't since there're no 3 clean pages one-by-one, you might have 15 clean pages in the diary totally, but they're not contiguous
   ☐ Segmentation
     ☐ What is a segment:
       ☐ Anything we want it to be
       ☐ A segment is a contiguous portion of the address space of a particular
     ☐ Examples:
       ☐ Code
       ☐ Heap
       ☐ Stack
     ☐ Segmentation: instead of a single base/bounds pair per process, we will have a base/bounds pair per segment of the address space
     ☐ then, we can place each segment in different places of the physical memory
     ☐ Virtual Address Space
       ☐ Two-dimensional
       ☐ Set of segments 0 ... n
       ☐ Each segment i is linear from 0 to MAXi
     ☐ Physical Address Space
       ☐ Set of segments, each linear
     ☐ Attempting to refer to an address outside of a segment causes a segmentation fault
     ☐ The MMU must know which segment is associated with each address
     ☐ One way to do this is to set aside the first two btis of each address and use them to indicate the segment (00/01/10).
     ☐ Course-grained = only a few segments (code/stack/heap)
     ☐ Fine-grained = a large number of smaller segments
     ☐ Pros of segmentation:
       ☐ Unlike base and bounds, avodis assigning an entire address space of code, heap, stack and unused virtual memory to process
         ☐ Instead, heap and stack can grow into unused physical memory
       ☐ Minimal overhead
     ☐ Problem: External Fragmentation
       ☐ Physical memory will quickly become full of little holes of free space
       ☐ Makes it difficult to allocate new segments, or grow existing ones
     ☐ Possible Solution: compact physical memory by rearranging existing segments (copything them into a contiguous block).
       ☐ But it's expensive, and makes growing segments harder
     ☐ Segmentation ~= multiple base-and-bounds
       ☐ No internal fragmentation inside each segment
       ☐ External fragmentation problem is similar
         ☐ Pieces are typically smaller
     ☐ Sharing Memory between processes
       ☐ Why would we want to do that?
       ☐ For instance,
         ☐ Run the same program twice in different processes
         ☐ May want to share code
         ☐ Read the same file twice in different processes
         ☐ May want to share memory corresponding to file
       ☐ Sharing not possible with base and bounds, but is possible with segmentation
    ☐ SEGMENTATION PROVIDES EASY SUPPORT FOR SHARING
      ☐ Create segment for shared data
      ☐ Add segment entry in segment table of both processes
      ☐ Points to hsared segment in memory
      ☐ Extra hardware support is needed for form of Portection bits.
        ☐ A few more bits per segment to indicate permissions of read, write, and execute
   ☐ Paging (see next note section)

Paging:
  ☐ Free Space Management:
    ☐ The problem (continuing from Segmentation notes):
      ☐ How to deal with external fragmentation? Can it be minimized? What are the overheads involved?
    ☐ Free list:
      ☐ A free list contains a set of elements that describe the free space remaining in the heap
    ☐ Low-level mechanisms
      ☐ Splitting memory
      ☐ Coalescing memory
      ☐ Keeping track of allocated chunks
      ☐ Memory for the free list
    ☐ Splitting (Lecture 5 Slide 29 Example)
    ☐ Coalescing:
      ☐ when memory is freed (e.g., through free()), the allocator coalesces (merges) that memory with any adjacent free chunks on the free list
      ☐ coalescing free space helps to reduce external fragmentation a little bit
      ☐ can be done when memory is freed, or it can be deferred until a later point
    ☐ Tracking size of allocations
      ☐ when free() is called, we don't need to pass the size of memory to free. The allcator keeps track on its own
      ☐ to do so, each allocated piece of memory has a header block (So if the caller asked for N bytes, the allocator must find a chunk of size N + sizeof header)
    ☐ Free list in memory
      ☐ The free list itself must be placed in memory. It is represented as a linked list
      ☐ Like allocated memory, each free chunk in memory will begin with a header, in this case being a node of this type
    ☐ One free chunk
      ☐ Assume that we are managing an empty heap of 4KB, and that the header for the free chunk will be 8 bytes. the head pointer point to the start of the header
    ☐ One allocation, one free chunk
      ☐ Upon an allocation request, a free chunk of sufficient size will be split into two. the first chunk will fit the header and request, and the second will be a free chunk (also with a header)
    ☐ Allocation strategies:
      ☐ When a piece of memory is requested, the allocator must choose which chunk of free space to use for the new allocaiton. Some possible strategies: 
        ☐ Best fit
          ☐ Find free chunks that are as big or bigger than requested size
          ☐ Return the one that is the smallest (the "best-fit" chunk)
          ☐ Tries to reduce wasted space
          ☐ DrawbackL: needs to look through all free chunks
        ☐ Worst fit
          ☐ Opposite of best fit: find largest free chunk, split it and return the first part and keep the second (large) chunk on the free list
          ☐ Overall effect: after frees, lots of big chunks are left in memory instead lots of small chunks.
          ☐ Drawback: full seach of free space still
          ☐ Also leads to excess external fragmentation
        ☐ First fit
          ☐ Finds the first free chunk that is big enough
          ☐ Fast: does not need to search through all
          ☐ Drawback: Beginning of the free list gets polluited with small objects
        ☐ Next fit
          ☐ Like first fit, but maintains a pointer to the location in the list where the last search ended, then starts looking there for the next search.
          ☐ Thus allocations are made more uniformly through the list, so the beginning is not crowded with small objects
          ☐ Exhaustive search also again avoided
  ☐ Paging:
    ☐ Space management:
      ☐ Segmentation: chop space up into variable-sized pieces
        ☐ leads to fragmentation
      ☐ Paging: chop space into fixed-sized pieces
        ☐ A process' address space (code, heap stack) is divided into fixed-sized units called pages
        ☐ Physical memory: an array of fixed-sized slots called page frames
    ☐ Virtual Address Space
      ☐ Linear from 0 up to a multiple of page size
    ☐ Physical Address Space
      ☐ Noncontiguous set of frames, one per page
      ☐ Frame size is same as page size
    ☐ Paging advantages
      ☐ Flexibility: no assumptions needed about how heap and stack are used, or which direction they grow, etc.
      ☐ Simplicity: free list of free pages. Easy to slot in pages
    ☐ A page table is a data structure kept by the OS for each process. It stores the mapping between each virtual page of the address space and its place in physical memory
      ☐ It is used for address translations from virtual page to physcial page frame
    ☐ Paging: Virtual Address
      ☐ Two components in the virtual address
        ☐ VPN: virtual page number
        ☐ Offset: offset within the page; Page size = 2^(offset)
      ☐ See Lecture 5 Slide 104 for address translation examples
      ☐ MMU for paging:
        ☐ Page table:
          ☐ Data structure used to map the virtual address to physical
          ☐ Indexed by a page number
          ☐ Contains frame number of page in memory
          ☐ Each process has a page table
        ☐ Also need: Pointer to page table in memory, Length of page table
      ☐ Porblem?
        ☐ Address space sparsely used
        ☐ Access to unused portion will appear valid
        ☐ Would rather have an error
      ☐ Solution Valid/Invalid Bit
        ☐ Page table has length 2^p
          ☐ Page table does not cover the entire possible virtual address space, only the pages that the process has allocated
        ☐ Have valid bit in each page table entry
          ☐ Set to valid for used portions of address space
          ☐ Invalid of unused portions
          ☐ (this is the most common approach)
    ☐ Main Memory allocation with paging:
      ☐ Logical address space: fixed size pages
      ☐ Physical address space: fixed size frames
      ☐ New process:
        ☐ Find frames for all of process's pages
      ☐ Easier to find memory than with segmentation
        ☐ Fixed size
    ☐ Internal Fragmentation in Paging:
      ☐ With paging
        ☐ Address space = multiple of page size
      ☐ Part of last page may be unused
      ☐ Not a big problem with reasonable page size
    ☐ In Reality:
      ☐ Base-and-bounds only for niche
      ☐ Segmentation abandoned
        ☐ High complexity for little gain
        ☐ Effect approximated with paging + valid bits
      ☐ Paging is now universal
    ☐ Faster translations
      ☐ Speed problem (See lecture 6 slide 26): Paging address translation performance
      ☐ Solution: a new cache in hardware
        ☐ to speed up paging, we add a cache into the MMU known as the translation-lookaside-buffer (TLB)
        ☐ Upon each virtual memory reference, we check the TLB to see if the requested translation is there. If so, we can use it without consulting the page table
      ☐ TLB:
        ☐ If the TLB holds the translation for the VPN (TLB hit), we can use the TLB entry to get the page frame number
        ☐ If the TLB does not hold the translation (TLB miss), the hardware accesses the page table to get the translation, adds to TLB, and retries
          ☐ Misses are expensive, so we want to avoid them as much as possible
        ☐ How to make TLB fast?
          ☐ Use associative memory (special hardware)
          ☐ Regular memory
            ☐ Lookup by address
          ☐ Associative Memory
            ☐ Lookup by contents
            ☐ Lookup in parallel
          ☐ See lecture 6 slide 40 for diagram
        ☐ TLB size
          ☐ Associative memory is very expensive
          ☐ Therefore, TLB small (64 - 1,024 entries)
          ☐ Want TLB hit-rate close to 100%
        ☐ TLB hitrare and locality
          ☐ Want TLB hit rate close to 100%
          ☐ To do this, try to take advantage of locality
          ☐ Temporal Locality
            ☐ An instruction or data item that has been recently accessed will likely be re-accessed soon in the future
          ☐ Spatial Locality
            ☐ If a program accesses memory at address x, it will likely soon access memory near x
        ☐ See accessing an array example in Lectur 7 around slide 50
      ☐ Revisiting Process Switching
        ☐ Suppose:
          ☐ Process P1 is running
          ☐ Entry (pageno, frameno) in TLB
          ☐ Switch from P1 to P2
          ☐ P2 issues virtual address in page pageno
        ☐ P2 accesses P1's memory!
        ☐ Or unable to distinguish between P1 and P2 mapping
        ☐ Solution 1: Add valid bit/column to TLB table. Invalidate (set valid column to 0) all TLB entries. That way, new entries that have the same pageno as invalid entried, there is a distinction between processes
        ☐ Solution 2:
          ☐ Add process identifieer to TLB entries
            ☐ Match = match on pid AND match on pageno
            ☐ :( makes TLB more complicated and expensive
          ☐ Process switch
            ☐ Nothing to do
            ☐ Cheaper
          ☐ All modern machines have this feature
      ☐ Cache replacement:
        ☐ As with any cache, TLB will become full at some point
        ☐ How to decide which entry to be replaced with new one?
          ☐ Goal: to minimize miss rate
          ☐ Main idea: take advantage of locality
          ☐ See some later notes
    ☐ Page table sizes:
      ☐ Space problem: Page tables can get large
      ☐ Dealing with large virtual address spaces
        ☐ 4kB size pages:
          ☐ Typical value for page size; normally, this value is given as part of the problem statement, or you'd have enough information todeduce it.
          ☐ 4kB size pages means 12 big page offset:
            ☐ Page size = 2^offset Bytes.. Why?
              ☐ Every byte needs to have an address and
              ☐ We can represent 2^offset addresses using offset bits
              ☐ Therefore, Page size = 4KB = 4 * 1024B = 2^2 * 2^10 = 2^12, so offset has 12 bits
        ☐ REVISIT QUIZ AT END OF LECTURE 6
      ☐ How to make Page Table smaller?
        ☐ Big pages
          ☐ Advantage: easy to implement
          ☐ Disadvantage: Larger pages have higher internal fragmentation
          ☐ Most Systems use 4KB or 8KB pages in common case
        ☐ Segmentation + Paging
          ☐ Divide address space into segments (code, heap, stack)
          ☐ Segments can be variable length
          ☐ Divide each segment into fixed-sized pages
          ☐ Implementation:
            ☐ Each segment has a page table
            ☐ Each segment track base (physical address) and bounds of page table for that segment
            ☐ SEE QUIZ LECTURE 7 SLIDE 37!!
          ☐ Advantages of Segmentation + Paging:
            ☐ Supports sparse address spaces
              ☐ Significant memory savings: Decreases size of page tables
                ☐ Unallocated pages between stack and heap no longer take ups space in page table
            ☐ Sharing
            ☐ No external fragmentation
          ☐ Disadvantages of Segmentation + Paging:
            ☐ Potentially large page tables (for each segment)
            ☐ Must allocate each table contiguously
              ☐ Can get tricky with large page table
        ☐ Multi-level page tables
          ☐ turns the linear page table we've seen so far into a tree structure
          ☐ 2-level Page Table:
            ☐ Chop up the page table into page-sized units
            ☐ If an entire page of page-table entries is invalid, don't allocate that page of the page table at all
            ☐ to track if a page of page table is valid, use a page directory (new structure)
              ☐ Essentially keeps track of which "page-sized units" are valid/invalid
              ☐ See lecture 7 slide 44
            ☐ Virtual addresses are no longer of structure, [VPN|Offset], they are instead [PageDirectoryIndex|PageTableIndex|Offset]
            ☐ Why Useful?
              ☐ For sparse address spaces:
                ☐ Most address spaces are sparsely populated
                ☐ One-level page table:
                  ☐ Need page table for entire address space
                ☐ Two-level page table:
                  ☐ Need top-level page table for entire address space
                  ☐ Need only second-level page tables for populated parts of the address space
            ☐ Are two levels enough?
              ☐ Need top-level page table (page directory) for entire address space
              ☐ Assume size of second-level page table == size of page
                ☐ Why? Easy to allocate
              ☐ Problem: Top-level can get too large if large address space (64 bits)
              ☐ Solution (?): More levels
            ☐ More Levels: The Price to be paid
              ☐ Each level adds another memory access
              ☐ N-level page table
                ☐ 1 memory address -> N + 1 memory accesses (N for page table + 1 for phys addr)
              ☐ But, TLB still works
                ☐ If TLB hit, 1 memory access -> 1 memory accesses
                ☐ If miss, 1 memory access -> N + 1 memory accesses
              ☐ -> TLB hit rate must be very high (99%+)
      ☐ Demand Paging
        ☐ What if "out of memory"
          ☐ Need to get rid of one or more processes, store them temporarily on disk
            ☐ this is called swapping
        ☐ Process Switch to a swapped process?
          ☐ Latency can be very high
          ☐ Need to read image from disk
          ☐ Better solution: Demand paging since not all of a process needs to be in memory
        ☐ Demand Paging: Main reason:
          ☐ Virtual address spaces >> Physical Address space
            ☐ No machine has 2^64 bytes
            ☐ Why such large virtual address space?
            ☐ Convenient for programmer
            ☐ Don't have to worry about running out
        ☐ Deman Paging Benefits:
          ☐ shorter processes startup latency
            ☐ Can start process without all of it in memory
            ☐ Even 1 page suffices
          ☐ Better use of main memory
            ☐ Program often does not use certain parts
              ☐ E.g., error handling routines
            ☐ Program often goes through different parts
              ☐ E.g., initialization, computation, termination
        ☐ If the program is not in memory, then where is it?
          ☐ Part of it is in memory, (typically) all of it in on idsk
            ☐ In a special partition called the Backing Store
        ☐ Swapping vs Demand Paging:
          ☐ Swapping = all of a program is in memory OR all of program is on disk
          ☐ Demand Paging = part of program is in memory
        ☐ Demand Paging Mechanism High Level:
          ☐ What if program needs to access part only on disk
            ☐ This is called a page fault
          ☐ Program is suspended
          ☐ OS runs, gets page from disk
          ☐ Program is restarted
            ☐ This is called page fault handling
        ☐ Demand Paging Issues:
          ☐ How to discover a page fault?
          ☐ How to suspend a process?
          ☐ How to get a page from disk?
            ☐ and how t ofind a free frame in memory?
          ☐ How to restart process?
          ☐ Q: Compact the page table?
            ☐ We saw that a page table has many entries. Each entry has a valid bi, indicating whether that entry corresponds to a page that is actually allocated in memory or not.
            ☐ Q: Why have entries for invalid pages? Why not just have entries for the valid pages?
              ☐ Remember that the first part of a virtual address is the VPN. To find the entry for this VPN in the page table (to get the PFN), we simply add the page table starting address to the VPN * sizeof(entry) to obtain the entry. If the page table only had valid pages (and thus its size would constantly be constantly be in flux), it wouldn't be this simple
        ☐ More on Demand Pagign issues:
          ☐ 1. Discover Page Fault:
            ☐ Idea: Use the valid bit in page table
            ☐ Without demand paging:
              ☐ Valid bit = 0, then invalid
              ☐ Valid bit = 1, then valid
            ☐ With Demand paging:
              ☐ Valid bit = 0, then page is invalid OR page is on disk
              ☐ Valid bit = 1, then page is valid AND page is in memory
              ☐ OS needs additional table: invalid/on-disk?
          ☐ 2. Suspending the Faulting Process
            ☐ Idea: Trap into the OS
              ☐ Invalid bit access generates trap
              ☐ Save process information in PCB when trapping into the OS
          ☐ 3. Getting the Page from Disk
            ☐ Idea: OS handles fetch from Disk
              ☐ Assume (for now) there is at least one free frame in memory
              ☐ Allocate a free frame to process
              ☐ Find page on disk
                ☐ OS needs to remember the swap space in page-sized units
                ☐ Need an extra table for that in OS
              ☐ Get disk to transfer page from disk to frame (slow)
              ☐ Since above is slow, while the disk is busy:
                ☐ Shouldn't waste CPU cycles
                ☐ Invoke scheduler to run another process
                ☐ When disk interrupt arrives
                  ☐ Suspend running process
                  ☐ get back to page fault handling
                ☐ Why OK to go through OS (and not in hardware)?
                  ☐ Disk is so slow that it makes the latency of handling this in OS
              ☐ Completing Page Fault Handling
                ☐ Pagetable[pageno].frameno = new frameno
                ☐ Pagetable[pageno].valid = 1
                ☐ Set process state to ready
                ☐ Invoke scheduler
          ☐ 4. When Process Runs Again
            ☐ Idea: Restarts the previously faulting instruction
            ☐ Process now finds
              ☐ Valid bit to be set to 1
              ☐ Page in corresponding frame in memory
              ☐ Note: different from context switch, which continues with the next instruction
        ☐ Page-replacement Policies:
          ☐ OPT
            ☐ An Optimal Algorithm
            ☐ Replace the page that will be referenced the furthest in the future
            ☐ Provably optimal
            ☐ Can't implement (can't predict the future)
            ☐ A basis of comparison for other algorithms
          ☐ Random
            ☐ Random page is replaced
              ☐ Easy to implement
              ☐ But does not take advantage of spatial/temporal locality
          ☐ FIFO
            ☐ Oldest page is replaced
              ☐ Age = Time since brought into memory
            ☐ Easy to implement
              ☐ Keep a queue of pages
              ☐ Bring in a page: stick at the end of the queue
              ☐ Need replacement: pick head of queue
            ☐ Fair
              ☐ All pages recieve equal residency
            ☐ But does not take into account "hot" pages that may always be needed
          ☐ LRU
            ☐ Least Recently Used
            ☐ Cannot look into the future, but can try to predict future using past
            ☐ Replace least recently accessed page
              ☐ With locality, LRU approximates OPT
              ☐ But harder to implement, must track which pages have been accessed
              ☐ But doesn't handle all worklaods well
                ☐ Example: Large array scans that repeat. popular in DBMS
            ☐ LRU approximation with hardware support
              ☐ Use a reference bit:
                ☐ bit in page table
                ☐ hardware sets bit when page is referenced
              ☐ Periodically
                ☐ Read out and store all reference bits
                ☐ Reset all reference bits to zero
              ☐ Keep all reference bits for some time
                ☐ the more bits kept, the better approximation
              ☐ replacement
                ☐ page with smallest value of reference bit history
        ☐ TLB replacement policies
          ☐ TLB can also become full
          ☐ TLB replacement policies are similar to page replacement policies, though they are often more simplifies and appriximations.
          ☐ Implementations have to be very fast (since the TLB must be very fast), and are typically done in hardware to avoid going into the OS

The Disk:
 ☐ I/O Devices
   ☐ Key Concepts:
     ☐ OS role for integrating I/O devices in systems
     ☐ Polling, Interrupts, Drivers
     ☐ Notion of "permanent" Storage
     ☐ File system interface
     ☐ Disk Management for HDDs
       ☐ Disk Allocation, Scheduling, Optimizations
   ☐ How should I/O be integrated into systems
     ☐ I/O = Input/Output
     ☐ For computer systems to be interesting, both input and output are required
     ☐ Many, many I/O devices
   ☐ System architecture: 
     ☐ Buses = data paths that enable information exchange between CPU, RAM and I/O devices
     ☐ PCIe = Peripheral Componenet Interconnect Express
     ☐ DMI = Direct Media Interface
     ☐ USB = Universal Serial Bus
     ☐ eSATA = external SATA
       ☐ SATA = Serial ATA
       ☐ ATA = the AT attachment, in reference to providing connection to the IBM PC AT
     ☐ CPU is connected to main memory via memory bus
     ☐ Some devices connected to CPU via I/O bus (PCI)
     ☐ Peripheral bus connects other, slower devices (SCSI, SATA, USB, etc.)
     ☐ The faster a bus, the shorter it should be.
     ☐ Also, components needing highest performance (e.g., graphics card) are closest to CPU
   ☐ Basics of a device:
     ☐ A device has two important components:
       ☐ The hardware interface (protocol) that it presents to the system, which allows the OS to control its operation
         ☐ Basics of a protocol: Consists of three registers
           ☐ a status register, which can be read to see the current status of the device
           ☐ a command register, to tell the device to perform a certain tasl; and
           ☐ a data register, to pass data to or get data from the device
         ☐ By reading and writing these 3 registers, the OS can control device behavior
       ☐ The internal structure. E.g., hardware chips, memory, simple CPU firmware (software within a hardware device)
     ☐ Typical OS interaction with a device:
       ☐ 1. Wait until device is not busy
       ☐ 2. starts device and executes the command
       ☐ 3. wait until device is done with request
       ☐ In this approach, the OS repeatedly checks the status register until the device is ready to receive a command, and then until the device is done executing the command. This is known as polling the device.
       ☐ Polling is inefficient (wastes CPU cycles). that's why we will see some better ways to check the status
     ☐ Interrupts
       ☐ Put process requesting I/O to sleep
       ☐ Context switch to a different process
       ☐ When I/O finishes, wake sleeping process with an interrupt
       ☐ Remember: same interrupt mechanism for demand paging
       ☐ Interrupt vs Syscall:
         ☐ Interrupt:
           ☐ generated by hardware to initiate a context switch
         ☐ Syscall: generated by process, to request functionality from kernel mode. Also, initiates a context switch
       ☐ See Lecture 9 slide 45 for example
       ☐ Advatange: No wasted CPU cycles
       ☐ Disadvantage:
         ☐ Expensive to context switch
           ☐ Polling can be better for fast devices
     ☐ Device drivers
       ☐ The OS should be device-netural; it should not contain details of device interactions in its subsystems
         ☐ E.g., the filesystem should not need to know the specifics of what kind of disk it is writing to or reading from
       ☐ we encapsulate specifics of device interation in a device driver
       ☐ Since device drivers are needed for any kind of device you might plug into your system, they represent a huge chunk of kernel code
       ☐ Over 70% of the Linux kernel is device drivers
         ☐ So when we say that an OS has millions lines of code, it really does
     ☐ Hard drives
       ☐ Permanent Sotrage Media
         ☐ Main memory - not suitable
         ☐ Battery-backed memory
         ☐ Nonvolatile memory
           ☐ Flash SSDs
           ☐ 3Dxpoint SSDs (released in 2018)
         ☐ Hard Disks (HDDs)
         ☐ Tapes
       ☐ The main form of persistent storage
       ☐ File system technology is based on their behaviour
     ☐ Interface
       ☐ A drive consists of a number of sectors (512-byte blocks) that can be read/written. We can view the drive as an array of sectors
       ☐ A single 512-byte write is atomic (will complete in its entirety or will not complete at all). If a larger write occurs and power is lost, not all the write may compelte
     ☐ Disk geometry
       ☐ A hard drive is made up of one more platters: circular, hard surfaces on which data is stored by applying magnetic charges
         ☐ Each platter has two sides or surfaces
       ☐ Platters are bound together around a spindle, which is connected to a motor that can spin the platters around, at a constant rate (rotaitons per minute, or RPM)
         ☐ A typical RMP is 7,200
       ☐ Each surface of a platter has thousands of concentric circles called tracks on which data is stored. Each track has a number of sectors
       ☐ Reading and writing from a surface is performed by a disk head (one head per surface); the heads are attached to a disk arm, which moves across the surface to position the head over the desired track
       ☐ REVIEW EXAMPLES ON LECTURE 9 SLIDE 90ish
    ☐ I/O Time
      ☐ head selection (to select platter)
      ☐ seek time (depends on disk; time to find track?)
      ☐ rotational delay (depends on disk)
      ☐ transfer time (depends on file size)
    ☐ Disk Interface
      ☐ Accessible by sector only
        ☐ ReadSector
        ☐ WriteSector
      ☐ Logical_sector_number
        ☐ Platter
        ☐ Cylinder or track
        ☐ Sector
    ☐ Flash-based SSDs
      ☐ No moving parts! No magnet problem!
      ☐ Built out of transistors, like memory and CPU but can retain data without power
    ☐ Bit storage
      ☐ In a flash chip, one or more bits are stored int oa single transistor
        ☐ In a single-level cell (SLC), a single bit is stored
        ☐ Multi-level cell (MLC): Two bits, encocded into different levels of charge (00, 01, 10, 11: low, medium-low, medium-high, high)
        ☐ Triple-level cell (TLC): 3 bits per cell
    ☐ Flash chip organization
      ☐ Flash chips are organized into banks (or planes)
      ☐ Each bank consists of a large number of block of size 128KB or 256 KB
      ☐ Each block contains a large number of pages of size 4KB
    ☐ Low-level operations
      ☐ Reading a page
        ☐ An SSD is a random access device. It can read any page in about the same amount of time regardless of location (Unlike a regular hard drive!)
      ☐ Erase a block
        ☐ Erase destroys the contents of a block (so if we want to write to a specific page in a block, must first erase the entire block before writing) by setting each bit to the value 1
          ☐ Erasing is the first step in actually writing anything to the disk
        ☐ Expensive: takes a few milliseconds
      ☐ Program a page
        ☐ Once a block is erase, the program command can change some of the 1's in a page to 0's
          ☐ That is, you must erase before you can write (Unusual)
          ☐ the only way to change bits to 1 is to erase. Prgramming a page can only flip it to 0
        ☐ Takes 100s of microseconds
      ☐ Example (Lecture 10 slide 60ish) Summary
        ☐ 4 8-bit pages, want to write to page 0. need to erase the entire block, setting all bits across all pages to 1, then we can program page 0.
          ☐ Note that if we wanted t okeep the data in the other three pages, we would have had to put them in memory (e.g.) and then write their contents back, along with sector 0
      ☐ Reading and writing:
        ☐ Reading is fast and can greatly exceed the random read performance of regular drives
        ☐ Writing is expensive: we have to erase the entire block (first moving any important data to another location) and then program it
          ☐ Frequent program/erase cyceles can wear out the flash chips
      ☐ Reliabaility
        ☐ Mechanical hard drives can fail for a variety of reasons
        ☐ Flash chips are pure silicon. But, they can wear out.
        ☐ When a flash block is erased/programmed, it slowly accrues a little bit of extra charge
        ☐ Over time, as the extra charge builds up, it becomes increasingly difficult to differentiate between a 0 and a 1
          ☐ at a certain point, the block becomes unusable 
        ☐ the typical lifetime of a block is currently not well known...
        ☐ SLC chips are rated at 100,000 P/E cycles. MLC 10,000.
        ☐ But research has shown lifetimes are likely much longer
      ☐ SSD structure
        ☐ An SSD consists of some number of flash chips, volatile memory (for aching, e.g.) and some control logic
        ☐ Control logic includes the flash translation layer (FTL). It takes read and write requests on logical blocks and turns them into low-level read, erase and program commands on the device.
      ☐ Direct-mapped FTL
        ☐ In a simple FTL approach we can call direct mapping:
          ☐ A read to a logical page N maps to a read of the physical page N
          ☐ A write to logical page N maps to a read of the block containing the physical page N, then erase of the block, and then programming of the block.
        ☐ Such a direct mapping approach has a lot of problems
          ☐ Very bad performance (costly to write).
          ☐ Even worse reliability: data that is frequently updated, such as file system metadata, would cause the same block to be erased and programmed over and over, leading to the chip wearing out!
      ☐ Log-based FTL
        ☐ Most FTLs today are log structured
          ☐ Upon a write to logical block N, the device appends the write to the next free spot in the currently-being-written-to block.
          ☐ A mapping table stores the physical address of each logical block in the system
        ☐ **See example in Lecture 10**
          ☐ First write: erase the whole first block, then write to first page
          ☐ For the subsequent writes, since the whole block was already erased, no more erases are needed to write pages in this block.
            ☐ Pages have states, wither Valid, Empty, or Invalid
            ☐ Once block erased, state changes to E; pages can only be used for writing if state is E (not invalid)
            ☐ Garbage:
              ☐ Writing to logical blocks a second time, after being used once, simply stores the desired content in the next free physical pages, then updates the mapping table to point the logical address to the new contents
              ☐ Now the old contents are garbage, that is they are the old versions of the logical blocks and are no longer referenced in the mapping table
              ☐ Because of the log-structured nature of an SSD, overwriting data creates garbage blocks (or dead blocks). The device must reclaim these to make space in a process known as garbage collection
      ☐ Garbage Collection:
        ☐ Basic process:
          ☐ Find a block with one or more garbage pages
          ☐ Read in the live (non-garbage) pages from that block
          ☐ Write them somewhere else ("to the log").
          ☐ Reclaim the entire block for writing
          ☐ See example in Lecture 10
        ☐ can be expensive (reading, rewriting).
        ☐ The ideal candidate for reclamation is a block of only dead pages (then there is no need to move data)
        ☐ Many modern drives add extra flash capacity to the drive so that garbage collection can be delayed and done in the background (like when the drive is less busy)
        ☐ This is known as overprovisioning
        ☐ Log-based FTL:
          ☐ Since erases are only required once in a while in this scheme, and the "read-modify-write" pattern of the direct-mapping approach is avoided completely, the log-based FTL greatly improved performance.
          ☐ It also improve reliability, since writes are spread across all pages
 ☐ File API
   ☐ The file:
     ☐ A linear array of bytes, each of which can be read/written
       ☐ could contain any data. the filesystem does not know; only the application knows what the data means
       ☐ Has a low-level name, known as the inode number
     ☐ Creating files:
       ☐ A file is created by calling the open() system call and passing the 0_CREAT flag
       ☐ The call returns a file descriptor: an integer for that process that you can use read/write to the file
         ☐ File descriptors are managed on a per-process basis
     ☐ Reading and writing files:
       ☐ Reading:
         ☐ If we were at the terminal, we might use cat to show the contents of a file
         ☐ How does cat access a file? we can find out by using the strace command, which traces the system calls of a program.
         ☐ Cat first opens the file (for reading only). It returns the file descriptor 3
           ☐ Each running process already has 3 files open at the start: standard input(0), standard output(1) and standard error(2)
         ☐ It then passes the file descriptor to the read() system call (along with a buffer into whuch the read data will be stored, and the size of the buffer). The call will return the number of bytes read
         ☐ Then, it likely calls printf, which will invoke the write() system call, giving 1 (standard output) as the file descriptor
         ☐ Finally, it tries to read from the file again. Since there is nothing else, read() returns 0 for 0 bytes, so cat knows there is nothing left. So it calls close() to indicate it is done with the file
       ☐ Writing:
         ☐ Done similarly, except we open a file for writing and use the write() system call (perhaps repeatedly for larger files), then close it
         ☐ So far, we haveread/written from the beginning to the end of a file. What if we want to read/write only in the moddle of a file, or some specific ofsset?
         ☐ We can do so using the lseek() system call
       ☐ lseek()
         ☐ takes three arguments:
           ☐ file descriptor
           ☐ the file offset
           ☐ how the seek should use the offset
             ☐ use offset as given
             ☐ use current location + offset
             ☐ use file size + offset
         ☐ lseek() only changes the current offset, it does not read or write
       ☐ Process file info
         ☐ The offset for a file, as well as other information like a pointer to the file's inode, is kept in a process file struct that has attributes like a reference counter, is_readable, is_writable, inode_ptr, current offset
       ☐ Open file table
         ☐ The file structs are kept together in an array called the open file table, again one for each process
         ☐ The index of a file struct in the array is its file descriptor
     ☐ The directory
       ☐ Also has an inode number
       ☐ Contains a list of files in format (filename, inode)
         ☐ Directories can also contain other (sub)directories
     ☐ Directory Hierarchy
       ☐ Starts at the root directory("/" in Unix-based systems).
       ☐ Uses separator to name subsequent sub-directories
       ☐ Absolute pathname of a file is its path from the root
     ☐ Hard links
       ☐ link() is a system call. it takes two arguments: an old path and a new path. It will create the same file at the new path of the file at the old path
       ☐ The two files will be exactly the same. They will not be copies nor aliases. They will have the exact same inode
       ☐ When a link is made, a reference count in the inode struct will increase by one. When a file is unlikned, the file is removed from its directory struct and the inode ref counter decreases by one
       ☐ When the ref count is 0, the filesystem can free inode and truly "delete" the file. thus only when the last hard link to an inode is deleted will the underlying file be deleted
       ☐ Hard links can also be made through the ln command-line tool
     ☐ Symbolic links
       ☐ A different type of link can be made using ln -s, known as a symbolic link
       ☐ Hard links are limited: they can't be made to directories or to files in other partitions, since they are based on inodes.
       ☐ Symbolic links have no such limits. But they are very different than hard links
       ☐ A symbolic link is a file itself with its own inode. The conents of the file is a pathname to the linked-to file (as such, symbolic links are typically only a few bytes in size).
       ☐ If we remove the linked-to file, the symbolic link will not be deleted (since its connection to the original is not stored with the original file). Therefore, the symbolic link will become an invalid reference
     ☐ Symbolic links vs. aliases:
       ☐ A regular file "alias" created in the Mac Finder, for example, is different from a symbolic link since the former typically stores the inode of the original file.
       ☐ A symbolic link contains only the pathname. So if the original file is moved, the symbolic link will no longer work
     ☐ See Lecture 10 slide 50ish for examples of Symbolic and Hard Links
     ☐ Mounting a filesystem
       ☐ mkfs: a command-line tool that takes a device as input and a filesystem type and writes an empty filesystem with root directory to disk
       ☐ mount: takes a device with a filesystem and makes it available at a given directory

The Filesystem:
 ☐ Implementing a filesystem
   ☐ File System role:
     ☐ The main tasks of the filesystem is to **translate** from **user interface** funtions (like Read()) to **disk interface** functions (ReadSector())
   ☐ The filesystem is pure software
   ☐ Data Structures
     ☐ Disks vs in-memory simple but golden rules:
       ☐ If it is not on disk and you crash, it is gone!
       ☐ If you need it after a crash, it must be on disk
 ☐ On-disk structures
   ☐ Disk Data Structures
     ☐ Data Region <- occupies most space in Filesystem
       ☐ User data
       ☐ Free space
     ☐ Metadata
       ☐ Inodes
       ☐ Free space management (bitmap)
       ☐ Superblock
   ☐ Remember, we want a translation between user filesystem interface to disk interface
     ☐ Need some structure to map files to disk blocks
       ☐ Reserve Data Region on disk to Store User Data
         ☐ Data region contains user data (files) and free space)
     ☐ Same principle: map logical abstraction to physical resource
   ☐ The inode:
     ☐ Inode is short for index node. Almost all filesystems have inodes. Inodes contain data for a file like:
       ☐ its type (regular file, directory, symbolic link)
       ☐ size (in bytes)
       ☐ number of blocks allocated to it
       ☐ owner (uid) + permissions (rwx)
       ☐ time info (created, modified, last accessed date)
       ☐ info about where its data blocks reside on disk
     ☐ An inode has an inode-number called the low-level name of the file
     ☐ Given the inode-number, you can directly locate where on disk the inode is located (i.e., where inside the inode table)
       ☐ Assume that the inode table starts at 12KB. Then inode 32 would be located at 12 KB + (32 * sizeof(inode))
   ☐ Reserve Inode Table to track files
     ☐ Inode table: holds an array of on-disk inodes
     ☐ Note that the reserved space for the inode table is different than the reserved data region for the user data
     ☐ Inodes are not too large (128 or 256 bytes per inode)
   ☐ How do we track free space?
     ☐ Allocation structures:
       ☐ For data
       ☐ For inodes
     ☐ Allocation Structures implementation:
       ☐ Free-lists (remember from memory management module)
       ☐ Bitmaps
         ☐ Data structure where each bit indicates if corresponding object is free or in use
         ☐ 1 = in use
         ☐ 0 = free
       ☐ We will use bitmaps (Data bitmap, Inodes bitmap)
     ☐ Reserve Blocks for Allocation Structures (in this case the bitmaps)
   ☐ At this point in the example for Lecture 11, there is 1 block left unallocated. This is the super block.
   ☐ Superblock
     ☐ Contains file system metadata
       ☐ #inodes
       ☐ #data blocks
       ☐ Start of inodes table
       ☐ ...
   ☐ Final question now: how to allocate files to data blocks (and the converse, how to handle free space)
     ☐ User Data Allocation Strategies:
       ☐ Contiguous
       ☐ Extent-based
       ☐ Linked list
       ☐ File-Allocation tables (FAT)
       ☐ Indexed
       ☐ Multi-level indexed
   ☐ Filesystem fragmentation types
     ☐ Internal fragmentation
       ☐ How much a file's allocated blocks is actually being used by the file? (Typically, all of it, except perhaps part at the end.)
       ☐ Not a big consideration
     ☐ External Fragmentation
       ☐ The location and size of the free blocks. Could pose aproblem to allocate new files, depending on the allocation strategy.
       ☐ Also, the location and size of the allocated blocks. Could pose a speed problem, depening on the hard drive type
   ☐ Contiguous Allocation
     ☐ Strategy:: Allocate file data blocks contiguously on disk
     ☐ Metadata: Starting block + size of file
       ☐ High Fragmentation (many unusable holes) :(
       ☐ Growing Files: may not be able to grow without moving file :(
       ☐ Sequential Access: Excellent :)
       ☐ Random Access: Simple calculation :)
       ☐ Metadata overhead: Low
       ☐ Conclusion: Impractical
   ☐ Extend-based Allocation
     ☐ Strategy: Allocate multiple contiguous regions (called extents) per file
     ☐ Metadata: Array of extends. Each entry contains extend starting block and size
       ☐ Helps external fragmentation
       ☐ Growing Files: can grow :)
       ☐ Sequential Access: Good performance :)
       ☐ Random Access: Simple Calculation :)
       ☐ Metadata overhead: Can get large if many extents :(
       ☐ Used in HFS+/NTFS
   ☐ Linked-List Allocation:
     ☐ Strategy: Allocate linked-list of blocks
     ☐ Metadata: Location of first block file, plus each block contains pointer to next block
       ☐ Fragmentation: No external fragmentation, low internal fragmentation :)
       ☐ Growing files: Can grow easily :)
       ☐ Sequential Access: Depends on data layout (may have to seek back and forth)
       ☐ Random Access: Slow :(
       ☐ Metadata overhead: Wastes space on pointers in data blocks :(
   ☐ File-Allocation Tables (FAT)
     ☐ Strategy: Keep links information for all files in a data structure on disk, called the file allocation table (FAT). Optimization: the FAT can be cached in memory
     ☐ Metadata: Location of first block of file, plus FAT table itself
       ☐ Fragmentation: Same as linked list :)
       ☐ Growing files: same as linked list :)
       ☐ Sequential access: same as linked list
       ☐ Random access: significantly better than linked list if FAT cached in memory :)
       ☐ Metadata overhead: Low for small files. what about large files :(
       ☐ Used in MS-DOS, early Windows
       ☐ Notes: links stored in FAT
   ☐ Indexed Allocation
     ☐ Strategy: Keep pointers to blocks of files in an index in the file's inode. Cap at maximum N pointers.
     ☐ Metadata: Index for each file.
       ☐ Fragmentation: No external fragmentation, low internal fragmentation :)
       ☐ Growing Files: Can grow easily :)
       ☐ Sequential Access: Efficient :) (seeks can be optimized)
       ☐ Random Access: Efficient :)
       ☐ Metadata overhead: Low for small files.
       ☐ Notes: Can be combined with extent allocation; Extent + indexing used in NTFS, ext4(Linux), APFS
   ☐ Indexed Allocation with Indirect Blocks
     ☐ N Pointers in inode block
     ☐ First M (<N) point to first M data blocks
     ☐ Blocks M+1 to N point to indirect blocks
     ☐ Indirect blocks
       ☐ Do not contain data
       ☐ But pointers to subsequent data blocks
     ☐ Double-indirect blocks also possible
     ☐ Same advantages as indexed allocation plus
     ☐ Possible to extend to very large files
   ☐ What about Directories?
     ☐ Directories stored as files
   ☐ SEE THE PRACTICE PROBLEM AT THE END OF LECTURE 12
   ☐ SEE PRACTICE ITERATION ON LECTURE 13 OF FILE SYSTEM MAIN ACCESS METHODS
 ☐ In-memory structures
   ☐ remember: File system Implementation
     ☐ Key aspects of the system:
     ☐ 1. Data Structures
       ☐ On disk
       ☐ In memory <- In memory data structures are used to make I/O more efficient
     ☐ 2. Access methods
       ☐ How do we open(), read(), write()
   ☐ In-Memory Data Structures
     ☐ Cache
     ☐ Cache Directory
     ☐ Queue of pending disk requests
     ☐ Queue of pending user requests
     ☐ Active file table
     ☐ Open file tables
   ☐ Cache
     ☐ Fixed contiguous area of kernel memory
     ☐ Size = max number of cache blocks x block size
     ☐ A large chunk of memory of the machine
     ☐ In genereal, write-behind is used
     ☐ for user data ok
     ☐ For metadata
       ☐ Written to disk more aggressibely
       ☐ Affects integrity of file system
   ☐ Cache Directory
     ☐ Usually a hash table
     ☐ index = hash(disk address)
     ☐ With an overflow list in case of collision
     ☐ Usually has a "dirty" bit
   ☐ Cache Replacement
     ☐ Keep LRU list
       ☐ Unlike memory management, here easy to do
       ☐ Accesses are far fewer (file vs memory access)
     ☐ If no more free entries in the cache
       ☐ Replace "clean" block according to the LRU
       ☐ Replace "dirty" block according to LRU
   ☐ Cache Flush
     ☐ Find "dirty" entries in cache
     ☐ Write them back to disk
       ☐ Periodically (30 seconds)
       ☐ When disk is idle
   ☐ (System-Wide) Active File Table
     ☐ One array for the entire system
     ☐ One entry per open file
     ☐ Each entry contains
       ☐ File inode
       ☐ Additional information
         ☐ Reference count of number of file opens
   ☐ (Per-Process) Open File Tables
     ☐ One array per process
     ☐ One entry per file open of that process
     ☐ Indexed by file descriptor fd
     ☐ Each entry contains
       ☐ Pointer to file inode in active file table
       ☐ File pointer fp
       ☐ additional information
   ☐ SEE EXAMPLE IN LECTURE 13
 ☐ Setting up the filesystem
   ☐ Setting up the FS:
     ☐ By default, OS sees all storage devices as 
       ☐ Chunks of unallocated space
       ☐ Which are unusable
     ☐ Cannot start writing files to a blank drive
     ☐ Need to set up the FS first
   ☐ Disk Partitioning (or Slicing)
     ☐ Filesystem needs a "container" on the storage device
     ☐ Container is called a partition
     ☐ Partitioning allows different FS to be installed on same OS
     ☐ Each partition appears to OS as a logical disk
     ☐ Disk stores partition info in partition table
       ☐ Partition locations
       ☐ Partition sizes
     ☐ OS reads partition table before any other part of the disk
   ☐ Mounting a File System (FS)
     ☐ FS lives inside a partition
     ☐ But OS cannot read/write files yet
     ☐ File system needs to be mounted for OS t oaccess its files
     ☐ Mounting attaches filesystem to a directory
     ☐ Directory is called mount point
   ☐ Multiple FS
     ☐ Users may want to have many FS at the same time
       ☐ Main disk
       ☐ Backup Disk
       ☐ USB drive
       ☐ etc
     ☐ How can OS support this?
     ☐ Idea: Stitch all the file systems together into a "super file system"!
     ☐ root(/) file system is always mounted
     ☐ OS keeps track of mounted FS in Mounted FS Table
   ☐ Booting
     ☐ We said root (/) file system was always mounted.
     ☐ How is this done?
     ☐ Very first thing that happens when computer turns on
     ☐ BIOS looks for clues on what it needs to start OS
     ☐ First place BIOS checks is the boot block
   ☐ Boot Block
     ☐ At fixed location on disk (usually sector 0)
       ☐ Contains boot loader
       ☐ Contains partition table
     ☐ Read by BIOS on machine boot
   ☐ File System Startup
     ☐ Normally, nothing would be necessary
     ☐ Sometimes things are not normal
       ☐ Disk sector goes bad
       ☐ File system software has bugs
       ☐ ...
     ☐ Common to "check" the file system (fsck)
   ☐ File system check
     ☐ No sectors are allocated twice
     ☐ No sectors are allocated an on free list
     ☐ Reconstruct free list
   ☐ Replication
     ☐ Some key sectors are replicated
       ☐ Boot blocks
       ☐ Sometimes also inode blocks
 ☐ Memory-mapped files
   ☐ Alternative File Access Method: Memory Mapping
     ☐ mmap()
       ☐ Map the contents of a file in memory
     ☐ munmap()
       ☐ Remove the mapping
   ☐ Recall typical virtual address space structure:
     ☐ code
     ☐ heap
     ☐ region of unused chunks
     ☐ stack
   ☐ We can use the region of unused chunks for the mmap region, and place an mmapped file in the mmapped region
   ☐ Remember Large Address Spaces?
     ☐ 64 bit address space
     ☐ Do you know now why desireable?
       ☐ 32 bits -> 4Gbytes
       ☐ A few big files mmaped()-ed
       ☐ You are out of virtual address space!
   ☐ Access to mmap()-ed Files
     ☐ Access to mmaped()-ed memory region
     ☐ Causes page fault
     ☐ Causes page/block of file to be brough in
   ☐ mmap() implementation
     ☐ On mmap()
       ☐ Allocate page table entries
       ☐ Set valid bit to "invalid"
     ☐ On access,
       ☐ Page Fault
       ☐ File = backing store for mapped region of memory
       ☐ Just like in demand paging
       ☐ Except paged from mapped file
     ☐ After page fault handling
       ☐ Set valid bit to true
   ☐ How to get data to disk for mmap?
     ☐ Through normal page replacement
     ☐ Or through an explicit call msync()
   ☐ What is mmap() good for?
     ☐ Random access to large file
   ☐ Random Access with mmap()
     ☐ addr = mmap()
     ☐ Use memory addresses in [addr, addr+len-1]
   ☐ Random Access with Read() Interface
     ☐ Open
     ☐ Read entire file into memory buffer
     ☐ Then use memory address in buffer
   ☐ Advantage with mmap()
     ☐ Only accessed portions brough in memory
     ☐ Huge advantage
       ☐ For large files
       ☐ Sparesely accessed
     ☐ Much easier programming model
       ☐ Follow pointer in memory
       ☐ As opposed to (Seek, Read) every time
     ☐ Easier if reuse
       ☐ VM system keeps page for you
       ☐ Otherwise, have to do your own replacement
   ☐ Random Access with Seek()
     ☐ Open
     ☐ Seek
     ☐ Read into Buffer
     ☐ Seek
     ☐ Read into Buffer
   ☐ mmap() Advantages for Random Access
     ☐ Easy to write
     ☐ Only bring in memory what you read
     ☐ Easy reuse
   ☐ Issues with mmap()
     ☐ Alignment on page boundary
     ☐ Not easy to extend a file
     ☐ For small files
       ☐ Read() more efficient than mmap() + page fault
   ☐ **YOU SUCH AT MMAP**
 ☐ Log-structured file systems
   ☐ Recall: Log-based FTL
     ☐ An SSD consists of some number of flash chips, volatile memory (for caching, e.g.) and soem control logic
     ☐ Control logic includes the flash translation layer (FTL). It takes read and write requests on logical blocks and turns them into low-level read, erase and program commands on the device
     ☐ Most FTLs today are log structured
       ☐ Upon a write to logical block N, the device appends the write to the next free spot in the currently-being-written-to block
       ☐ A mapping table stores the physical address of each logical block in the system
   ☐ Log-Structured File System (LFS)
     ☐ Alternative way of structuring file system
     ☐ Log = append-only data structure (on disk)
   ☐ LFS Motivation
     ☐ LFS design takes into account:
     ☐ Growing memory sizes:
       ☐ Most frequent reads are cached
       ☐ FS performance comes from write performance
       ☐ Optimize for writes!
     ☐ Large gap between random I/O and sequential I/O performance
   ☐ LFS Idea
     ☐ Use disk purely sequentially
     ☐ Easy for writes
       ☐ Can do all writes near each other to empty space - new copy
     ☐ Hard for reads
       ☐ User might read files X and Y not near each other on disk
       ☐ Maybe not be too bad if disk reads are slow - why?
         ☐ Memory sizes are growing (cache more reads)
   ☐ LFS Strategy
     ☐ File system buffers writes in main memory until "enough" data
       ☐ Write both Inodes and data
     ☐ Write buffered information sequentially to new segment on disk
       ☐ Segment: large (MBs) contiguous regions on disk
     ☐ Never overwrite old info: old copies left behind
       ☐ Old copies garbage collected later
     ☐ What data structures can LFS remove?
       ☐ Allocation structs: data + inode bitmaps
     ☐ What is much more complicated?
       ☐ Inodes are no longer at fixed offset
     ☐ New Data Structure: imap (inode map)
       ☐ The imap
         ☐ Table of inode disk addresses
           ☐ Maps uid to disk address of last inode for that uid
           ☐ Updated every tiem inode is written to disk
         ☐ Using the imap
           ☐ Open()
             ☐ Get inode address from inode imap
             ☐ Read inode from disk into Active File Table
           ☐ Read(): as before
             ☐ Get from cache
             ☐ Get from disk address in inode
         ☐ Where to keep imap?
           ☐ Write imap in segments, keep pointers to pieces of imap in memory
     ☐ Crash?
       ☐ What data needs to be recovered after crash?
         ☐ need imap
         ☐ Write copy of imap to fixed location on disk
         ☐ Put marker in the log (called tail marker)
         ☐ Checkpoint periodically. Example: every 30 seconds
       ☐ time Interval between Checkpoints:
         ☐ Too short: lots of disk I/O to write checkpoints
         ☐ Too lon: long recovery time (forward scan)
         ☐ Comprimose
           ☐ Crashes are rare
           ☐ So recovery seldom happens
           ☐ Can tolerate longer recovery time
     ☐ Crash Recovery with Checkpointing
       ☐ Start from imap in checkpoint
         ☐ Contains addresses of all inodes written before last checkpoin
       ☐ How to find inodes?
         ☐ That were in in-memory inode map before crash
         ☐ but not written in the checkpoint
       ☐ Roll forward
         ☐ Remember: checkpoint put marker in log
         ☐ From marker forward
           ☐ scan for inodes in the log
           ☐ Add their addresses to inode map
             ☐ Result: All inode addresses in imap before crash are in imap afterwards
     ☐ What if Crash during checkpoint?
       ☐ Two checkpoint regions
       ☐ Overwrite one checkpoint at a time
       ☐ Use timestamps to identify most recent checkpoint
     ☐ What if the Disk is full?
       ☐ No sector is ever over written
         ☐ Always written to end of log
       ☐ No sector is ever put on free list
       ☐ So disk will get full (quickly)
       ☐ Need to "clean" the disk
     ☐ Disk Cleaning
       ☐ Reclaim "old" data
       ☐ "Old" here means
         ☐ Logically overwritten
         ☐ But not physically overwritten
           ☐ Older version of (uid, blockno) somewhere in the log
       ☐ Segments can contain a mix of old and new data
       ☐ Done one segment at a time:
         ☐ Determine which blocks are new
         ☐ Write them into buffer
         ☐ If buffer is full, write new segment
         ☐ Cleaned segment is marked free
       ☐ See example in Lecture 15
     ☐ Summary: LFS
       ☐ reads mostly from cache
       ☐ writes to disk heavily optimized: few seeks
       ☐ Reads from disk: bit more expensive but few
       ☐ Cost cleaning
       ☐ Has not become maintsteram 
         ☐ cost of cleaning is considerable (similar to garbage collection)
         ☐ Unpredictable performance dips
       ☐ Similar ideas in some commercial systems
         ☐ Log-structered merge key-value stores
 ☐ Bad Sectors
   ☐ Due to manufacturing errors, regular wear-and-tear, physical impact or dust, sectors on a hard drive can sometime become defective or "bad".
   ☐ the contents of these sectors will be lost, and writing to them will fail
   ☐ Detecting bad sectors
     ☐ Typically, the disk firmware can detect a bad sector if it tried to read from that sector and fails. It will then mark that sector as bad and will not try to read/write to it again
       ☐ Instead, it remaps the logical sector number to a different physical sector number (recall the FTL which does this kind of mapping)
       ☐ It remaps to a "spare" sector that was kept empty for this purpose. Once all the spare sectors have been used for remapping, the drive will need to be replaced
     ☐ The firmware reports the number of bad sectors to the OS through SMART, which can be read by software utilities
   ☐ A sector may not become bad all at once. there may be warning signs.
   ☐ Whenever data is written to a sector, a checksum (error-correcting code, or ECC) is stored along with the data
   ☐ When reading the same data, we can recalculate the checksum, and if it is different, we know the sector is bad
     ☐ However, depending on the difference of the checksum, we can recover the corrupted data (very common)
   ☐ Recovering deleted files:
     ☐ When a file is deleted, and has no more links, its inode and data blocks are markes as free in the bitmap. (And its directory entry will be removed.)
     ☐ But the contents of the file will still be stored on disk until such point that the space is needed for a new file
     ☐ Depending on the particular file system, its garbage collection routines, and the volume of disk writes, there will be a window of time starting from the deletion of the file where the file may still be recoverable
   ☐ Recovering inode:
     ☐ If we can recover the deleted inode, then we can check the pointers to teh allocated data blocks and try to stitch together the deleted file contents
   ☐ Recovering data blocks:
     ☐ If we can't recover the inode, we have to scan the data blocks marked as free
     ☐ This strategy is known as file carving
     ☐ A big issue here is the data block allocation policy
   ☐ File Carving:
     ☐ File carvers search a disk for occurrences of particular headers and footers for given file formats
       ☐ every Java class file starts with the hex calues CA FE BA BE
     ☐ This allows them to identify the starting and ending point of files
     ☐ They can then "carve" (copy) some of the bytes of these (possibly deleted) files
     ☐ This can be done even if the filesystem metadata has been completely corrupted/destoryed
     ☐ But the file(s) must be stored in contiguous data blocks for it to work out well
   ☐ File carvers and allocation policies
     ☐ certain filesystems do try to allocate contiguously as much as possible, to avoid disk fragmentation.
       ☐ Fragmented files (e.g., files that are spread out all over the disk) lead to much higher seeks times.
       ☐ So file carving will work well here
     ☐ But newer filesystems that are SSD-aware don't try as much, since fragmentation is not an issue (no seek time)
       ☐ So file carvers don't work as well
 ☐ Crash recovery
   ☐ Crashes:
     ☐ What happens if, in the middle of updating on-disk structures, there is a power loss or system crash
     ☐ The on-disk structure may be left in an inconsistent state
   ☐ Crash consistency problem
     ☐ What we'd like is to update file system state atomically (e.g., after inode, bitmap and data block are written).
     ☐ But disk only commits one write at a time.
     ☐ This is known as the crash consistency problem
     ☐ Solution 1: fsck
       ☐ Early file systems let these inconsistencies occur, and then fix them later (during rebooting).
       ☐ They could run a tool called fsck (file system checker)
       ☐ It finds inconsistencies and repairs them.
         ☐ But it can't fix all problems; e.g., if the data block is never written
       ☐ fsck phases:
         ☐ fsck is run before the filesystem is mounted. It has several phases:
           ☐ It does sanity checks on the superblock. E.g., if the filesystem size is greater than the number of allocated blocks.
           ☐ It scans all inodes, indirect blocks, double indirect blocks, et.c, to find out which blocks are allocated and which are free. Then it compares this information with the data bitmaps and inode bitmaps. If any inconsistency, it trusts the info in the inodes
           ☐ It checks each inode for corruption. E.g., if the type fields are valid (regular file, directory, symbolic link). If there are problems, the inode is removed.
           ☐ It checks the link count of each allocated inode. (The link count indicates the number of directories that contain a reference to this file.) It verifies this count by scanning the entire directory tree to build its own link count. If a mismatch it corrects the inode
           ☐ It checks for bad block pointers. A pointer is bad if it points to something outside its valid range (e.g., outside the partition size.) It removes bad pointers
           ☐ It does some checks on the contents of each directory: if. and .. are the first entries, if each reference node is allocated, etc.
       ☐ The problem with fsck is that it is too slow. Could take many minutes, or hours, with larger disks
       ☐ The basic premise is also slightly irrational: if a crash occurred between three writes, it is very expensive to scan the entire disk to fix this small problem
     ☐ Solution 2: Implement Atomicity
 ☐ Atomocity - Another solution to the crash consistency problem
   ☐ Atomocity in a file seystem means all updates are on disk, or no updates at all; nothing in between (atomicity = "all or nothing")
   ☐ 2 Ways to implement Atomicity:
     ☐ Approach 1: Shadow Paging
       ☐ Have old copy on disk
       ☐ Have new copy on disk
       ☐ Switch atomically between the two, using WriteSector()
       ☐ Write-through or Write-behind
     ☐ Approach 2: Intentions Log
       ☐ On Write:
         ☐ Write to cache
         ☐ Write to log
         ☐ Make in-memory Inode point to update in log
       ☐ On Close()
         ☐ Write old and new inode to log in one disk write
         ☐ Copy updates frm log to original disk locations
         ☐ When all updates done, overwrite inode with new value
         ☐ Remove updates and old and new inode from log
       ☐ Crash Recovery:
         ☐ Search forward through log
         ☐ For each new inode found
           ☐ Find and copy updates are done, write new inode
           ☐ Remove updates, old inode, and new inode from log
         ☐ If crash and new inode in log, then use the new inode after crash
         ☐ If crash and new inode not in log, then use the old inode after crash
   ☐ Which approach better?
     ☐ Count the number of disk I/Os
     ☐ count the number of random disk I/Os
     ☐ Shadow Paging: two disk writes; one for data block, one for inode
     ☐ Intentions log: four disk writes: two for data block, two for inode
   ☐ 

Concurrency:
  ☐ Option 1
    ☐ Build apps from many communicating processes
    ☐ Communicate through message passing
      ☐ No shared memory
    ☐ Pros
      ☐ If one process crashes, other processes unaffected
    ☐ Cons
      ☐ High communication overheads
      ☐ Expensive context switching
  ☐ Option 2
    ☐ New abstraction: thread
    ☐ Multiple threads in process
    ☐ Threads are like processes, exept
      ☐ Multiple threads in the same process share an address sapce
      ☐ Communicate through shared address space
      ☐ If one thread crashes,
        ☐ the entire process, including other threads, crashes
  ☐ Processes provide separation
    ☐ In particular, memory separation (no shared data)
    ☐ Suitable for coarse-grain interaction
  ☐ Threads do not provide separation
    ☐ In particular, threads share memory (shared data)
    ☐ Suitable for tighter integration
  ☐ The purpose of threads:
    ☐ Parallelism: use each CPU core to do part of the work of a big task
    ☐ Efficiency: to avoid program slowdowns from slow I/O. Enable overlap of I/O with other activies in a program
    ☐ Threads share an address space, so are more natural to use for these cases instead of having multiple processes
  ☐ Why concurrency in OS?
    ☐ OS was first concurrent program
      ☐ Internal OS structures (e.g., block size for files) must be updated carefully to ensure data not lost
  ☐ Joining threads: need place to store return value for a thread
  ☐ Different possible executions with multiple threads; up to scheduler
    ☐ See Lecture 17 slide 30 for different possibilities
  ☐ Shared Data: data that all threads "share" or can refer to
    ☐ Advatages
      ☐ many threads can read/write it
    ☐ Disadvantages:
      ☐ Many thres can read/write it
      ☐ Can lead to data races
  ☐ Data Race
    ☐ unexpected/unwanted access to sahred data
    ☐ Result of interleaving of thread executtions 
    ☐ Prgram must be correct for all interleavings
  ☐ Race condition: when the results depend on the timing of the code's execution. It occurs when there is unexpected/unwanted access to shared data
  ☐ Non-detminism
    ☐ Concurrency leads to non-deterministic results
      ☐ Different results even with the same inputs
      ☐ Race conditions
  ☐ Basic approach to multithreading
    ☐ Divide "work" among multiple threads &
    ☐ Share data
      ☐ Which data is shared?
        ☐ global variables and heap
        ☐ not local variables, not read-only variables
      ☐ Where is shared data accessed?
        ☐ Put shared data access in critical section
  ☐ Critical Section
    ☐ Want 3 instructions to execute as an uninterupptable group
    ☐ That is, we want them to be atomic
    ☐ Need mutual exclusion for critical sections
      ☐ If thread A is in critical section C, thread B can't enter C
      ☐ Ok if other processes do unrelated work
    ☐ Mutual Exclusion
      ☐ Prevents simultaneous access to a shared resource
        ☐ In this case, shared resource = shared memory region
      ☐ How can we achieve mutual exclusion?
  ☐ Synchronization
    ☐ Build higher-level synchronization primitives in OS
      ☐ Operations that ensure correct ordering of instructions across threads
    ☐ Monitors, Locks, Semaphores, Condition variables
  ☐ Locks:
    ☐ A lock is a special variable + function
    ☐ It lets us execute a critical section atomically, thus making sure that only one thread executes the ciritical section at a time
    ☐ critical section/code goes between lock() and unlock() calls
    ☐ Goals of a lock:
      ☐ Mutual exclusion
      ☐ Fairness (each thread should get a fair shot at acquiring the lock)
      ☐ Performance (time overheads)
    ☐ Implementation for a lock: "Test-and-set" is optimal
      ☐ Using loads/stores has bad performance, plus it does not actually enforce mutual exclusion
      ☐ Controlling interupts:
        ☐ programas would be trusted with privileged operation; could be abused to grab hold of CPU and maybe never let go
        ☐ Does not work with multiple CPUs (two threads on different CPUs could both enter critical section)
        ☐ If interrupts are disabled, what will happen with I/O requests?
      ☐ In the x86 architecture, the xchg instruction provides the test-and-set functionality
      ☐ Implementation involves TestAndSet() function, which fetches old value, sets to new, and returns the old
      ☐ Spin locks:
        ☐ a lock "locks" by spinning; while (TestAndSet() == 1)
          ☐ spinning only stops when the old value (the returned value) is 0, allowing access to the critical section only once nothing else is accessing it
        ☐ By making the testing and setting of the flag atomic, only one thread can acquire the lock. Thus, we have enforced mutual exclusion
        ☐ These locks are known as spin locks, since they use a while loop to spin and waste CPU cycles. How do they do on our three goals?
          ☐ Correctness: OK (provides mutual exclusion)
          ☐ Fairness: no guarantees (a thread may spin forever)
          ☐ Performance:
            ☐ Bad on single CPU. If a thread is preempted in CS, scheduler may run all other threads which try to acquire the lock, thus wasting CPU time spinning.
            ☐ Multiple CPUs: not so bad, since only one CPU will be spinning, and can spin while CS is being executed
            ☐ Addressing this: yield. In the while loop, instead of looping over and over, call yield() to deschedule the current thread to not waste CPU cycles
    ☐ Deadlocks
      ☐ Threads are stuck waiting for blocked resources and no amount of retry (backoff) will help
  ☐ Condition Variables
    ☐ Conditions
      ☐ Sometimes, a thread may want to check whether some condition is true before continuing
      ☐ One example we've already seen is when a parent thread wants to wait until a child thread has finished executing
      ☐ It does so by calling join -- but how could we implement such a wait ourselves
    ☐ To wait for a condition to become true, a thread can use a condition variable. This is a queue that threads can put themselves on when some condition is not true, by calling wait(). A waiting thread will go to sleep (and thus not waste CPU time)
    ☐ When the condition changes, i.e., when another thread calls signal(), then the (first) waiting thread can wake up and continue
    ☐ Producer-consumer problem:
      ☐ A classic synchronization problem is knows as the producer/consumer problem
      ☐ Imagine there is a producer thread and one or more consumer threads
        ☐ The producer thread produces data items and places them in a buffer
        ☐ The consumer threads consume the data items from the buffer (by doing some processing on each)
      ☐ Since the queue (also knows as bounded buffer) is a shared resource between threads, we must implement synchronized access to avoid race conditions

  ☐ Semaphores
    ☐ A shared, non-negative counter
    ☐ Two primary operations
      ☐ Wait -> attempts to decrement the counter, and blocks when counter is 0
      ☐ Post (or Signal) -> attempts to increment the coutner
    ☐ Initializing the Semaphore
      ☐ Here we create a semaphore with value 1 with the POSIX library
    ☐ sem_wait()
      ☐ Decrements the semaphore value by 1 (blocks if value is negative)
    ☐ sem_post()
      ☐ increments the semaphore value by 1. Wakes one waiting thread, if anyj
    ☐ Can also be used to order events in a concurrent program
  ☐ Correct use in Producer-consumer problem:
    ☐ Acquire mutex lock only after acquiring full/empty semaphore
  ☐ Dining Philosopher's Problem
    ☐ Solution: Change how at least one philospoher grabs the forks E.g., P4 grabs right fork first, and then left
    ☐ cycle of waiting now broken
  ☐ Alice and Bob share a Pond and Pet Dragons
    ☐ The Flag Principle


IPC:
 ☐ A very common pattern:
   ☐ Client
     ☐ Send (send reqeust to server)
     ☐ Blocking recieve ( wait for reply)
   ☐ Server
     ☐ Blocking receive (wait for request)
     ☐ Send (send reply)
 ☐ Remote Procedure Call: when a client wants to call a function that belongs to server code
   ☐ Interface
     ☐ List of remotely callable procedures
     ☐ With their arguments and return values
   ☐ Example: file system interface
     ☐ Open(string filename)
     ☐ returns int fd
       ☐ fd = file descriptor
   ☐ Problem:
     ☐ Want a procedure call interface
     ☐ Have only message passing between processes
     ☐ How to bridge the gap?
   ☐ Solution: Stub Library
     ☐ Client stub and server stub
     ☐ Client stub linked with client process
     ☐ Server stub linked with server process
     ☐ Two message types:
       ☐ Call message
         ☐ From client to server
         ☐ Contains arguments
       ☐ Return message
         ☐ From server to client
         ☐ Contains return values
     ☐ Client stub:
       ☐ Sends arguments in call message
       ☐ Revieives return values in return message
     ☐ Server Stub:
       ☐ Receives arguments in call message
       ☐ Invokes procedure
       ☐ Sends return values in return message





 ☐ Websockets
   ☐ A protocol that allows a client and server process to communicate and exchange data