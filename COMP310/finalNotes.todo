OS HISTORY:
 ☐ ENIAC:
    ☐ First programmable general purpose computer
    ☐ UPENN
    ☐ Program was input by flipping switches and connecting cables. Usually took weeks.
    ☐ At this time, NO OS as none had yet existed
    ☐ Did not store programs in memory
    ☐ Instead, computer resources were manged by operators
    ☐ Can be considered "mainframe computer"
      ☐ mainframe computer -> used by big companies for big data processing
      ☐ all computers of this era were of this type
 ☐ IBM 701:
   ☐ IBM's first comercially-available, stored-program mainframe computer
   ☐ Used for scientific computation (aircraft, nuclear explosives)
   ☐ Ran study on blackjack: rules ofr hitting, standing, doubling (1954)
   ☐ First "AI" program: checkers
   ☐ First automated translation software (Russion to English) for US Air Force
   ☐ Unlike ENIAC, IBM 701 was a stored-program computer.
   ☐ Programs were fed in via punch cards
     ☐ Much easier than having to rewire/flip switches every time
   ☐ No OS
 ☐ 1956 GM-NAA I/O
   ☐ Technological Advancements
     ☐ Tape systems: could read up to 15,000 characters per second, 45 times faster than punch card readers
       ☐ Allowed for larger programs to be read into memory
       ☐ Provided access to a library of commonly-used routines
     ☐ First computer user groups founded
       ☐ Regular meetings organized for computer users to share programs and exchange info on their programming practices
     ☐ First software companies founded
   ☐ The Operator
     ☐ The Eniac, IBM 701 and other computers of the time necessitated an operator
     ☐ The operator was the person who would take the pumch cards anc place them in the card reader
     ☐ They would start the program and supervise the use of the computer's resources.
     ☐ They would deal with malfunctions or bad commands
   ☐ Batch processing
     ☐ The IBM 701 was a batch processing system. Multiple jobs were put on the same tape (a "batch" of jobs), and executed one after the other; thus reducing idle time
     ☐ But manual intervention by human operators was often required between jobs and the supervise the jobs
  ☐ The Mock 701 Monitor (1955)
    ☐ Made at North American Aviation for the IBM 701
    ☐ Could switch betwen jobs in a batch automatically, and handle basic I/O operations without the need for an operator
    ☐ Knows as a resident monitor ("resident" since it was always in memory)
  ☐ GM-NAA I/O (1956):
    ☐ Made for the IBM 704
    ☐ Based on the 701 monitor ans similar work by one if its creators (Robert Patrick)
    ☐ More sophisticated, with better I/O control/automation
    ☐ Considered to be the first Operating System!
    ☐ Used on ~40 Systems
  ☐ Later Developments:
    ☐ 1959: SHARE (IBM user group) takes over. Makes their own version called SHARE OS
    ☐ 1960: IBM makes their own OS called IBSYS
 ☐ 1961: CTSS
   ☐ Recap:
     ☐ Computer mainframes being installed in big companies & universities
     ☐ Goal is to maximize throughput
   ☐ Debugging:
     ☐ 1959: Christopher Strachey suggested a system in which a programmer could debug a job while another job was running on the same computer
     ☐ Would take place at an auxiliary terminal connected to teh computer, such as a teletype machine
     ☐ Also in 1959, John McCarthy at MIT suggested that multiple users work simultaneously on the same compute. In other words, that the computer's processing time could be shared amongst multiple users
     ☐ This was accomplished by implementing a special interupt into the CPU (of the IBM 7094), called a clock interrupt
   ☐ Clock interrupts
     ☐ A clock interrupt is a signal generated by a timer in the CPU at some time interval, e.g., 0.2 seconds.
     ☐ When the clock interrupt is triggered, the CPU regains control from the current job
     ☐ It then decides whether to resume that job, or to start or resume another jb
   ☐ CTSS
     ☐ The Compatibile Time-Sharing System (CTSS)
     ☐ The first general purpose time-sharing OS
     ☐ Supported four terminals connected to the computer's I/O channel
     ☐ A clock interrupt would be triggered every 0.2 seconds. At each clock interrupt the OS regained control and could assign the CPU to another user's job.
     ☐ The first file system + accounts
       ☐ Each user was given their own tape to score their files
       ☐ The tapes were stored in the computer room
         ☐ The first file system*
       ☐ To access the files on their tape when were given an accouin was a password to login with
         ☐ Liekly the first invention of a computer password
     ☐ CTSS was also the first OS to let users enter text commands. Each time a user pressed the return key, their command was sent to the CPU and an interrup was triggered.
  ☐ OS/360  
    ☐ In the 1950s, many different machines were made, each with different architecture, memory, processing power, etc.
    ☐ The OS's we have seen so far (GM NAA I/O, CTSS) were each designed for a specific machine (IBM 701, 7094)
      ☐ In this era, IBM dominated the computer market
    ☐ IBM System/360
      ☐ Starting in 1965, IBM began releasing a family of general-purpose mainframe computers called System/360
      ☐ The computers had a large range in performance.
      ☐ Some were smaller, (relatively) cheaper, low-end systems for business use, while others were very expensive, very fast and with special capabilities for scientific omputing, and o on.
      ☐ But all used the same instruction set
      ☐ The goal was that all System/360 computers could run the same programas without need for modification
        ☐ A major shift: software often had to be rewritten to work on different models of a computer
      ☐ Each computer had different hardware components (CPU, memory, etc.) So they designed an OS that could work on all the hardware configurations and abstract the details of the differences from programs.
      ☐ In fact, one of the most important purposes of today's OS's is that they can work on all kinds of different computer models.
      ☐ IBM's OS/360 was the first such OS to be written. It provided a standardized API across a wide range of machines for file hadnling, memory management and task scheduling
        ☐ REALLY IMPORTANT: OS/360 set a prescedent for how OS's could abstract hardware differences and provide an API for software to use
      ☐ One of the most ambitious software engineering projects at the time
      ☐ More than 1,000 developers hired
      ☐ Tight deadline, and infancy of software engineering practice, led to huge delays, and over 1,000 known bugs at launch
 ☐ 1965-1972: Multics
   ☐ The Computers of Tomorrow
     ☐ Idea suggestion: information utility
     ☐ An information utility would be one which provides information to the community, through phone lines. People would have a terminal in their home, and connect to a computer mainframe to receive information
   ☐ The successor of CTSS:
     ☐ CTSS allowed users t oconnect to a system using dial-up terminals
     ☐ But it was an experimental system, and could not support many users at a time
     ☐ MIT decided to work on a more robust OS that could serve as a true information utility
   ☐ Multics:
     ☐ Developed by MIT in conjuction with GE and Bell Labs, funded by a DARPA grant (US Defense Advanced Research Projects Agency).
     ☐ Work started in 1964 and finished in 1969
     ☐ Designed for GE 646 (which was specially designed for it)
     ☐ Designed to be a 24/7 computing utility
       ☐ Could Continue running while additional processors, memory or disks were attached or removed
       ☐ Designed with security in mind since multiple users would be connecting
       ☐ It also featured innovations in memory management and filesystem to better accommodate multiple users.
     ☐ Virtual Memory
       ☐ Earl days: memory overlaying
       ☐ 1959: U. of Manchester working on Atlas Computer (one of the first supercomputers), introduces something called a one-level storage system, which was a system using paging
         ☐ The first distinction between virtual and physical addresses. They build hardware to do the address translation
         ☐ Invented "demand paging": lets the hard disk be used as a temporary memory. And build a page replacement policy
       ☐ virtual memory, being an untested technology, was not implemented in all computers right away (it took about a decade for implementations to become fast and for translation hardware to be more easily build)
       ☐ 1961: segmentation (Burroughs B5000)
       ☐ 1964: segmentation + paging (Multics/GE 645)
     ☐ Multics filesystem:
       ☐ CTSS: one directory per user (no subdirectories), simplistic protetion scheme
       ☐ Multics: first OS with hierarchial filesystem (nested subdirectories permitted)
         ☐ Also introduced the idea of Access Control Lists for each file (read/write/execute)
     ☐ Multics security
       ☐ Multics introduced a ring structure of access. Ring 0 had the highest level of privilege, and higher rings had lesser and lesser privileges.
       ☐ Each process operated in a specific ring. Processes in a lower ring could not be affected by thiose in higher rings. Hardware enforced the ring scheme.
       ☐ Prior OS's did not have such a formal approach to security were they supported with hardware to this extent.
       ☐ Today's architectures still use protection rings
         ☐ x86 has four rings, but modern OS's use only two of them (ring 0 - kernel mode, ring 3 - user mode)
         ☐ ARM TrustZone: two rings("worlds")
         ☐ Multics had support for 8 rings
 ☐ 1969-75: Unix:
   ☐ PDP:
     ☐ Minicomputer created by DEC
     ☐ Minicomputer = smaller, much lower cost computer than a mainframe
     ☐ Was used to start the code for Unics by Thompson and Dennis Ritchie from Bell Labs, both of which were working on multics
     ☐ Unics, based on Multics was aimed to be more simple and minimalistic designed for lower-end machines.
     ☐ Later shortened to Unix
   ☐ Unix:
     ☐ Ritchie and Thompson began implementing a hierarchial file system (similat to Multics), along with a shell and assembler
       ☐ Devices treated as files
       ☐ Process control via form, et.c)
       ☐ Pipes for inter-process communication
     ☐ v1.0 released in 1971
     ☐ B and C:
       ☐ At the same time that they were working on Unix, Thompson and Ritchie developed the B programming language, a simplified version of something called BCPL
       ☐ ritchie continued to improve the new language, and by 1972, renamed it to C.
     ☐ Portability:
       ☐ In 1973, Unix v4 was released. Its kernel was rewritten in C, one of the first OS kernels to be written in a high-level lanfuage.
       ☐ the migration to C made Unix a portable OS, meaning that it could be easily ported to other machines, since only a small amount of machine-dependednt code had to be changed
     ☐ Popularity:
       ☐ Because of the continuing development, portability and low licensing cost, Unix became increasingly popular.
       ☐ New Version continued to be released in the 1980s, including versions for microcomputers
         ☐ A microcomputer was a small inexpensive computer for hobbyists, which first came out in the early 1970s
       ☐ By 1978, over 600 machines were running some form of Unix, and over 2,000 by 1981. By 1986, it had grown to 230,000 machines.
     ☐ The Unix Philosophy:  
       ☐ Make it easy to write, test, and run programs.
       ☐ Interactive use insteaad of batch processing.
       ☐ Economy and elegance of design due to size constraints
       ☐ Self-supporting System: all Unix software is maintained under Unix
 ☐ 1974: CP/M:
   ☐ CP/M = Control program for Microcomputers
   ☐ Like OS/360 (and Unix), allowed software developers to not have to worry about technical details about the computer hardware
   ☐ The most popular OS for microcomputers in the 70s
     ☐ Supported many different brands
   ☐ Sold by mail-order to hobbyists for $74
   ☐ 1982: Annual sales over $20 million
   ☐ Components:
     ☐ Basic Input/Output system (BIOS): Controlled hardware components (e.g., device input, output to display, reading/writing disk sectors). Differed based on the hardware. Small.
     ☐ Basic Disk Operating System (BDOS): Filesystem, process redirection etc. Well-defined set of system calls
     ☐ Console Command Processor (CCP): Shell (DIR, ERA, COM fiels for utility programs)
   ☐ Software Industry
     ☐ Software copies would sell in the hundreds/thousands for mainframs and microcomputers since they were limited to big corporations/institutions. But microcomputers were selling in the hundreds of thousands
     ☐ Since CP/M was dominant in this time period, software that targeted CP/M could run on a large array of differnt machines
     ☐ The market share for a software product thus expanded greatly. Companies started making software products for microcomputer users to buy directly.
 ☐ 1981: MS-DOS:
   ☐ IBM:
     ☐ Founded as a record-keeping/measuring system company in 1911
     ☐ Became leading manufacturer of punch-card tabulating systems
     ☐ 1940s: Initial forays into computers with MARK I (Harvard); first commercial hard drive
     ☐ 1964: System/360 and OS/360. Dominant in the computer mainframe industry
     ☐ IBM eventually decided to make minicomputers in 1976, 12 years after DEC's PDP-8. But they couldn't keep pace. By the start of the 1980s, their market share in minicomputers was falling
     ☐ Annual sales of microcomputers booming, so IBM also decided to make a microcomputer, but was also late to the game and couldn't keep pace; they knew they needed to enter the market with a strong product t oavoid another failure
     ☐ They started work on the IBM Personal Computer (IBM PC) in 1980 and released it the next year
       ☐ IBM PC used Intel's newest 8088 microprocessor, a varient of the 8088 (i.e. x86 instruction set)
       ☐ As the most popular OS, with the msot available software, CP/M (designed for the intel 8080 and Z80 chips) was expected to be updated to work on the 8087/88 as well
       ☐ IBM talked with Gary Kildall (?) about having an x86 version of CP/M come pre-installed on their PC. But they could not reach a deal, so IBM looked elsewhere
     ☐ Quick and Dirty Operating System (QDOS)
       ☐ Tim Paterson of Seattle Computer Products was one the people eagerly awaiting CP/M for the 8086/88 chip, since sales of their computer kits were falling. But it kept getting delayed by DRI
       ☐ So... he wrote his own version of CP/M for x86
       ☐ He took the CP/M reference manual and duplicated its API, making as exact of a copy as possible for the API, but with a different filesystem (FAT) and other changes.
         ☐ He called it the Quick and Dirty OS (QDOS)
     ☐ Microsoft
       ☐ 1975: Bill Gates and Paul Allen found Microsoft to distribute an implementation of BASIC for the Altair 8800 (the first microcomputer, released the year before)
     ☐ MS-DOS: 
       ☐ Seattle Computer Products was a business partner with Microsoft, as was IBM
       ☐ When Bill Gates heard that IBM was looking for an OS and Gary KIldall had turned them down, he deceaded to licese QDOS, work on it a bit and license it to IBM
       ☐ In 1981, Microsoft purchased QDOS outright for 75,000 and renamed it to Microsoft DISK Operating System, OR MS-DOS
       ☐ MS-DOS became the OS of choice for 8086-like chips
       ☐ Manufactureres would license the OS and add their own device driver, then pre-install it on their machines
     ☐ IBM PC
       ☐ The IBM PC became a runaway hit, highest sale in microcomputer market. 200,000 sold per month by second year. IBM becomes most valuable company in the world by 1984
 ☐ 1984: GUIs and the Mac
   ☐ December 9, 1968: "The mother of all Demos"
     ☐ the computer mouse
     ☐ window-based interface
     ☐ basic hypertext
     ☐ basic word processing
     ☐ video conferencing
     ☐ real-time collaborative document editing
     ☐ Today it is recognized as one of the most important events in the development of modern personal computing
   ☐ Xerox
     ☐ Founded 1906. Sold papers, photocopiers etc.
     ☐ In the late 1960s, with computers on the rise and the notion of the "paperless office" taking hold in some businesses, Xerox got nervous.
     ☐ They wanted to keep pace with the new technology. So, in 1970, they formed a research lab called PARC: Palo Alto Research Center
     ☐ Xerox PARC:
       ☐ PARC attracted the top CS researchers in the country, promising them that hey could work on whatever dream project they wanted.
       ☐ In particular, PARC was situated very close to ARC, at the Stanford Research Park. As ARC's funding began drying up, many ARC researchers moved over to PARC
       ☐ First invention was the laser printer, was THE printer of the time
       ☐ Next, researchers decided to build a computer that could prepare nice documents to be printed on the laser printer
       ☐ They called it the Alto. It was released in 1973
         ☐ A display with full raster-based, bitmapped graphics
           ☐ Each pixel could be turned on and off independently, unlike typical terminals of the time
         ☐ Had a version of Englebart's mouse. Mouse was depicted on screen as a diagonal pointing arrow
       ☐ 1974: Xerox makes Smalltalk for the Alto, a programming language and development environment
         ☐ The first object oriented language. Automatic memory management.
       ☐ Introduced many modern GUI concepts: movable windows with title bars, overlapping windows, icons, popup menus, scroll bars, radio buttons, dialog boxes
         ☐ Desktop metaphor/WIMP
   ☐ The desktop metaphor
     ☐ the idea that a computer monitor is like a "desk" with documents and file folders placed on top of it, which can be opened
     ☐ Small applications called "desk accessories" (alarm clock, note pad, etc.) can also be opened
     ☐ Trash can, etc.
   ☐ WIMP: Windows, icons, menus, pointer
     ☐ Model of human-computer interaction. Developed at PARC
     ☐ Most desktop GUIs use this model (But note that mobile OS's do not!)
   ☐ Failure of PARC
     ☐ The Alto was never commercial product. Only about 2,000 were made, mostly in various Xerox labs and some in universities. Cost was high (32,000)
     ☐ In 1981, Xerox commercialized the idea with an updated computer called the Xerox Star. But it also cost a lot ($17,000) and had very little market impact, even though it had a GUI. (First commercial computer with a GUI)
     ☐ Researchers started leaving
   ☐ Apple
     ☐ Apple founded on April 1 1976
     ☐ It was formed t osell the Apple I, a motherboard to be used as part of a microcomputer.
     ☐ In 1977, they released the Apple II microcomputer. It became the best-selling microcomputer, the first to be able to display color graphics (Could run DOS/CPM)
     ☐ But by 1983, it was surpassed in sales by the IBM PC
     ☐ Steve Jobs and Xerox:
       ☐ In 1978, Apple began working on a new computer called the Lisa, with a home-made oeprating system.
       ☐ Steve Jobs visited PARC in 1979, and was astounded by the Alto's GUI
       ☐ In exchange for some Apple shares, he was allowed to bring his Lisa team to get a tour of PARC and demo of the GUI
         ☐ Many PARC engineers also came over to Apple
     ☐ Lisa OS
       ☐ The Lisa OS expanded on the Alto's GUI
         ☐ First pull-down menu bar (menus at the top)
         ☐ Keyboard shortcuts (e.g., for copy and paste)
         ☐ Trash can
         ☐ Single-button mouse
         ☐ Icons for everything, windows for directories
         ☐ Drag and drop!!
       ☐ Supported pre-emptive shceduling so it could run many programs at once!
         ☐ Remember that CP/M, MS-DOS and others did not have this! Only Unix did, but it was only starting to be updated for microprocessors. (Unix also didn't have support for GUIs)
       ☐ Supported virutal memory with segmentation + swapping!
         ☐ 24-bit virtual addresses: 7 bits (segment number) + 17 (offset). (Total virtual address space 16 MB, while physical RAM was 2MB)
       ☐ Had its own filesystem
    ☐ Lisa Computer
      ☐ The Lisa was released in 1983
        ☐ 1MB of RAM
        ☐ Price: 10,000 (30,000 today)
        ☐ Commercial failure
    ☐ Macintosh
      ☐ Apple realized they had to lower the cost
      ☐ So they designed a new computer: the Macintosh, with lesser components (128KB of RAM vs Lisa's 1MB for example)
    ☐ Mac OS
      ☐ In part because of the smaller memory footprint, the Mac OS did not have the niceties of Lisa:
        ☐ No multitasking
        ☐ No MMU / no memory protection
        ☐ No user/kernel mode! Everythin ran in same mode!
      ☐ But, it still had the GUI (with improvements over Lisa's)
    ☐ Macintosh computer:
      ☐ Released in January 1984, announced by a Super Bowl commercial directed by Ridley Scott
      ☐ Cost: $2,495
      ☐ Like the Lisa, the Macintosh did not do well commercially. It was slow, had no hard drive, no fan, limited software.
        ☐ All software had been written for text-based interface, so it would be a huge challenge to rewrite them for a GUI
        ☐ But the GUI had a huge impact on future UI deisgn
      ☐ Jobs and Wozniak resigned from Apple
      ☐ 1986: The faster Mac Plus comes out. Becomes a hit with creative professionals
    ☐ Susan Kare:
      ☐ Employee #10 at Apple
      ☐ Designed the icons, symbols, typefaces for the Mac UI
        ☐ Trash can, dog-eared paper icon, cursor, Command key, etc.
      ☐ Later went to Microsoft and worked on Windows 3.0, then IBM, Facebook, Pinterest, others
      ☐ Pioneer of the graphical user interface
 ☐ 1985: Windows
   ☐ Designed to run on top of MS-DOS
   ☐ MS-DOS was still the OS, Windows was just an executable that provided a GUI
   ☐ Only needed an 8088 processor and 256k RAM
   ☐ Provided a GUI with (tiled) windows and mouse support
   ☐ Different business model thatn Mac: Windows only sold OS, not a whole machine. And the OS was compatible with many different computers/brands
   ☐ Problems:
     ☐ Slow
     ☐ Limited software compatibility
     ☐ did not come pre-installed like Mac did
   ☐ (Early) Mac vs. Windows
     ☐ Mac: OS build from scratch | Windows: built on top of MS-DOS. Not a standalone OS
     ☐ MAC: OS bundled with Mac computers only ($2,495) | Windows: could be bought for $99 for MS-DOS machines
     ☐ MAC: GUI (WIMP paradigm) | Windows: More primitive form of GUI. Tiled windows only
     ☐ MAC: set the standard for GUI | Windows: laid groundwork for 90s Windows, which would dominate the OS field for personal computers
   ☐ "Inspired" by the Mac
     ☐ Apple complains that Microsoft is coopying their idea. But Microsoft threatens to stop developing Word for Mac, a very popular product
     ☐ 1985: Sculley (Apple CEO) signes deal with Gates as follows
     ☐ In exchange for Microsoft continuing to work on Word for Mac, Microsoft revieces
       ☐ a non-exclusive, worldwide, royalty-free, perpetual, nontransferable license to use [parts ofthe Mac technology] in present and future software programs, and the license them to and through third parties for use in their software programs
     ☐ A huge coup for Microsoft
   ☐ Windows 2.0
     ☐ Overlapping windows. Icons. Keyboard shortcuts. Word and Excel
     ☐ Protected mode, Cooperative multitasking
       ☐ Mac got cooperative multitasking with System 5, released just a couple months before
   ☐ Look and Feel:
     ☐ 1988 Apple files lawsuit accusing of copyright violation. Known as the "look and feel" lawsuit
     ☐ Court ruled that apple could not patent the idea of a GUI, nor the desktop metaphor because they were either not invented by Apple, or they were the only way to express and idea
       ☐ And Apple had also licensed elements of the GUI to Microsoft
     ☐ Gui becomes open source
   ☐ Windows 3.0
     ☐ 1990, $3 million USD release party
     ☐ total overhaul of GUI. Emphasis on icons for file, instead of simple list. Improved multitasking, customizability of appearance (desktop picture, etc.). Better memory use.
     ☐ Huge success
       ☐ Cost was $149 compared to Mac OS which could only be used on an expensive Mac
       ☐ First big success of a GUI-based OS
       ☐ Software industry starts coalescing around Windows
     ☐ Microsoft becomes first software company to reach $1 billion revenue in a single year



The Process:
  ☐ Direct Execution -> allow one process to run until it's done
    ☐ No overhead since process has full control of the CPU, can do anything it wants and will not be paused
    ☐ Since full control of CPU, can hog all resources
    ☐ Can corrupt files on hard drive, crash OS, etc
    ☐ No isolation between processes
  ☐ Kernel:
    ☐ Subset of the OS with special rights and responsibilities
    ☐ Trust with full hardware access
    ☐ Has its own stack and special subset of instructions that only it can perform
  ☐ Processor Modes:
    ☐ User mode:
      ☐ code has restrictions: it cannot directly issue I/O requests. If it tried to do so, the processor would raise an exception and the OS would likely then kill the process
      ☐ No Privileged instructions
      ☐ No direct access to all of memory
      ☐ No direct access to devices
    ☐ Kernel Mode:
      ☐ code can do anything it likes, what the OS runs in
      ☐ Priviliged Instrictions
        ☐ Set mode bit
      ☐ Direct access to all of memory
      ☐ Direct access to devices
 ☐ System Calls:
   ☐ To safely switch between kernel and user mode, the OS exposes certain sensitive operations through a set of system calls, or syscalls for short.
   ☐ A system call is a special function call that goes into OS code and runs on the CPU in kernel mode
     ☐ Only in kernel mode can sensitive operations (like access to hardware be performed)
   ☐ The API to manage processes has a standard called the POSIX API: a standard set of system calles an OS must implement
   ☐ Most modern OS's are POSIX compliant
   ☐ Programs written for the POSIZ API can run on any POSIX compliant OS, ensuring program protability
 ☐ Perfmorming a system Call:
   ☐ Each OS has a kernel API that wraps system calls in function calls, to make them easier to use and more protable
   ☐ Then, the kernel API is then wrapped by program language libraries (like libc)
     ☐ the C library function printf invokes the write system call
     ☐ although libc makes system calls look like regular function calls, they are not function calls. Instead, they are a transition between the user and kernel (and thus much more expensive)
 ☐ Process APIs
   ☐ Some system calls involve management of processes. This is knows as the Process API and includes:
     ☐ create (creates a process)
     ☐ destroy (destroys a process)
     ☐ wait
     ☐ misc
     ☐ get info
 ☐ fork
   ☐ fork is a system call that can create a new process
   ☐ calling fork creates an (almost) exact copy of the calling process, which starts running at the line directly after the fork call. It is called the child process, while the initial process (which is still running) is called the parent.)
   ☐ the child will have its own copy of the address space, registers, PC, etc. One difference from the parent: the value it recieves from the fork call -- it recieves 0, while the parent receives the PID of the newly-created child.
     ☐ output of fork program is not detministic
   ☐ In Unix-like OS's, every process is created by forking from another process
 ☐ wait
   ☐ wait is a system call that lets a parent wait until a child process has finished running before continuing to execute. By using wait, we make out fork program deterministic
   ☐ will not return until the child has run and exited
   ☐ waitpid also exists which waits for a child process with a specific PID to end
 ☐ exec
   ☐ exec is a system call that lets us run a program different from the current (calling) program.
   ☐ it transforms the current process into the new one: it overwrites the ucrrent instructions in memory with the new program's instructions (after loading them from disk), and re-initializes the heap and stack and other parts of the program's memory space.
   ☐ exec never returns
 ☐ fork, wait, and exec together:
   ☐ these 3 are essential in building a shell
   ☐ When typing a command into the shell, it calls fork to make new child process, exec to make the child run the commnad, and wait to wait until the child completes before printing out a new prompt.
 ☐ Traps
   ☐ A trap is generated by the CPU as a result of error
     ☐ Divide by zero
     ☐ Execute privileged instruction in user mode
     ☐ Illegal acces to memory
   ☐ Works like an "involuntary" system call
     ☐ Sets mode to kernel mode
     ☐ Transferl control to kernel
   ☐ return-from-trap
     ☐ the trap and return-from-trap instructions are special. They do the following all at once:
       ☐ jump into kernel code (or process code, for return-from-trap)
       ☐ change the processor mode (user to kernel, or kernel to user)
       ☐ load the kernel stack (or process stack, for return-from-trap)
 ☐ Interupt:
   ☐ an interupt is generated by a device needing attention
     ☐ packet arrived from the network
     ☐ Disk I/O completed
     ☐ etc.
   ☐ Also works like an involuntary system call
 ☐ OS control flow
   ☐ the OS is an event-driven program. It only runs when a trap, interrupt or system call is generated
   ☐ In each of these cases, the processor will effect a mode switch from user mode to kernel
   ☐ Once in kernel mode, the kernel can perform whatever operation, then return control back to the process, using a special return-from-trap instruction
 ☐ Limited Direct Execution
   ☐ Recall direct execution: one process runs on CPU until done, and has full control of hardware (CPU, memory, disk)
   ☐ Limited direct execution: when a proces wants to do something sensitive, it issues a system call, which enters into the OS and then comes back using a return-from-trap. Once done, the process exits, which also traps back into the OS
 ☐ OS structure:
   ☐ User/OS separation -> "monolihthic OS"
     ☐ Downside:
       ☐ The OS is a huge piece of software; Millions of lines of code and growing
       ☐ If something goes wrong in kernel mode, most likely, machine will halt or crash.
       ☐ Incentive to move stuff out of kernel mode
     ☐ No need for entire OS in kernel
       ☐ Some pieces can be in user mode
         ☐ no need for privileged access
         ☐ no need for speed
     ☐ Example: daemons
       ☐ system log
       ☐ printer daemon
       ☐ Etc.
   ☐ Microkernel
     ☐ Absolute minimum in kernel mode
       ☐ interprocess communication primitives
     ☐ All the rest in user mode
     ☐ In practice, have failed commercieally (except for niches)
     ☐ The "systems programs" model has won out
 ☐ Single vs multiple process system
   ☐ Process operation
   ☐ From the point of view of an OS, a process does two things:
     ☐ either it computes (using the CPU), or
     ☐ it does I/O (using a device)
   ☐ In single process systems, process must be completed before moving onto next
     ☐ causes long wait times for processes waiting for the previous to complete
     ☐ Inefficient due to long CPU idle times and bad interactivity (can't do anything while a process is running)
   ☐ The issues of a single process system can be addressed by using a multi-process system.
     ☐ In multi-process systems, while a process waits for an I/O request to complete, another process can use the CPU!
     ☐ During the idle times of a process, the CPU will move to the next process until the CPU can resume the original process again
       ☐ High utilization, short CPU idle times
 ☐ Process States
   ☐ Running: executing instructions
   ☐ Ready: ready to run, but not executing at the moment
   ☐ Blocked: waiting for some event to take place (e.g., for OS to perform I/O request) until it can become ready
   ☐ See Lec 2. slide 89 of the Process State Diagram for Multiprocessing System
 ☐ Process switching
   ☐ Multiprocessing
     ☐ In a system that supports multiple processes, there are two important considerations:
       ☐ how to switch between processes, and 
       ☐ how to determine which process to run (scheduling)
     ☐ If a process is running on the CPU, then the OS is not. But if it is waiting on I/O, then another process should run
       ☐ Also, a process should not be able to run forever
     ☐ 2 problems: regaining control and context switching
     ☐ Problem 1: regaining control:
       ☐ How can the OS regain control of the CPU so that it can switch to another process?
       ☐ Two approaches:
         ☐ non-preemptive scheduling ("coorperative multitasking")
         ☐ preemptive scheduling (noncooperative)
       ☐ Cooperative Approach:
         ☐ The OS trusts processes to behave responsibly
         ☐ Special syscall "yield" lets the process give back control to the CPU... when it wants
           ☐ Note: if the program performs an illegal operation (like a bad memory access), OS also regains control
         ☐ Known as non-preemptive scheduling
         ☐ Problem: A process could run forever, locking all other processes out
         ☐ Solution: Timer interupts, which is resolved by the alternative "Non-cooperative approach"
       ☐ Non-cooperative approach:
         ☐ Timer interrups
           ☐ A timer device can be programmed to raise an interrupt every so often, at which point the process is forcibly paused and the OS executes and interrupt handler, regaining control
         ☐ Known as preemptive scheduling
       ☐ Non preemptive vs. Preemptive
         ☐ Non-preemptive:
           ☐ Process can monopolize the CPU
           ☐ Only useful in special circumstances
         ☐ Preemptive
           ☐ Process can be thrown out at any time
           ☐ Usually not a problem, but sometimes it is
         ☐ Intermediate solutions are possible

     ☐ Problem 2: context switching:
       ☐ When the OS regains control, it can decide whether to continue running the current process or switch to another
         ☐ This decision is made by the OS scheduler
       ☐ If the OS decides to switch, it executes a context switch
   ☐ Process info:
     ☐ Consists of:
       ☐ Code
       ☐ Stack
       ☐ Heap
       ☐ Registers (including program counter)
       ☐ MMU info
     ☐ Code, stack and heap are already stored in memory private to a process
     ☐ But registers and MMU info are stored in a place for the current running process
   ☐ Process switch P1 -> P2:
     ☐ Save registers (P1) to somewhere
     ☐ Restore registers (P2) from somewhere
     ☐ Where to save and restore from? -> see PCB
   ☐ Process Control Block (PCB)
     ☐ The kernel maintains an array of process control blocks (PCB) which contains information about current processes, including:
       ☐ Process identifier (unique id)
       ☐ Process state
       ☐ Space to support process switch (save area)
     ☐ The PCB array is indexed by a hash of the PID
   ☐ Process switch P1 -> P2 (with PCB in mind):
     ☐ Save registers -> PCB[P1].SaveArea
     ☐ Restore PCB[P2].SaveArea -> registers
     ☐ Run return-from-trap instruction
   ☐ Process siwtching is slow:
     ☐ A process switch is an expensive operation!
     ☐ Requires saving and restoring lots of stuff (registers, PC, MMU, info, etc.).
     ☐ Has to be implemented very efficiently and used with care.
   ☐ Context switching overhead
     ☐ Cost of saving registers
     ☐ Cost of scheduler to determine which job to run next
     ☐ Cost of restoring registers
     ☐ Cost of flushing caches (L1, L2, L3, TLB)
 ☐ Process Scheduling
   ☐ Multiprocessing
     ☐ In a system that supports multiple process, there are two important considerations:
       ☐ how to switch between processes, and 
       ☐ how to determine which process to run (scheduling)
   ☐ Scheduling
     ☐ The OS scheduler decides when (and which) to run a ready process
       ☐ If a ready process is run, we say it has been scheduled
       ☐ Or if it running and moves to ready, we say it has been de-scheduled
     ☐ The role of the scheduler:
       ☐ think of scheduler as managing a queue
       ☐ when a process is ready, it is inserted into the queue, according to a scheduling policy
       ☐ scheduling decision: run head of queue
     ☐ Implementation:
       ☐ Remeber running process
       ☐ Maintain sets of queues
         ☐ (CPU) ready queue
         ☐ I/O device queue (one per device)
       ☐ PCBs sit in queues
     ☐ How does athe scheduler run?
       ☐ The scheduler is part of the kernel, so it can run when the kernel runs, i.e. when:
         ☐ process starts or terminates (system call)
         ☐ running process performs an I/O (system call)
         ☐ I/O compeltes (I/O interrupt)
         ☐ timer expires (timer interrupt)
         ☐ a trap occurs
     ☐ Metrics:
       ☐ a scheduling policy determines which ready process will be scheduled next
       ☐ To compare scheduling policies, we need scheduling metrics
 ☐ Scheduling Metrics and Policies
   ☐ Workload:
     ☐ The workload is the set of running processes, or jobs. We will make the following assumptions about jobs:
       ☐ Each job runs for the same amount of time
       ☐ All jobs arrive at the same time
       ☐ Once started, each job runs to completion
       ☐ All jobs only use the CPU
       ☐ The run-time of each job is known
   ☐ Turnaround time:
     ☐ The time it takes for a job to complete: a measure of performance
     ☐ Turaround = Time of completion - Time of arrival
       ☐ Since we assume all jobs arrive at the same time, Time of arrival will be 0 for now
   ☐ Response time:
     ☐ Response time: the difference between the time the job arrives and the time it is first scheduled
     ☐ Response = Time of first run - Time of arrival
       ☐ So that users can see some kind of progress on their job without waiting too long
   ☐ Types of jobs: Interactive vs. Batch
     ☐ What makes a good scheduler for interactive or batch?
     ☐ Interactive = you are waiting for the result
       ☐ E.g., browser, editior
       ☐ Tend to be short
     ☐ Batch = you will look at result later
       ☐ E.g., supercomputing center, offline analysis
       ☐ Tend to be long
     ☐ What makes a good scheduler for interactive?
       ☐ Short response time
       ☐ Response time = wait from ready -> running
         ☐ Initial_schedule_time = arrival_time
     ☐ What makes a good scheduelr for batch?
       ☐ High throughput
       ☐ Throughput = number of jobs completed
         ☐ minimize scheduling overhead
           ☐ Reduce number of ready -> running switches
     ☐ Other Concerns:
       ☐ Fiarness: who gets the resources?
         ☐ We want an equitable division of resources
       ☐ Starvation: how bad can it get?
         ☐ Lack of progress by some job
       ☐ Oberhead: how much useless work?
         ☐ Time wasted switching between jobs
       ☐ Predictability: how consistent?
         ☐ Low variance in respnse time for repeated requests
     ☐ Basic Scheduling Algorithms
       ☐ FIFO (first in, first out)
         ☐ the most basic we can implement. Jobs are executed by the time they arrived in the system. (They are inserted at tail of queue)
         ☐ By definition, non-preemptive
         ☐ Pros and Cons:
           ☐ Low overhead - few scheduling events
           ☐ Good throughput
           ☐ Uneven response time - stuck behind long job
           ☐ Extreme cases - process monopolizes CPU
       ☐ SJF (shortest job first)
         ☐ Improves average turnaround time
         ☐ Optimal scheduling algorithm if all jobs arrive at the same time
         ☐ Pros and Cons:
           ☐ good response time for short jobs, but only if all jobs arrive at same time
           ☐ Can lead to starvation of long jobs
       ☐ STCF (Shortest Time To Completion First)
         ☐ Under the circumstance where jobs DONT NEED to run until done and the scheduler can preempt a job (pause one, and start another), this is the optimal algorithm
       ☐ Round-Robin
         ☐ Instead of running jobs to completion, run a job for a time slice (aka scheduling quantum), then switch to the next job in the queue (the one that hasn't been run for the longest time)
         ☐ Continue until all are done
         ☐ Pros and Cons:
           ☐ Good compromise for long and short jobs
           ☐ Short jobs finish quickly
           ☐ Long jobs are not postponed forever
           ☐ No need to know job length
         ☐ Picking a scheduling quantum:
           ☐ Too small
             ☐ Many scheduling events
             ☐ Good response time
             ☐ Low throughput
           ☐ Too large
             ☐ few scheduling events
             ☐ Good throughput
             ☐ Poor response time
           ☐ Typical value 10 milliseconds

The Memeory:
  ☐ Memory: the dream
    ☐ What every programmer would like is a:
      ☐ private,
      ☐ infinitely large,
      ☐ infinitely fast,
      ☐ nonvolatile, and 
      ☐ cheap memory
  ☐ ASSUMPTION START FOR LECTURE: All of a program must be in main memory
  ☐ Memory allocation: roadmap
    ☐ Where to locate the kernel?
    ☐ Cocept of address space
    ☐ Virtual vs physical mempory
    ☐ Goals of memory management
  ☐ Allocating Main memory for kernel:
    ☐ Almost always low in memory (0 - 100) because Interrupt vectors are in low memory
  ☐ The address space:
    ☐ The OS provides an abstraction of physical memory to each process, called the address space
    ☐ The address space of a pcoress contains:
      ☐ the code (instructions)
      ☐ the stack (local vars, params, return vals), and
      ☐ the heap (dynamically allocated memory)
  ☐ The problem:
    ☐ The address space for each process starts at address 0. But the address in this space are virtual addresses, not real addresses into physical memory
    ☐ How can the OS virtualize the single, physical memory, so that each process has its own private address space, with virtual addresses translated into actual physical addresses
  ☐ Virtual vs. Physical Address space
    ☐ Virtual/logical address space = What the program thinks is its memory
      ☐ Address generated by program/CPU
    ☐ Physical address space = where the program actually is in physical memory
      ☐ Address that the memory sees
    ☐ Must map/translate every access from the CPU to memory
  ☐ Goals:
    ☐ Transparency: should be invisible
      ☐ Processes shouldnt need to know that memory is shared
      ☐ Should work regardless of number and/or location of proceses
      ☐ Programmer shouldn't have to worry about:
        ☐ Where their program is in memory, or
        ☐ where or what other programs are in memory
    ☐ Protection: a process should not be able to do bad things
      ☐ Privacy: Should not be able to read/write the memory of another process or the kernel
      ☐ Should not be able to corrupt OS or other porcesses
    ☐ Efficiency: in terms of time and memory space
  ☐ Memory Management Unit (MMU)
    ☐ Provides mapping virtual-to-physical
    ☐ Provides protection at the same time
    ☐ Hardware!
  ☐ Size of address sapces:
    ☐ Maximum virtual address space size
      ☐ Limited by address size of CPU
      ☐ Typically 32 or 64 bit addresses
      ☐ So, 2^32 = 4 GB, or 2^64 = 16 exabytes (BIG!)
    ☐ Physical address space size:
      ☐ Limited by size of memory
      ☐ Nowadays, order of tens/hundreds of GB
  ☐ Stack and heap:
     ☐ Motivation for dynamic memory allocation
       ☐ Why do processes need dynamic allocation of memory:
         ☐ Do not know amount of memory needed at compile time
         ☐ Must be pessimistic for static memory allocation.
         ☐ If they statically allocate enough for worst possible case, storage is used inefficiently
         ☐ Recursive procedures:
           ☐ Do not know how many times procedure will be nested
         ☐ Complex data structures: lists and trees
     ☐ Stack and heap are 2 types of dynamic allocation
     ☐ Stack:
       ☐ OS uses stack for procedure call frames (local variables and parameters)
       ☐ Memory is freed in opposite order from allocation
       ☐ Ex:
         ☐ alloc(A);
         ☐ alloc(B);
         ☐ alloc(C);
         ☐ ...puts on stack, reading from bottom up: A->B->C
           ☐ C on top, A on bottom
         ☐ So, to free memory, need to call free(C) first, freeing from the top of the stack
       ☐ Simple and efficient implementation:
         ☐ Pointer separates allocated freed space
         ☐ Allocate: Increment pointer
         ☐ Free: Decrement pointer
       ☐ No fragmentation
       ☐ Stack management done automatically
     ☐ Heap:
       ☐ Allocate from any random location
         ☐ Heap consists of allocated areas and free areas (holes)
         ☐ Order of allocation and free is unpredictable
       ☐ + Works for all Data Structres
       ☐ - Allocation can be slow
       ☐ - Fragmentation
       ☐ Programmers manage allocations/deallocations with library calls (malloc/free)
 ☐ The Memory API
   ☐ The stack
     ☐ Allocations and deallocations managed by the compiler
   ☐ The heap
     ☐ You handle the allocations and deallocations
     ☐ malloc() allocates memory on the heap. You pass it a size, and it gives you a pointer to the newly-allocated space (or 0 on failure).
       ☐ The size is usually given by sizeof()
       ☐ It returns a pointer of type void*, so we must cast it
       ☐ double *d = (double *) malloc(sizeof(double));
     ☐ free() deallocates memory on the heap. You pass it a pointer to the memory you want to deaalocate
       ☐ int *x = malloc(10 * sizeof(int));
       ☐ // ...
       ☐ free(x);
 ☐ Address Translation
   ☐ Virtual addresses are passed through the MMU and translated (mapped) to physical addresses
     ☐ This occurs during an instruction fetch, load or store
   ☐ We will look at a simple example of a translation, then see three difference translation (mapping) approaches
     ☐ This lecture assumes that the process address space is placed contiguously in physical memory, and that its size is smaller than the physical memory
 ☐ Mapping Schemes
   ☐ Base and bounds
     ☐ Main idea: use two hardware registers in the MMU: the base register and the bounds register
     ☐ It will let us place a process' address space anywhere in physical memory, while maintaining isolation of the address space from others'
     ☐ When a process starts, the OS will decide where in physical memory the address space of that process should start. It then sets the base register to that address.
     ☐ When the process references a virtual memoryy address, the MMU translates it to a physical address by incrementing it by the base address
     ☐ The bounds register stores the size of the process' address space
     ☐ The MMU will check that any memory reference used by the process is within bounds, i.e., that the virtual address is between 0 and the bounds register
       ☐ If not, the MMU generates a trap, so that the OS can kill the process for trying to access memory outside of its address space
     ☐ Virtual address space
       ☐ linear address space from 0 to MAX
     ☐ Physical address space:
       ☐ linear address space from BASE to BOUNDS = BASE + MAX
     ☐ The base and bounds registers are kept in the MMU
     ☐ Modifications to the base and bounds are privileged operations, i.e., they can only be changed in kernel mode
     ☐ Main memory:
       ☐ Regions in use
       ☐ "Holes" are regions not in use, so a new process must go in a hole
     ☐ Free List:
       ☐ A list of the range of the physical memory not in use
     ☐ Problem: External Fragmentation
       ☐ Small holes become unusable, part of memory can't be used
     ☐ Problem: Internal fragmentation
       ☐ Big chunk of "free" space in virtual address space
       ☐ "free" space takes up physical memory
       ☐ Inefficient
   ☐ Fragmentation:
     ☐ Internal fragmentation: Typical paper book is a collection of pages (text divided into pages). When a chapter's end isn't located at the end of page and new chapter starts from new page, there's a gap between those chapters and it's a waste of space — a chunk (page for a book) has unused space inside (internally) — "white space"
     ☐ External fragmentation: Say you have a paper diary and you didn't write your thoughts sequentially page after page, but, rather randomly. You might end up with a situation when you'd want to write 3 pages in row, but you can't since there're no 3 clean pages one-by-one, you might have 15 clean pages in the diary totally, but they're not contiguous
   ☐ Segmentation
     ☐ What is a segment:
       ☐ Anything we want it to be
       ☐ A segment is a contiguous portion of the address space of a particular
     ☐ Examples:
       ☐ Code
       ☐ Heap
       ☐ Stack
     ☐ Segmentation: instead of a single base/bounds pair per process, we will have a base/bounds pair per segment of the address space
     ☐ then, we can place each segment in different places of the physical memory
     ☐ Virtual Address Space
       ☐ Two-dimensional
       ☐ Set of segments 0 ... n
       ☐ Each segment i is linear from 0 to MAXi
     ☐ Physical Address Space
       ☐ Set of segments, each linear
     ☐ Attempting to refer to an address outside of a segment causes a segmentation fault
     ☐ The MMU must know which segment is associated with each address
     ☐ One way to do this is to set aside the first two btis of each address and use them to indicate the segment (00/01/10).
     ☐ Course-grained = only a few segments (code/stack/heap)
     ☐ Fine-grained = a large number of smaller segments
     ☐ Pros of segmentation:
       ☐ Unlike base and bounds, avodis assigning an entire address space of code, heap, stack and unused virtual memory to process
         ☐ Instead, heap and stack can grow into unused physical memory
       ☐ Minimal overhead
     ☐ Problem: External Fragmentation
       ☐ Physical memory will quickly become full of little holes of free space
       ☐ Makes it difficult to allocate new segments, or grow existing ones
     ☐ Possible Solution: compact physical memory by rearranging existing segments (copything them into a contiguous block).
       ☐ But it's expensive, and makes growing segments harder
     ☐ Segmentation ~= multiple base-and-bounds
       ☐ No internal fragmentation inside each segment
       ☐ External fragmentation problem is similar
         ☐ Pieces are typically smaller
     ☐ Sharing Memory between processes
       ☐ Why would we want to do that?
       ☐ For instance,
         ☐ Run the same program twice in different processes
         ☐ May want to share code
         ☐ Read the same file twice in different processes
         ☐ May want to share memory corresponding to file
       ☐ Sharing not possible with base and bounds, but is possible with segmentation
    ☐ SEGMENTATION PROVIDES EASY SUPPORT FOR SHARING
      ☐ Create segment for shared data
      ☐ Add segment entry in segment table of both processes
      ☐ Points to hsared segment in memory
      ☐ Extra hardware support is needed for form of Portection bits.
        ☐ A few more bits per segment to indicate permissions of read, write, and execute
   ☐ Paging (see next note section)

Paging:
  ☐ Free Space Management:
    ☐ The problem (continuing from Segmentation notes):
      ☐ How to deal with external fragmentation? Can it be minimized? What are the overheads involved?
    ☐ Free list:
      ☐ A free list contains a set of elements that describe the free space remaining in the heap
    ☐ Low-level mechanisms
      ☐ Splitting memory
      ☐ Coalescing memory
      ☐ Keeping track of allocated chunks
      ☐ Memory for the free list
    ☐ Splitting (Lecture 5 Slide 29 Example)
    ☐ Coalescing:
      ☐ when memory is freed (e.g., through free()), the allocator coalesces (merges) that memory with any adjacent free chunks on the free list
      ☐ coalescing free space helps to reduce external fragmentation a little bit
      ☐ can be done when memory is freed, or it can be deferred until a later point
    ☐ Tracking size of allocations
      ☐ when free() is called, we don't need to pass the size of memory to free. The allcator keeps track on its own
      ☐ to do so, each allocated piece of memory has a header block (So if the caller asked for N bytes, the allocator must find a chunk of size N + sizeof header)
    ☐ Free list in memory
      ☐ The free list itself must be placed in memory. It is represented as a linked list
      ☐ Like allocated memory, each free chunk in memory will begin with a header, in this case being a node of this type
    ☐ One free chunk
      ☐ Assume that we are managing an empty heap of 4KB, and that the header for the free chunk will be 8 bytes. the head pointer point to the start of the header
    ☐ One allocation, one free chunk
      ☐ Upon an allocation request, a free chunk of sufficient size will be split into two. the first chunk will fit the header and request, and the second will be a free chunk (also with a header)
    ☐ Allocation strategies:
      ☐ When a piece of memory is requested, the allocator must choose which chunk of free space to use for the new allocaiton. Some possible strategies: 
        ☐ Best fit
          ☐ Find free chunks that are as big or bigger than requested size
          ☐ Return the one that is the smallest (the "best-fit" chunk)
          ☐ Tries to reduce wasted space
          ☐ DrawbackL: needs to look through all free chunks
        ☐ Worst fit
          ☐ Opposite of best fit: find largest free chunk, split it and return the first part and keep the second (large) chunk on the free list
          ☐ Overall effect: after frees, lots of big chunks are left in memory instead lots of small chunks.
          ☐ Drawback: full seach of free space still
          ☐ Also leads to excess external fragmentation
        ☐ First fit
          ☐ Finds the first free chunk that is big enough
          ☐ Fast: does not need to search through all
          ☐ Drawback: Beginning of the free list gets polluited with small objects
        ☐ Next fit
          ☐ Like first fit, but maintains a pointer to the location in the list where the last search ended, then starts looking there for the next search.
          ☐ Thus allocations are made more uniformly through the list, so the beginning is not crowded with small objects
          ☐ Exhaustive search also again avoided
  ☐ Paging:
    ☐ Space management:
      ☐ Segmentation: chop space up into variable-sized pieces
        ☐ leads to fragmentation
      ☐ Paging: chop space into fixed-sized pieces
        ☐ A process' address space (code, heap stack) is divided into fixed-sized units called pages
        ☐ Physical memory: an array of fixed-sized slots called page frames
    ☐ Virtual Address Space
      ☐ Linear from 0 up to a multiple of page size
    ☐ Physical Address Space
      ☐ Noncontiguous set of frames, one per page
      ☐ Frame size is same as page size
    ☐ Paging advantages
      ☐ Flexibility: no assumptions needed about how heap and stack are used, or which direction they grow, etc.
      ☐ Simplicity: free list of free pages. Easy to slot in pages
    ☐ A page table is a data structure kept by the OS for each process. It stores the mapping between each virtual page of the address space and its place in physical memory
      ☐ It is used for address translations from virtual page to physcial page frame
    ☐ Paging: Virtual Address
      ☐ Two components in the virtual address
        ☐ VPN: virtual page number
        ☐ Offset: offset within the page; Page size = 2^(offset)
      ☐ See Lecture 5 Slide 104 for address translation examples
      ☐ MMU for paging:
        ☐ Page table:
          ☐ Data structure used to map the virtual address to physical
          ☐ Indexed by a page number
          ☐ Contains frame number of page in memory
          ☐ Each process has a page table
        ☐ Also need: Pointer to page table in memory, Length of page table
      ☐ Porblem?
        ☐ Address space sparsely used
        ☐ Access to unused portion will appear valid
        ☐ Would rather have an error
      ☐ Solution Valid/Invalid Bit
        ☐ Page table has length 2^p
          ☐ Page table does not cover the entire possible virtual address space, only the pages that the process has allocated
        ☐ Have valid bit in each page table entry
          ☐ Set to valid for used portions of address space
          ☐ Invalid of unused portions
          ☐ (this is the most common approach)
    ☐ Main Memory allocation with paging:
      ☐ Logical address space: fixed size pages
      ☐ Physical address space: fixed size frames
      ☐ New process:
        ☐ Find frames for all of process's pages
      ☐ Easier to find memory than with segmentation
        ☐ Fixed size
    ☐ Internal Fragmentation in Paging:
      ☐ With paging
        ☐ Address space = multiple of page size
      ☐ Part of last page may be unused
      ☐ Not a big problem with reasonable page size
    ☐ In Reality:
      ☐ Base-and-bounds only for niche
      ☐ Segmentation abandoned
        ☐ High complexity for little gain
        ☐ Effect approximated with paging + valid bits
      ☐ Paging is now universal
    ☐ Faster translations
      ☐ Speed problem (See lecture 6 slide 26): Paging address translation performance
      ☐ Solution: a new cache in hardware
        ☐ to speed up paging, we add a cache into the MMU known as the translation-lookaside-buffer (TLB)
        ☐ Upon each virtual memory reference, we check the TLB to see if the requested translation is there. If so, we can use it without consulting the page table
      ☐ TLB:
        ☐ If the TLB holds the translation for the VPN (TLB hit), we can use the TLB entry to get the page frame number
        ☐ If the TLB does not hold the translation (TLB miss), the hardware accesses the page table to get the translation, adds to TLB, and retries
          ☐ Misses are expensive, so we want to avoid them as much as possible
        ☐ How to make TLB fast?
          ☐ Use associative memory (special hardware)
          ☐ Regular memory
            ☐ Lookup by address
          ☐ Associative Memory
            ☐ Lookup by contents
            ☐ Lookup in parallel
          ☐ See lecture 6 slide 40 for diagram
        ☐ TLB size
          ☐ Associative memory is very expensive
          ☐ Therefore, TLB small (64 - 1,024 entries)
          ☐ Want TLB hit-rate close to 100%
        ☐ TLB hitrare and locality
          ☐ Want TLB hit rate close to 100%
          ☐ To do this, try to take advantage of locality
          ☐ Temporal Locality
            ☐ An instruction or data item that has been recently accessed will likely be re-accessed soon in the future
          ☐ Spatial Locality
            ☐ If a program accesses memory at address x, it will likely soon access memory near x
        ☐ See accessing an array example in Lectur 7 around slide 50
      ☐ Revisiting Process Switching
        ☐ Suppose:
          ☐ Process P1 is running
          ☐ Entry (pageno, frameno) in TLB
          ☐ Switch from P1 to P2
          ☐ P2 issues virtual address in page pageno
        ☐ P2 accesses P1's memory!
        ☐ Or unable to distinguish between P1 and P2 mapping
        ☐ Solution 1: Add valid bit/column to TLB table. Invalidate (set valid column to 0) all TLB entries. That way, new entries that have the same pageno as invalid entried, there is a distinction between processes
        ☐ Solution 2:
          ☐ Add process identifieer to TLB entries
            ☐ Match = match on pid AND match on pageno
            ☐ :( makes TLB more complicated and expensive
          ☐ Process switch
            ☐ Nothing to do
            ☐ Cheaper
          ☐ All modern machines have this feature
      ☐ Cache replacement:
        ☐ As with any cache, TLB will become full at some point
        ☐ How to decide which entry to be replaced with new one?
          ☐ Goal: to minimize miss rate
          ☐ Main idea: take advantage of locality
          ☐ See some later notes
    ☐ Page table sizes:
      ☐ Space problem: Page tables can get large
      ☐ Dealing with large virtual address spaces
        ☐ 4kB size pages:
          ☐ Typical value for page size; normally, this value is given as part of the problem statement, or you'd have enough information todeduce it.
          ☐ 4kB size pages means 12 big page offset:
            ☐ Page size = 2^offset Bytes.. Why?
              ☐ Every byte needs to have an address and
              ☐ We can represent 2^offset addresses using offset bits
              ☐ Therefore, Page size = 4KB = 4 * 1024B = 2^2 * 2^10 = 2^12, so offset has 12 bits
        ☐ REVISIT QUIZ AT END OF LECTURE 6
      ☐ How to make Page Table smaller?
        ☐ Big pages
          ☐ Advantage: easy to implement
          ☐ Disadvantage: Larger pages have higher internal fragmentation
          ☐ Most Systems use 4KB or 8KB pages in common case
        ☐ Segmentation + Paging
          ☐ Divide address space into segments (code, heap, stack)
          ☐ Segments can be variable length
          ☐ Divide each segment into fixed-sized pages
          ☐ Implementation:
            ☐ Each segment has a page table
            ☐ Each segment track base (physical address) and bounds of page table for that segment
            ☐ SEE QUIZ LECTURE 7 SLIDE 37!!
          ☐ Advantages of Segmentation + Paging:
            ☐ Supports sparse address spaces
              ☐ Significant memory savings: Decreases size of page tables
                ☐ Unallocated pages between stack and heap no longer take ups space in page table
            ☐ Sharing
            ☐ No external fragmentation
          ☐ Disadvantages of Segmentation + Paging:
            ☐ Potentially large page tables (for each segment)
            ☐ Must allocate each table contiguously
              ☐ Can get tricky with large page table
        ☐ Multi-level page tables
          ☐ turns the linear page table we've seen so far into a tree structure
          ☐ 2-level Page Table:
            ☐ Chop up the page table into page-sized units
            ☐ If an entire page of page-table entries is invalid, don't allocate that page of the page table at all
            ☐ to track if a page of page table is valid, use a page directory (new structure)
              ☐ Essentially keeps track of which "page-sized units" are valid/invalid
              ☐ See lecture 7 slide 44
            ☐ Virtual addresses are no longer of structure, [VPN|Offset], they are instead [PageDirectoryIndex|PageTableIndex|Offset]
            ☐ Why Useful?
              ☐ For sparse address spaces:
                ☐ Most address spaces are sparsely populated
                ☐ One-level page table:
                  ☐ Need page table for entire address space
                ☐ Two-level page table:
                  ☐ Need top-level page table for entire address space
                  ☐ Need only second-level page tables for populated parts of the address space
            ☐ Are two levels enough?
              ☐ Need top-level page table (page directory) for entire address space
              ☐ Assume size of second-level page table == size of page
                ☐ Why? Easy to allocate
              ☐ Problem: Top-level can get too large if large address space (64 bits)
              ☐ Solution (?): More levels
            ☐ More Levels: The Price to be paid
              ☐ Each level adds another memory access
              ☐ N-level page table
                ☐ 1 memory address -> N + 1 memory accesses (N for page table + 1 for phys addr)
              ☐ But, TLB still works
                ☐ If TLB hit, 1 memory access -> 1 memory accesses
                ☐ If miss, 1 memory access -> N + 1 memory accesses
              ☐ -> TLB hit rate must be very high (99%+)
      ☐ Demand Paging
        ☐ What if "out of memory"
          ☐ Need to get rid of one or more processes, store them temporarily on disk
            ☐ this is called swapping
        ☐ Process Switch to a swapped process?
          ☐ Latency can be very high
          ☐ Need to read image from disk
          ☐ Better solution: Demand paging since not all of a process needs to be in memory
        ☐ Demand Paging: Main reason:
          ☐ Virtual address spaces >> Physical Address space
            ☐ No machine has 2^64 bytes
            ☐ Why such large virtual address space?
            ☐ Convenient for programmer
            ☐ Don't have to worry about running out
        ☐ Deman Paging Benefits:
          ☐ shorter processes startup latency
            ☐ Can start process without all of it in memory
            ☐ Even 1 page suffices
          ☐ Better use of main memory
            ☐ Program often does not use certain parts
              ☐ E.g., error handling routines
            ☐ Program often goes through different parts
              ☐ E.g., initialization, computation, termination
        ☐ If the program is not in memory, then where is it?
          ☐ Part of it is in memory, (typically) all of it in on idsk
            ☐ In a special partition called the Backing Store
        ☐ Swapping vs Demand Paging:
          ☐ Swapping = all of a program is in memory OR all of program is on disk
          ☐ Demand Paging = part of program is in memory
        ☐ Demand Paging Mechanism High Level:
          ☐ What if program needs to access part only on disk
            ☐ This is called a page fault
          ☐ Program is suspended
          ☐ OS runs, gets page from disk
          ☐ Program is restarted
            ☐ This is called page fault handling
        ☐ Demand Paging Issues:
          ☐ How to discover a page fault?
          ☐ How to suspend a process?
          ☐ How to get a page from disk?
            ☐ and how t ofind a free frame in memory?
          ☐ How to restart process?
          ☐ Q: Compact the page table?
            ☐ We saw that a page table has many entries. Each entry has a valid bi, indicating whether that entry corresponds to a page that is actually allocated in memory or not.
            ☐ Q: Why have entries for invalid pages? Why not just have entries for the valid pages?
              ☐ Remember that the first part of a virtual address is the VPN. To find the entry for this VPN in the page table (to get the PFN), we simply add the page table starting address to the VPN * sizeof(entry) to obtain the entry. If the page table only had valid pages (and thus its size would constantly be constantly be in flux), it wouldn't be this simple
        ☐ More on Demand Pagign issues:
          ☐ 1. Discover Page Fault:
            ☐ Idea: Use the valid bit in page table
            ☐ Without demand paging:
              ☐ Valid bit = 0, then invalid
              ☐ Valid bit = 1, then valid
            ☐ With Demand paging:
              ☐ Valid bit = 0, then page is invalid OR page is on disk
              ☐ Valid bit = 1, then page is valid AND page is in memory
              ☐ OS needs additional table: invalid/on-disk?
          ☐ 2. Suspending the Faulting Process
            ☐ Idea: Trap into the OS
              ☐ Invalid bit access generates trap
              ☐ Save process information in PCB when trapping into the OS
          ☐ 3. Getting the Page from Disk
            ☐ Idea: OS handles fetch from Disk
              ☐ Assume (for now) there is at least one free frame in memory
              ☐ Allocate a free frame to process
              ☐ Find page on disk
                ☐ OS needs to remember the swap space in page-sized units
                ☐ Need an extra table for that in OS
              ☐ Get disk to transfer page from disk to frame (slow)
              ☐ Since above is slow, while the disk is busy:
                ☐ Shouldn't waste CPU cycles
                ☐ Invoke scheduler to run another process
                ☐ When disk interrupt arrives
                  ☐ Suspend running process
                  ☐ get back to page fault handling
                ☐ Why OK to go through OS (and not in hardware)?
                  ☐ Disk is so slow that it makes the latency of handling this in OS
              ☐ Completing Page Fault Handling
                ☐ Pagetable[pageno].frameno = new frameno
                ☐ Pagetable[pageno].valid = 1
                ☐ Set process state to ready
                ☐ Invoke scheduler
          ☐ 4. When Process Runs Again
            ☐ Idea: Restarts the previously faulting instruction
            ☐ Process now finds
              ☐ Valid bit to be set to 1
              ☐ Page in corresponding frame in memory
              ☐ Note: different from context switch, which continues with the next instruction
        ☐ Page-replacement Policies:
          ☐ OPT
            ☐ An Optimal Algorithm
            ☐ Replace the page that will be referenced the furthest in the future
            ☐ Provably optimal
            ☐ Can't implement (can't predict the future)
            ☐ A basis of comparison for other algorithms
          ☐ Random
            ☐ Random page is replaced
              ☐ Easy to implement
              ☐ But does not take advantage of spatial/temporal locality
          ☐ FIFO
            ☐ Oldest page is replaced
              ☐ Age = Time since brought into memory
            ☐ Easy to implement
              ☐ Keep a queue of pages
              ☐ Bring in a page: stick at the end of the queue
              ☐ Need replacement: pick head of queue
            ☐ Fair
              ☐ All pages recieve equal residency
            ☐ But does not take into account "hot" pages that may always be needed
          ☐ LRU
            ☐ Least Recently Used
            ☐ Cannot look into the future, but can try to predict future using past
            ☐ Replace least recently accessed page
              ☐ With locality, LRU approximates OPT
              ☐ But harder to implement, must track which pages have been accessed
              ☐ But doesn't handle all worklaods well
                ☐ Example: Large array scans that repeat. popular in DBMS
            ☐ LRU approximation with hardware support
              ☐ Use a reference bit:
                ☐ bit in page table
                ☐ hardware sets bit when page is referenced
              ☐ Periodically
                ☐ Read out and store all reference bits
                ☐ Reset all reference bits to zero
              ☐ Keep all reference bits for some time
                ☐ the more bits kept, the better approximation
              ☐ replacement
                ☐ page with smallest value of reference bit history
        ☐ TLB replacement policies
          ☐ TLB can also become full
          ☐ TLB replacement policies are similar to page replacement policies, though they are often more simplifies and appriximations.
          ☐ Implementations have to be very fast (since the TLB must be very fast), and are typically done in hardware to avoid going into the OS











The Disk:
 ☐ I/O Devices
   ☐ Key Concepts:
     ☐ OS role for integrating I/O devices in systems
     ☐ Polling, Interrupts, Drivers
     ☐ Notion of "permanent" Storage
     ☐ File system interface
     ☐ Disk Management for HDDs
       ☐ Disk Allocation, Scheduling, Optimizations
   ☐ How should I/O be integrated into systems
     ☐ I/O = Input/Output
     ☐ For computer systems to be interesting, both input and output are required
     ☐ Many, many I/O devices
   ☐ System architecture: 
     ☐ Buses = data paths that enable information exchange between CPU, RAM and I/O devices
     ☐ PCIe = Peripheral Componenet Interconnect Express
     ☐ DMI = Direct Media Interface
     ☐ USB = Universal Serial Bus
     ☐ eSATA = external SATA
       ☐ SATA = Serial ATA
       ☐ ATA = the AT attachment, in reference to providing connection to the IBM PC AT
     ☐ CPU is connected to main memory via memory bus
     ☐ Some devices connected to CPU via I/O bus (PCI)
     ☐ Peripheral bus connects other, slower devices (SCSI, SATA, USB, etc.)
     ☐ The faster a bus, the shorter it should be.
     ☐ Also, components needing highest performance (e.g., graphics card) are closest to CPU
   ☐ Basics of a device:
     ☐ A device has two important components:
       ☐ The hardware interface (protocol) that it presents to the system, which allows the OS to control its operation
         ☐ Basics of a protocol: Consists of three registers
           ☐ a status register, which can be read to see the current status of the device
           ☐ a command register, to tell the device to perform a certain tasl; and
           ☐ a data register, to pass data to or get data from the device
         ☐ By reading and writing these 3 registers, the OS can control device behavior
       ☐ The internal structure. E.g., hardware chips, memory, simple CPU firmware (software within a hardware device)
     ☐ Typical OS interaction with a device:
       ☐ 1. Wait until device is not busy
       ☐ 2. starts device and executes the command
       ☐ 3. wait until device is done with request
       ☐ In this approach, the OS repeatedly checks the status register until the device is ready to receive a command, and then until the device is done executing the command. This is known as polling the device.
       ☐ Polling is inefficient (wastes CPU cycles). that's why we will see some better ways to check the status
     ☐ Interrupts
       ☐ Put process requesting I/O to sleep
       ☐ Context switch to a different process
       ☐ When I/O finishes, wake sleeping process with an interrupt
       ☐ Remember: same interrupt mechanism for demand paging
       ☐ Interrupt vs Syscall:
         ☐ Interrupt:
           ☐ generated by hardware to initiate a context switch
         ☐ Syscall: generated by process, to request functionality from kernel mode. Also, initiates a context switch
       ☐ See Lecture 9 slide 45 for example
       ☐ Advatange: No wasted CPU cycles
       ☐ Disadvantage:
         ☐ Expensive to context switch
           ☐ Polling can be better for fast devices
     ☐ Device drivers
       ☐ The OS should be device-netural; it should not contain details of device interactions in its subsystems
         ☐ E.g., the filesystem should not need to know the specifics of what kind of disk it is writing to or reading from
       ☐ we encapsulate specifics of device interation in a device driver
       ☐ Since device drivers are needed for any kind of device you might plug into your system, they represent a huge chunk of kernel code
       ☐ Over 70% of the Linux kernel is device drivers
         ☐ So when we say that an OS has millions lines of code, it really does
     ☐ Hard drives
       ☐ Permanent Sotrage Media
         ☐ Main memory - not suitable
         ☐ Battery-backed memory
         ☐ Nonvolatile memory
           ☐ Flash SSDs
           ☐ 3Dxpoint SSDs (released in 2018)
         ☐ Hard Disks (HDDs)
         ☐ Tapes
       ☐ The main form of persistent storage
       ☐ File system technology is based on their behaviour
     ☐ Interface
       ☐ A drive consists of a number of sectors (512-byte blocks) that can be read/written. We can view the drive as an array of sectors
       ☐ A single 512-byte write is atomic (will complete in its entirety or will not complete at all). If a larger write occurs and power is lost, not all the write may compelte
     ☐ Disk geometry
       ☐ A hard drive is made up of one more platters: circular, hard surfaces on which data is stored by applying magnetic charges
         ☐ Each platter has two sides or surfaces
       ☐ Platters are bound together around a spindle, which is connected to a motor that can spin the platters around, at a constant rate (rotaitons per minute, or RPM)
         ☐ A typical RMP is 7,200
       ☐ Each surface of a platter has thousands of concentric circles called tracks on which data is stored. Each track has a number of sectors
       ☐ Reading and writing from a surface is performed by a disk head (one head per surface); the heads are attached to a disk arm, which moves across the surface to position the head over the desired track
       ☐ REVIEW EXAMPLES ON LECTURE 9 SLIDE 90ish
    ☐ I/O Time
      ☐ head selection (to select platter)
      ☐ seek time (depends on disk; time to find track?)
      ☐ rotational delay (depends on disk)
      ☐ transfer time (depends on file size)
    ☐ Disk Interface
      ☐ Accessible by sector only
        ☐ ReadSector
        ☐ WriteSector
      ☐ Logical_sector_number
        ☐ Platter
        ☐ Cylinder or track
        ☐ Sector
    ☐ Flash-based SSDs
      ☐ No moving parts! No magnet problem!
      ☐ Built out of transistors, like memory and CPU but can retain data without power
    ☐ Bit storage
      ☐ In a flash chip, one or more bits are stored int oa single transistor
        ☐ In a single-level cell (SLC), a single bit is stored
        ☐ Multi-level cell (MLC): Two bits, encocded into different levels of charge (00, 01, 10, 11: low, medium-low, medium-high, high)
        ☐ Triple-level cell (TLC): 3 bits per cell
    ☐ Flash chip organization
      ☐ Flash chips are organized into banks (or planes)
      ☐ Each bank consists of a large number of block of size 128KB or 256 KB
      ☐ Each block contains a large number of pages of size 4KB
    ☐ Low-level operations
      ☐ Reading a page
        ☐ An SSD is a random access device. It can read any page in about the same amount of time regardless of location (Unlike a regular hard drive!)
      ☐ Erase a block
        ☐ Erase destroys the contents of a block (so if we want to write to a specific page in a block, must first erase the entire block before writing) by setting each bit to the value 1
          ☐ Erasing is the first step in actually writing anything to the disk
        ☐ Expensive: takes a few milliseconds
      ☐ Program a page
        ☐ Once a block is erase, the program command can change some of the 1's in a page to 0's
          ☐ That is, you must erase before you can write (Unusual)
          ☐ the only way to change bits to 1 is to erase. Prgramming a page can only flip it to 0
        ☐ Takes 100s of microseconds
      ☐ Example (Lecture 10 slide 60ish) Summary
        ☐ 4 8-bit pages, want to write to page 0. need to erase the entire block, setting all bits across all pages to 1, then we can program page 0.
          ☐ Note that if we wanted t okeep the data in the other three pages, we would have had to put them in memory (e.g.) and then write their contents back, along with sector 0
      ☐ Reading and writing:
        ☐ Reading is fast and can greatly exceed the random read performance of regular drives
        ☐ Writing is expensive: we have to erase the entire block (first moving any important data to another location) and then program it
          ☐ Frequent program/erase cyceles can wear out the flash chips
      ☐ Reliabaility
        ☐ Mechanical hard drives can fail for a variety of reasons
        ☐ Flash chips are pure silicon. But, they can wear out.
        ☐ When a flash block is erased/programmed, it slowly accrues a little bit of extra charge
        ☐ Over time, as the extra charge builds up, it becomes increasingly difficult to differentiate between a 0 and a 1
          ☐ at a certain point, the block becomes unusable 
        ☐ the typical lifetime of a block is currently not well known...
        ☐ SLC chips are rated at 100,000 P/E cycles. MLC 10,000.
        ☐ But research has shown lifetimes are likely much longer
      ☐ SSD structure
        ☐ An SSD consists of some number of flash chips, volatile memory (for aching, e.g.) and some control logic
        ☐ Control logic includes the flash translation layer (FTL). It takes read and write requests on logical blocks and turns them into low-level read, erase and program commands on the device.
      ☐ Direct-mapped FTL
        ☐ In a simple FTL approach we can call direct mapping:
          ☐ A read to a logical page N maps to a read of the physical page N
          ☐ A write to logical page N maps to a read of the block containing the physical page N, then erase of the block, and then programming of the block.
        ☐ Such a direct mapping approach has a lot of problems
          ☐ Very bad performance (costly to write).
          ☐ Even worse reliability: data that is frequently updated, such as file system metadata, would cause the same block to be erased and programmed over and over, leading to the chip wearing out!
      ☐ Log-based FTL
        ☐ Most FTLs today are log structured
          ☐ Upon a write to logical block N, the device appends the write to the next free spot in the currently-being-written-to block.
          ☐ A mapping table stores the physical address of each logical block in the system
        ☐ **See example in Lecture 10**
          ☐ First write: erase the whole first block, then write to first page
          ☐ For the subsequent writes, since the whole block was already erased, no more erases are needed to write pages in this block.
            ☐ Pages have states, wither Valid, Empty, or Invalid
            ☐ Once block erased, state changes to E; pages can only be used for writing if state is E (not invalid)
            ☐ Garbage:
              ☐ Writing to logical blocks a second time, after being used once, simply stores the desired content in the next free physical pages, then updates the mapping table to point the logical address to the new contents
              ☐ Now the old contents are garbage, that is they are the old versions of the logical blocks and are no longer referenced in the mapping table
              ☐ Because of the log-structured nature of an SSD, overwriting data creates garbage blocks (or dead blocks). The device must reclaim these to make space in a process known as garbage collection
      ☐ Garbage Collection:
        ☐ Basic process:
          ☐ Find a block with one or more garbage pages
          ☐ Read in the live (non-garbage) pages from that block
          ☐ Write them somewhere else ("to the log").
          ☐ Reclaim the entire block for writing
          ☐ See example in Lecture 10
        ☐ can be expensive (reading, rewriting).
        ☐ The ideal candidate for reclamation is a block of only dead pages (then there is no need to move data)
        ☐ Many modern drives add extra flash capacity to the drive so that garbage collection can be delayed and done in the background (like when the drive is less busy)
        ☐ This is known as overprovisioning
        ☐ Log-based FTL:
          ☐ Since erases are only required once in a while in this scheme, and the "read-modify-write" pattern of the direct-mapping approach is avoided completely, the log-based FTL greatly improved performance.
          ☐ It also improve reliability, since writes are spread across all pages





















 ☐ File API
   ☐ The file:
     ☐ A linear array of bytes, each of which can be read/written
       ☐ could contain any data. the filesystem does not know; only the application knows what the data means
       ☐ Has a low-level name, known as the inode number
     ☐ Creating files:
       ☐ A file is created by calling the open() system call and passing the 0_CREAT flag
       ☐ The call returns a file descriptor: an integer for that process that you can use read/write to the file
         ☐ File descriptors are managed on a per-process basis
     ☐ Reading and writing files:
       ☐ If we were at the terminal, we might use cat to show the contents of a file
       ☐ How does cat access a file? we can find out by using the strace command, which traces the system calls of a program.
         ☐ 